{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6f4b0ba3-1eca-4e08-a935-8aa7f4465c40",
   "metadata": {},
   "source": [
    "![DLI Header](images/DLI_Header.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4eb1df6-6e7e-4a84-bd61-5d72091226e8",
   "metadata": {},
   "source": [
    "# 聊天机器人\n",
    "｜Chatbot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a78aff64-cba8-490f-a48e-45966b2e68b9",
   "metadata": {},
   "source": [
    "在此笔记本中，您将开始创建聊天机器人功能，创建一个能够保留对话历史记录的 AI 机器人。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "504b12a7-0bf2-4974-a6c4-05b5a7b64644",
   "metadata": {},
   "source": [
    "## 学习目标\n",
    "｜Learning Objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25eb791e-c828-4593-889b-2c95068ed2ff",
   "metadata": {},
   "source": [
    "在完成此笔记本时，您将能够：\n",
    "- 从我们的 LLaMA-2 模型创建聊天机器人功能，能够保留对话历史记录。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed9cda51",
   "metadata": {},
   "source": [
    "## 视频演练"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3164076",
   "metadata": {},
   "source": [
    "执行下面的单元格以加载此笔记本的视频演练。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a147ae85-0e68-487f-8c4e-99dbbe704310",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    " from IPython.display import HTML\n",
    "\n",
    "video_url = \"https://d36m44n9vdbmda.cloudfront.net/assets/s-fx-12-v1/v2/06-chatbot.mp4\"\n",
    "\n",
    "video_html = f\"\"\"\n",
    "<video controls width=\"640\" height=\"360\">\n",
    "    <source src=\"{video_url}\" type=\"video/mp4\">\n",
    "    Your browser does not support the video tag.\n",
    "</video>\n",
    "\"\"\"\n",
    "\n",
    "display(HTML(video_html))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bffe9a8",
   "metadata": {},
   "source": [
    "## 创建 LLaMA-2 管道\n",
    "｜Create LLaMA-2 Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab62455a-b3bf-41eb-8499-b85dc053c31f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "model = \"TheBloke/Llama-2-13B-chat-GPTQ\"\n",
    "# model = \"TheBloke/Llama-2-7B-chat-GPTQ\"\n",
    "\n",
    "llama_pipe = pipeline(\"text-generation\", model=model, device_map=\"auto\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82975443-881e-4188-a5dc-c4f44ceb58ea",
   "metadata": {},
   "source": [
    "## 辅助函数和类\n",
    "｜Helper Functions and Classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ee245ec-bfef-4fe3-9617-31a5b2893b70",
   "metadata": {},
   "source": [
    "在此笔记本中，我们将使用以下函数和类来支持我们与 LLM 的交互。现在可以随意浏览一下，因为在下面使用时会更详细地介绍。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c77ceca2-1a25-4ede-8cf8-11f98dcd14e6",
   "metadata": {},
   "source": [
    "### 生成模型响应\n",
    "｜Generate Model Responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42343e1f-40d8-4b9a-a56c-53f60774dcd5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate(prompt, max_length=1024, pipe=llama_pipe, **kwargs):\n",
    "    \"\"\"\n",
    "    Generates a response to the given prompt using a specified language model pipeline.\n",
    "\n",
    "    This function takes a prompt and passes it to a language model pipeline, such as LLaMA, \n",
    "    to generate a text response. The function is designed to allow customization of the \n",
    "    generation process through various parameters and keyword arguments.\n",
    "\n",
    "    Parameters:\n",
    "    - prompt (str): The input text prompt to generate a response for.\n",
    "    - max_length (int): The maximum length of the generated response. Default is 1024 tokens.\n",
    "    - pipe (callable): The language model pipeline function used for generation. Default is llama_pipe.\n",
    "    - **kwargs: Additional keyword arguments that are passed to the pipeline function.\n",
    "\n",
    "    Returns:\n",
    "    - str: The generated text response from the model, trimmed of leading and trailing whitespace.\n",
    "\n",
    "    Example usage:\n",
    "    ```\n",
    "    prompt_text = \"Explain the theory of relativity.\"\n",
    "    response = generate(prompt_text, max_length=512, pipe=my_custom_pipeline, temperature=0.7)\n",
    "    print(response)\n",
    "    ```\n",
    "    \"\"\"\n",
    "\n",
    "    def_kwargs = dict(return_full_text=False, return_dict=False)\n",
    "    response = pipe(prompt.strip(), max_length=max_length, **kwargs, **def_kwargs)\n",
    "    return response[0]['generated_text'].strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77d2cbc1-a033-48bd-9f84-39aed664a4de",
   "metadata": {},
   "source": [
    "### 构造提示，可选地使用系统上下文和/或示例\n",
    "｜Costruct Prompt, Optionally With System Context and/or Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40abda7d-fffc-4afb-bbb9-922dea42057b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def construct_prompt_with_context(main_prompt, system_context=\"\", conversation_examples=[]):\n",
    "    \"\"\"\n",
    "    Constructs a complete structured prompt for a language model, including optional system context and conversation examples.\n",
    "\n",
    "    This function compiles a prompt that can be directly used for generating responses from a language model. \n",
    "    It creates a structured format that begins with an optional system context message, appends a series of conversational \n",
    "    examples as prior interactions, and ends with the main user prompt. If no system context or conversation examples are provided,\n",
    "    it will return only the main prompt.\n",
    "\n",
    "    Parameters:\n",
    "    - main_prompt (str): The core question or statement for the language model to respond to.\n",
    "    - system_context (str, optional): Additional context or information about the scenario or environment. Defaults to an empty string.\n",
    "    - conversation_examples (list of tuples, optional): Prior exchanges provided as context, where each tuple contains a user message \n",
    "      and a corresponding agent response. Defaults to an empty list.\n",
    "\n",
    "    Returns:\n",
    "    - str: A string formatted as a complete prompt ready for language model input. If no system context or examples are provided, returns the main prompt.\n",
    "\n",
    "    Example usage:\n",
    "    ```\n",
    "    main_prompt = \"I'm looking to improve my dialogue writing skills for my next short story. Any suggestions?\"\n",
    "    system_context = \"User is an aspiring author seeking to enhance dialogue writing techniques.\"\n",
    "    conversation_examples = [\n",
    "        (\"How can dialogue contribute to character development?\", \"Dialogue should reveal character traits and show personal growth over the story arc.\"),\n",
    "        (\"What are some common pitfalls in writing dialogue?\", \"Avoid exposition dumps in dialogue and make sure each character's voice is distinct.\")\n",
    "    ]\n",
    "\n",
    "    full_prompt = construct_prompt_with_context(main_prompt, system_context, conversation_examples)\n",
    "    print(full_prompt)\n",
    "    ```\n",
    "    \"\"\"\n",
    "    \n",
    "    # Return the main prompt if no system context or conversation examples are provided\n",
    "    if not system_context and not conversation_examples:\n",
    "        return main_prompt\n",
    "\n",
    "    # Start with the initial part of the prompt including the system context, if provided\n",
    "    full_prompt = f\"<s>[INST] <<SYS>>{system_context}<</SYS>>\\n\" if system_context else \"<s>[INST]\\n\"\n",
    "\n",
    "    # Add each example from the conversation_examples to the prompt\n",
    "    for user_msg, agent_response in conversation_examples:\n",
    "        full_prompt += f\"{user_msg} [/INST] {agent_response} </s><s>[INST]\"\n",
    "\n",
    "    # Add the main user prompt at the end\n",
    "    full_prompt += f\"{main_prompt} [/INST]\"\n",
    "\n",
    "    return full_prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a17c1228-3280-4798-9558-e19139b77b6c",
   "metadata": {},
   "source": [
    "### LlamaChatbot 类\n",
    "｜LlamaChatbot Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36e18ccd-72e4-40aa-a2fb-d8359fb5fda8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class LlamaChatbot:\n",
    "    \"\"\"\n",
    "    A chatbot interface for generating conversational responses using the LLaMA language model.\n",
    "\n",
    "    Attributes:\n",
    "    - system_context (str): Contextual information to provide to the language model for all conversations.\n",
    "    - conversation_history (list of tuples): Stores the history of the conversation, where each\n",
    "      tuple contains a user message and the corresponding agent response.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, system_context):\n",
    "        \"\"\"\n",
    "        Initializes a new instance of the LlamaChatbot class.\n",
    "\n",
    "        Parameters:\n",
    "        - system_context (str): A string that sets the initial context for the language model.\n",
    "        \"\"\"\n",
    "        self.system_context = system_context\n",
    "        self.conversation_history = []  # Initializes the conversation history\n",
    "\n",
    "    def chat(self, user_msg):\n",
    "        \"\"\"\n",
    "        Generates a response from the chatbot based on the user's message.\n",
    "\n",
    "        This method constructs a prompt with the current system context and conversation history,\n",
    "        sends it to the language model, and then stores the new user message and model's response\n",
    "        in the conversation history.\n",
    "\n",
    "        Parameters:\n",
    "        - user_msg (str): The user's message to which the chatbot will respond.\n",
    "\n",
    "        Returns:\n",
    "        - str: The generated response from the chatbot.\n",
    "        \"\"\"\n",
    "        # Generate the prompt using the conversation history and the new user message\n",
    "        prompt = construct_prompt_with_context(user_msg, self.system_context, self.conversation_history)\n",
    "        \n",
    "        # Get the model's response\n",
    "        agent_response = generate(prompt)\n",
    "\n",
    "        # Store this interaction in the conversation history\n",
    "        self.conversation_history.append((user_msg, agent_response))\n",
    "\n",
    "        return agent_response\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        Resets the conversation history of the chatbot.\n",
    "\n",
    "        This method clears the existing conversation history, effectively restarting the conversation.\n",
    "        \"\"\"\n",
    "        # Clear conversation history\n",
    "        self.conversation_history = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d710c5b-5ac6-4f34-b0c1-e6518af71615",
   "metadata": {},
   "source": [
    "## 没有对话记忆\n",
    "｜No Conversation Memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e530c8c9-0156-49c3-9670-b41abd86410e",
   "metadata": {},
   "source": [
    "让我们与我们的 LLM 开始对话。我们将提供一个**系统上下文** ，即它应该是一个友好的聊天机器人，并且（天真地）鼓励它始终记住我们在对话中提供的姓名。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10bd66e2-8337-4eb9-9f0a-3e49939d0247",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "system_context = \"\"\"\n",
    "You are a friendly chatbot always eager to help and engage in meaningful conversation. \\\n",
    "You always remember the details of our previous conversations, \\\n",
    "especially if a user gives them their name.\n",
    "\"\"\"\n",
    "\n",
    "prompt = \"Hello my name is Star. Nice to meet you!\"\n",
    "\n",
    "print(generate(construct_prompt_with_context(prompt, system_context)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe86fb71-f720-458f-af70-c897efa81990",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "837cd227-0900-498a-9f4d-bcdf0a9e77bc",
   "metadata": {},
   "source": [
    "该模型肯定似乎渴望表明它记得我们是谁。让我们看看当我们真正对其姓名保留能力进行测试时会发生什么。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2d0bcd6-11f5-4f33-9a8e-f657ce4a4ca8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "system_context = \"\"\"\n",
    "You are a friendly chatbot always eager to help and engage in meaningful conversation. \\\n",
    "You always remember the details of our previous conversations, \\\n",
    "especially if a user gives them their name.\n",
    "\"\"\"\n",
    "\n",
    "prompt = \"Can you remind me what my name is?\"\n",
    "\n",
    "print(generate(construct_prompt_with_context(prompt, system_context)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51c9400d-64a8-4107-a441-6cb70004e603",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45217bd0-699e-4126-9e69-f2f4ab22ce38",
   "metadata": {},
   "source": [
    "它可能并不令人意外，该模型并不记得我们的姓名，因为尽管它向我们展示了自己，但我们没有提供任何能力来记住之前对话交换中的任何细节。该模型似乎坚持认为我们的姓名是“Emily”，这显然是不正确的。当模型生成虚构的响应时，通常充满信心，我们称之为**幻觉（hallucination）** 。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ffb9670-9447-43bd-9f42-1c36adc9fe00",
   "metadata": {},
   "source": [
    "## 创建对话记忆\n",
    "｜Create Conversation Memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb21775d-758f-4f92-9224-7227156f3243",
   "metadata": {},
   "source": [
    "为了创建聊天机器人体验，以便模型能够保留先前交换的信息，我们将使用一个`LlamaChatbot`类（如上所定义）。以下是我们类定义的`help`输出。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdf3e428-7890-4e39-9f7c-639044c947d4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "help(LlamaChatbot)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18d28517-0239-450d-a077-ff087cbe8226",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51ecb676-bdf1-4e58-a150-dcfc7e09e2e8",
   "metadata": {},
   "source": [
    "与我们的直接目标最相关的是创建`conversation_history`列表，我们将在每次调用`chat`方法时追加到该列表。我们将重用之前笔记本中的一些相同逻辑，特别是利用 LLaMA-2 的**提示模板（prompt template）** ，以便每次用户/模型交互都正确格式化，然后在用户和模型之间后续交换的提示之前添加。\n",
    "\n",
    "可以准确地说，在每次交互中，我们都在执行**少样本学习（few-shot learning）** ，其中指示性示例仅仅是先前的交互。\n",
    "\n",
    "让我们看看它的实际效果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f807d3b9-00a0-428d-beb4-07e061fcc06d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "system_context = \"\"\"\n",
    "You are a friendly chatbot always eager to help and engage in meaningful conversation. You are always kind \\\n",
    "but also repectful and professional.\n",
    "\"\"\"\n",
    "\n",
    "chatbot = LlamaChatbot(system_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f94b8d9d-b76a-4df8-9886-eeca66fccaff",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(chatbot.chat(\"Hi, my name is Star. Nice to meet you!\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d8995df-c073-4f59-b098-f5c9034c285b",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "932831fc-9227-4aef-992e-21e2926f24a9",
   "metadata": {},
   "source": [
    "到目前为止一切顺利。现在让我们看看模型是否能够“回忆”起我们的姓名。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2950da7-9863-4765-8750-da1de864e4fe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(chatbot.chat(\"Can you remind me what my name is?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c83c561b-f85d-4771-ae86-dfba2d58e973",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb67e3de-2ac6-4aeb-b251-337df87a7846",
   "metadata": {},
   "source": [
    "成功！让我们看一下模型的对话历史记录。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "073ad5ac-3e79-40ac-8291-ad811aa0bd6c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "chatbot.conversation_history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ad988f1-c292-41fd-9660-4d9eb066fcea",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45d5d2ef-27aa-4bbe-922b-bd025c82010a",
   "metadata": {},
   "source": [
    "鉴于`conversation_history`被添加到每个新提示之前，因此模型能够根据先前的交换生成响应是有道理的。\n",
    "\n",
    "`reset`方法将清除`conversation_history`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "729a3ad7-2d20-4cc8-8b96-95254b4e0e51",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "chatbot.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "543f81f1-aefe-4334-a470-cbca4a521d64",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(chatbot.chat(\"Can you remind me what my name is?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7defa717-6ade-4757-894d-8ddd544f5d1c",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f22e5d0-457d-49e8-b718-72c37a5d8722",
   "metadata": {},
   "source": [
    "现在，模型无法“回忆”起我们先前交换的详细信息也就不足为奇了。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ca0b735-208a-4b3d-be23-17a6a17262e2",
   "metadata": {},
   "source": [
    "## 练习：任务跟踪器\n",
    "｜Exercise: Task Tracker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "913bdea1-877b-484b-bb7d-39f116f908ec",
   "metadata": {},
   "source": [
    "创建一个助手，可以跟踪您今天需要完成的事情。它应该能够根据您的对话添加和删除列表中的内容，并且在任何给定时间准确地提醒您还有哪些事情需要做。\n",
    "\n",
    "如果您遇到困难，请查看下面的解决方案。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00f41c8e-3220-464c-b411-f6598e9b50d6",
   "metadata": {},
   "source": [
    "### 您的工作在此处"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc457e8-33de-4e41-9cb2-d583a7095f8f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "01eaf61f-9fe0-47f1-95f5-9902fb336469",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 解决方案"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f49ea64-5bab-4fa1-9a1d-f311b8c9fc32",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "system_context = \"\"\"\n",
    "You are an assistant that help me keep track of what I need to do. You keep track of tasks that I \\\n",
    "provide you, and when asked remind me of what I have left to do.\n",
    "\"\"\"\n",
    "\n",
    "chatbot = LlamaChatbot(system_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42af759c-4e54-442d-a823-7dcda80c1708",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(chatbot.chat(\"I need to do the following things today: eat breakfast, eat lunch, eat dinner, go to work, exercise, and clean the house\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efdac879-04c6-4dc8-bdb6-5a46ad1cbd86",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(chatbot.chat(\"Yes, I also need to hang out with friends.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70360868-62f1-4029-98c0-f20f9ad80d10",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(chatbot.chat(\"Okay, I'm done eating breakfast and exercising.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d488ee-64d4-41ee-b59b-aae1b15ea4e5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(chatbot.chat(\"I've eaten lunch. Sometime today I need to call the bike shop.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1afe3a2-e054-4c89-bd98-9c3b9026b3e9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(chatbot.chat(\"I finished work, hung out with friends, cleaned the house, and called the bike shop.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c333c312-dd3b-401f-a6cc-1281f6ba4b91",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(chatbot.chat(\"I just ate dinner. Now I just need to go to sleep.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b823025-3bbd-4958-a5ef-acbcca859654",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "chatbot.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05a8ae85-9c93-463a-a327-3d93397a412b",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 关键概念回顾"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc97760f-ee70-4a39-ade0-9fe63231ec5b",
   "metadata": {},
   "source": [
    "本笔记本中介绍了以下关键概念：\n",
    "\n",
    "- **幻觉（Hallucination）：** 当模型生成通常带有某种表达的自信的虚假或不准确的响应时。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd47cc91-8655-4a58-98dc-75b4dfc26cbb",
   "metadata": {},
   "source": [
    "## 可选的高级练习"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2f282b3-1d7f-4f49-96e5-e803692d3276",
   "metadata": {},
   "source": [
    "**注意：** 在下一个笔记本中，我们将发现模型能够存储的对话量存在限制，在超过限制后，事情开始出错。考虑到这一点，并且在建议其他实验之前，如果您发现模型开始仅产生空响应，请继续下一节，以便您了解这是怎么回事。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "834be291-bf2d-4f78-9038-81af05bb8981",
   "metadata": {},
   "source": [
    "如果您想超越课程的要求，以下是一些供您尝试的额外开放式练习。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9c74a75-ba20-4682-bb42-fed8cbe4f647",
   "metadata": {},
   "source": [
    "### 使用 7B 模型\n",
    "｜Use the 7B Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33f8b94c-afe7-4131-be09-84e73072e533",
   "metadata": {},
   "source": [
    "在笔记本的顶部，在重新启动内核（请参见下面的单元格）之后，取消注释并使用 7B 模型而不是我们演示的 13B 模型。尝试在使用较小（较弱）模型的情况下获得令人满意的结果。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28c23284-7338-4036-a80a-e543186f951c",
   "metadata": {},
   "source": [
    "### 创建一个助手机器人\n",
    "｜Make a Helper Bot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c259b51-701d-4b4e-92ba-f178302cd8a7",
   "metadata": {},
   "source": [
    "创建一个机器人，人们在度过艰难的一天时可以使用它，它会给予鼓励、赞扬和同理心，并知道何时响应以及何时询问有关用户正在发生的事情的更多问题。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fc4fd6e",
   "metadata": {},
   "source": [
    "## 重新启动内核\n",
    "｜Restart the Kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d696023e",
   "metadata": {},
   "source": [
    "为了释放下一个笔记本的 GPU 内存，请运行以下单元格以重新启动内核。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ce3340b-1e90-4485-abfb-bdaab9cc0894",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from IPython import get_ipython\n",
    "\n",
    "get_ipython().kernel.do_shutdown(restart=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e998941",
   "metadata": {},
   "source": [
    "# [5LOI DEEP LEARNING INSTITUE](https://5loi.com)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d4cf25b",
   "metadata": {},
   "source": [
    "### [AI COMMUNITY](https://www.theforage.cn/community)\n",
    "\n",
    "### [5LOI](https://5loi.com/about_loi)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1528b26a-6725-405f-a845-1300f4022626",
   "metadata": {},
   "source": [
    "![DLI Header](images/DLI_Header.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
