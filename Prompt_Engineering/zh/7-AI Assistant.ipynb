{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "347dc7ef-aace-48a0-8a23-ecd9861a4ca3",
   "metadata": {},
   "source": [
    "![DLI Header](images/DLI_Header.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c220782-b82e-48ea-873b-92a5530db930",
   "metadata": {},
   "source": [
    "# 星辰自行车 AI 助手\n",
    "｜Star Bikes AI Assistant"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4713e8c2-05c4-4815-a01f-c22340e868d3",
   "metadata": {},
   "source": [
    "在此笔记本中，您将创建一个 AI 助手，以帮助客户就从星辰自行车购买新自行车做出最佳决策。您还将简要了解您正在使用的模型的**令牌限制（token limits）** 及其对保留对话历史记录的影响。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17942686-f99d-4077-be2e-8d40977391a8",
   "metadata": {},
   "source": [
    "## 学习目标\n",
    "｜Learning Objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "013c3ef1-24e2-4545-bfcf-1cf7871d7d95",
   "metadata": {},
   "source": [
    "在完成此笔记本时，您将能够：\n",
    "- 解释**令牌限制** 及其对 LLM 行为的影响。\n",
    "- 构建一个能够（有限）记忆对话的 AI 助手，该助手不受设置的**令牌限制** 的影响。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ee9989c",
   "metadata": {},
   "source": [
    "## 视频演练"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85353ac8",
   "metadata": {},
   "source": [
    "执行下面的单元格以加载此笔记本的视频演练。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45cc9eab-80fa-48e2-891f-dcebf4c0c458",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    " from IPython.display import HTML\n",
    "\n",
    "video_url = \"https://d36m44n9vdbmda.cloudfront.net/assets/s-fx-12-v1/v2/07-assistant.mp4\"\n",
    "\n",
    "video_html = f\"\"\"\n",
    "<video controls width=\"640\" height=\"360\">\n",
    "    <source src=\"{video_url}\" type=\"video/mp4\">\n",
    "    Your browser does not support the video tag.\n",
    "</video>\n",
    "\"\"\"\n",
    "\n",
    "display(HTML(video_html))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "793cb321",
   "metadata": {},
   "source": [
    "## 创建 LLaMA-2 管道\n",
    "｜Create LLaMA-2 Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab62455a-b3bf-41eb-8499-b85dc053c31f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "model = \"TheBloke/Llama-2-13B-chat-GPTQ\"\n",
    "# model = \"TheBloke/Llama-2-7B-chat-GPTQ\"\n",
    "\n",
    "llama_pipe = pipeline(\"text-generation\", model=model, device_map=\"auto\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b3faff0-3616-4186-8436-6365d4ebcd55",
   "metadata": {},
   "source": [
    "## 获取 LLaMA-2 分词器\n",
    "｜Get LLaMA-2 Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fe3f005-1da2-4ae5-bf39-832aedf33a77",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfaf1de0-1019-4f34-a1ba-a44943ea4b48",
   "metadata": {},
   "source": [
    "## 辅助函数和类\n",
    "｜Helper Functions and Classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bc39d8e-4667-45a9-a4c7-ae0bba32ebe3",
   "metadata": {},
   "source": [
    "在此笔记本中，我们将使用以下函数和类来支持我们与 LLM 的交互。现在可以随意浏览一下，因为在下面使用时会更详细地介绍。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c77ceca2-1a25-4ede-8cf8-11f98dcd14e6",
   "metadata": {},
   "source": [
    "### 生成模型响应\n",
    "｜Generate Model Responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42343e1f-40d8-4b9a-a56c-53f60774dcd5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate(prompt, max_length=4096, pipe=llama_pipe, **kwargs):\n",
    "    \"\"\"\n",
    "    Generates a response to the given prompt using a specified language model pipeline.\n",
    "\n",
    "    This function takes a prompt and passes it to a language model pipeline, such as LLaMA, \n",
    "    to generate a text response. The function is designed to allow customization of the \n",
    "    generation process through various parameters and keyword arguments.\n",
    "\n",
    "    Parameters:\n",
    "    - prompt (str): The input text prompt to generate a response for.\n",
    "    - max_length (int): The maximum length of the generated response. Default is 1024 tokens.\n",
    "    - pipe (callable): The language model pipeline function used for generation. Default is llama_pipe.\n",
    "    - **kwargs: Additional keyword arguments that are passed to the pipeline function.\n",
    "\n",
    "    Returns:\n",
    "    - str: The generated text response from the model, trimmed of leading and trailing whitespace.\n",
    "\n",
    "    Example usage:\n",
    "    ```\n",
    "    prompt_text = \"Explain the theory of relativity.\"\n",
    "    response = generate(prompt_text, max_length=512, pipe=my_custom_pipeline, temperature=0.7)\n",
    "    print(response)\n",
    "    ```\n",
    "    \"\"\"\n",
    "\n",
    "    def_kwargs = dict(return_full_text=False, return_dict=False)\n",
    "    response = pipe(prompt.strip(), max_length=max_length, **kwargs, **def_kwargs)\n",
    "    return response[0]['generated_text'].strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeec8244-ebf1-4ffd-af1d-85224c9e3927",
   "metadata": {},
   "source": [
    "### 构造提示，可选地使用系统上下文和/或示例\n",
    "｜Costruct Prompt, Optionally With System Context and/or Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40abda7d-fffc-4afb-bbb9-922dea42057b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def construct_prompt_with_context(main_prompt, system_context=\"\", conversation_examples=[]):\n",
    "    \"\"\"\n",
    "    Constructs a complete structured prompt for a language model, including optional system context and conversation examples.\n",
    "\n",
    "    This function compiles a prompt that can be directly used for generating responses from a language model. \n",
    "    It creates a structured format that begins with an optional system context message, appends a series of conversational \n",
    "    examples as prior interactions, and ends with the main user prompt. If no system context or conversation examples are provided,\n",
    "    it will return only the main prompt.\n",
    "\n",
    "    Parameters:\n",
    "    - main_prompt (str): The core question or statement for the language model to respond to.\n",
    "    - system_context (str, optional): Additional context or information about the scenario or environment. Defaults to an empty string.\n",
    "    - conversation_examples (list of tuples, optional): Prior exchanges provided as context, where each tuple contains a user message \n",
    "      and a corresponding agent response. Defaults to an empty list.\n",
    "\n",
    "    Returns:\n",
    "    - str: A string formatted as a complete prompt ready for language model input. If no system context or examples are provided, returns the main prompt.\n",
    "\n",
    "    Example usage:\n",
    "    ```\n",
    "    main_prompt = \"I'm looking to improve my dialogue writing skills for my next short story. Any suggestions?\"\n",
    "    system_context = \"User is an aspiring author seeking to enhance dialogue writing techniques.\"\n",
    "    conversation_examples = [\n",
    "        (\"How can dialogue contribute to character development?\", \"Dialogue should reveal character traits and show personal growth over the story arc.\"),\n",
    "        (\"What are some common pitfalls in writing dialogue?\", \"Avoid exposition dumps in dialogue and make sure each character's voice is distinct.\")\n",
    "    ]\n",
    "\n",
    "    full_prompt = construct_prompt_with_context(main_prompt, system_context, conversation_examples)\n",
    "    print(full_prompt)\n",
    "    ```\n",
    "    \"\"\"\n",
    "    \n",
    "    # Return the main prompt if no system context or conversation examples are provided\n",
    "    if not system_context and not conversation_examples:\n",
    "        return main_prompt\n",
    "\n",
    "    # Start with the initial part of the prompt including the system context, if provided\n",
    "    full_prompt = f\"<s>[INST] <<SYS>>{system_context}<</SYS>>\\n\" if system_context else \"<s>[INST]\\n\"\n",
    "\n",
    "    # Add each example from the conversation_examples to the prompt\n",
    "    for user_msg, agent_response in conversation_examples:\n",
    "        full_prompt += f\"{user_msg} [/INST] {agent_response} </s><s>[INST]\"\n",
    "\n",
    "    # Add the main user prompt at the end\n",
    "    full_prompt += f\"{main_prompt} [/INST]\"\n",
    "\n",
    "    return full_prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9d3b17d-2b8b-49a3-ba12-d6b1a6b20cc9",
   "metadata": {},
   "source": [
    "### LlamaChatbot 类\n",
    "｜LlamaChatbot Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36e18ccd-72e4-40aa-a2fb-d8359fb5fda8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class LlamaChatbot:\n",
    "    \"\"\"\n",
    "    A chatbot interface for generating conversational responses using the LLaMA language model.\n",
    "\n",
    "    Attributes:\n",
    "    - system_context (str): Contextual information to provide to the language model for all conversations.\n",
    "    - conversation_history (list of tuples): Stores the history of the conversation, where each\n",
    "      tuple contains a user message and the corresponding agent response.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, system_context):\n",
    "        \"\"\"\n",
    "        Initializes a new instance of the LlamaChatbot class.\n",
    "\n",
    "        Parameters:\n",
    "        - system_context (str): A string that sets the initial context for the language model.\n",
    "        \"\"\"\n",
    "        self.system_context = system_context\n",
    "        self.conversation_history = []  # Initializes the conversation history\n",
    "\n",
    "    def chat(self, user_msg):\n",
    "        \"\"\"\n",
    "        Generates a response from the chatbot based on the user's message.\n",
    "\n",
    "        This method constructs a prompt with the current system context and conversation history,\n",
    "        sends it to the language model, and then stores the new user message and model's response\n",
    "        in the conversation history.\n",
    "\n",
    "        Parameters:\n",
    "        - user_msg (str): The user's message to which the chatbot will respond.\n",
    "\n",
    "        Returns:\n",
    "        - str: The generated response from the chatbot.\n",
    "        \"\"\"\n",
    "        # Generate the prompt using the conversation history and the new user message\n",
    "        prompt = construct_prompt_with_context(user_msg, self.system_context, self.conversation_history)\n",
    "        \n",
    "        # Get the model's response\n",
    "        agent_response = generate(prompt)\n",
    "\n",
    "        # Store this interaction in the conversation history\n",
    "        self.conversation_history.append((user_msg, agent_response))\n",
    "\n",
    "        return agent_response\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        Resets the conversation history of the chatbot.\n",
    "\n",
    "        This method clears the existing conversation history, effectively restarting the conversation.\n",
    "        \"\"\"\n",
    "        # Clear conversation history\n",
    "        self.conversation_history = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "607c5fd5-0c47-40e6-894b-b673e4957de1",
   "metadata": {},
   "source": [
    "### LlamaChatBotWithHistoryLimit 类\n",
    "｜LlamaChatBotWithHistoryLimit Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36917bf9-73e2-4580-a30e-7315e40b9a43",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class LlamaChatbotWithHistoryLimit:\n",
    "    \"\"\"\n",
    "    A chatbot interface for generating conversational responses using the LLaMA language model.\n",
    "\n",
    "    Attributes:\n",
    "        - system_context (str): Contextual information to provide to the language model for all conversations.\n",
    "        - conversation_history (list of tuples): Stores the history of the conversation, where each\n",
    "          tuple contains a user message and the corresponding agent response.\n",
    "        - tokenizer: The tokenizer used to tokenize the conversation for maintaining the history limit.\n",
    "        - max_tokens (int): The maximum number of tokens allowed in the conversation history.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, system_context, tokenizer, max_tokens=2048):\n",
    "        \"\"\"\n",
    "        Initializes a new instance of the LlamaChatbot class with a tokenizer and token limit.\n",
    "\n",
    "        Parameters:\n",
    "            - system_context (str): A string that sets the initial context for the language model.\n",
    "            - tokenizer: The tokenizer used to process the input and output for the language model.\n",
    "            - max_tokens (int): The maximum number of tokens to retain in the conversation history.\n",
    "        \"\"\"\n",
    "        self.system_context = system_context\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_tokens = max_tokens\n",
    "        self.conversation_history = []  # Initializes the conversation history\n",
    "\n",
    "    def chat(self, user_msg):\n",
    "        \"\"\"\n",
    "        Generates a response from the chatbot based on the user's message.\n",
    "\n",
    "        This method constructs a prompt with the current system context and conversation history,\n",
    "        sends it to the language model, and then stores the new user message and model's response\n",
    "        in the conversation history, ensuring that the history does not exceed the specified token limit.\n",
    "\n",
    "        Parameters:\n",
    "            - user_msg (str): The user's message to which the chatbot will respond.\n",
    "\n",
    "        Returns:\n",
    "            - str: The generated response from the chatbot.\n",
    "        \"\"\"\n",
    "        # Generate the prompt using the conversation history and the new user message\n",
    "        prompt = construct_prompt_with_context(user_msg, self.system_context, self.conversation_history)\n",
    "        \n",
    "        # Get the model's response\n",
    "        agent_response = generate(prompt)\n",
    "\n",
    "        # Store this interaction in the conversation history\n",
    "        self.conversation_history.append((user_msg, agent_response))\n",
    "\n",
    "        # Check and maintain the conversation history within the token limit\n",
    "        self._trim_conversation_history()\n",
    "\n",
    "        return agent_response\n",
    "\n",
    "    def _trim_conversation_history(self):\n",
    "        \"\"\"\n",
    "        Trims the conversation history to maintain the number of tokens below the specified limit.\n",
    "        \"\"\"\n",
    "        # Concatenate the conversation history into a single string\n",
    "        history_string = ''.join(user + agent for user, agent in self.conversation_history)\n",
    "        \n",
    "        # Calculate the number of tokens in the conversation history\n",
    "        history_tokens = len(self.tokenizer.encode(history_string))\n",
    "\n",
    "        # While the history exceeds the maximum token limit, remove the oldest items\n",
    "        while history_tokens > self.max_tokens:\n",
    "            # Always check if there's at least one item to pop\n",
    "            if self.conversation_history:\n",
    "                # Remove the oldest conversation tuple\n",
    "                self.conversation_history.pop(0)\n",
    "                # Recalculate the history string and its tokens\n",
    "                history_string = ''.join(user + agent for user, agent in self.conversation_history)\n",
    "                history_tokens = len(self.tokenizer.encode(history_string))\n",
    "            else:\n",
    "                # If the conversation history is empty, break out of the loop\n",
    "                break\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        Resets the conversation history of the chatbot.\n",
    "\n",
    "        This method clears the existing conversation history, effectively restarting the conversation.\n",
    "        \"\"\"\n",
    "        # Clear conversation history\n",
    "        self.conversation_history = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd12556-fb8e-4768-9044-811622cec89b",
   "metadata": {},
   "source": [
    "### 打印给定字符串中的令牌数\n",
    "｜Print Number of Tokens in a Given String"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f2f0379-07aa-4583-9b37-2995aa882ef0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def print_token_count(text, tokenizer):\n",
    "    \"\"\"\n",
    "    Calculate and return the number of tokens in a given text using a specified tokenizer.\n",
    "\n",
    "    This function takes a string of text and a tokenizer. It uses the tokenizer to encode the text\n",
    "    into tokens and then returns the count of these tokens.\n",
    "\n",
    "    Parameters:\n",
    "    - text (str): The input string to be tokenized.\n",
    "    - tokenizer: A tokenizer instance capable of encoding text into tokens.\n",
    "\n",
    "    Returns:\n",
    "    - int: The number of tokens in the input text as determined by the tokenizer.\n",
    "    \"\"\"\n",
    "    return len(tokenizer.encode(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2821401d-ac98-4b30-9f6f-70ce07b731ea",
   "metadata": {},
   "source": [
    "### 连接对话历史记录\n",
    "｜Concatenate Conversation History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ea30bb2-94e3-4f6d-94fd-d46e43645c9c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def concat_history(tuples_list):\n",
    "    \"\"\"\n",
    "    Concatenates texts from a list of 2-tuples.\n",
    "\n",
    "    Each tuple in the list is expected to contain two strings. The function\n",
    "    will concatenate all the first elements followed by all the second elements\n",
    "    in their respective order of appearance in the list.\n",
    "\n",
    "    Parameters:\n",
    "    - tuples_list (list of 2-tuples): A list where each element is a tuple of two strings.\n",
    "\n",
    "    Returns:\n",
    "    - str: A single string that is the result of concatenating all the texts from the tuples.\n",
    "\n",
    "    Example usage:\n",
    "    ```\n",
    "    conversation_tuples = [\n",
    "        ('Question 1', 'Answer 1'),\n",
    "        ('Question 2', 'Answer 2'),\n",
    "        ('Question 3', 'Answer 3')\n",
    "    ]\n",
    "\n",
    "    concatenated_text = concatenate_texts_from_tuples(conversation_tuples)\n",
    "    print(concatenated_text)\n",
    "    ```\n",
    "    \"\"\"\n",
    "    # Concatenate all the first and second elements of the tuples\n",
    "    return ''.join(question + response for question, response in tuples_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6781a436-aec7-47d6-83a3-3cccf373c978",
   "metadata": {},
   "source": [
    "## 数据\n",
    "｜Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef88361d-a326-4a8d-bd75-31742be6378b",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 星辰自行车详细信息\n",
    "｜Star Bikes Details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09ea1e03-4438-4bea-af72-e5a5fe2783ef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "bikes = [\n",
    "    {\n",
    "        \"model\": \"Galaxy Rider\",\n",
    "        \"type\": \"Mountain\",\n",
    "        \"features\": {\n",
    "            \"frame\": \"Aluminum alloy\",\n",
    "            \"gears\": \"21-speed Shimano\",\n",
    "            \"brakes\": \"Hydraulic disc\",\n",
    "            \"tires\": \"27.5-inch all-terrain\",\n",
    "            \"suspension\": \"Full, adjustable\",\n",
    "            \"color\": \"Matte black with green accents\"\n",
    "        },\n",
    "        \"usps\": [\"Lightweight frame\", \"Quick gear shift\", \"Durable tires\"],\n",
    "        \"price\": 799.95,\n",
    "        \"internal_id\": \"GR2321\",\n",
    "        \"weight\": \"15.3 kg\",\n",
    "        \"manufacturer_location\": \"Taiwan\"\n",
    "    },\n",
    "    {\n",
    "        \"model\": \"Nebula Navigator\",\n",
    "        \"type\": \"Hybrid\",\n",
    "        \"features\": {\n",
    "            \"frame\": \"Carbon fiber\",\n",
    "            \"gears\": \"18-speed Nexus\",\n",
    "            \"brakes\": \"Mechanical disc\",\n",
    "            \"tires\": \"26-inch city slick\",\n",
    "            \"suspension\": \"Front only\",\n",
    "            \"color\": \"Glossy white\"\n",
    "        },\n",
    "        \"usps\": [\"Sleek design\", \"Efficient on both roads and trails\", \"Ultra-lightweight\"],\n",
    "        \"price\": 649.99,\n",
    "        \"internal_id\": \"NN4120\",\n",
    "        \"weight\": \"13.5 kg\",\n",
    "        \"manufacturer_location\": \"Germany\"\n",
    "    },\n",
    "    {\n",
    "        \"model\": \"Cosmic Comet\",\n",
    "        \"type\": \"Road\",\n",
    "        \"features\": {\n",
    "            \"frame\": \"Titanium\",\n",
    "            \"gears\": \"24-speed Campagnolo\",\n",
    "            \"brakes\": \"Rim brakes\",\n",
    "            \"tires\": \"700C road\",\n",
    "            \"suspension\": \"None\",\n",
    "            \"color\": \"Metallic blue\"\n",
    "        },\n",
    "        \"usps\": [\"Super aerodynamic\", \"High-speed performance\", \"Professional-grade components\"],\n",
    "        \"price\": 1199.50,\n",
    "        \"internal_id\": \"CC5678\",\n",
    "        \"weight\": \"11 kg\",\n",
    "        \"manufacturer_location\": \"Italy\"\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ffb9670-9447-43bd-9f42-1c36adc9fe00",
   "metadata": {},
   "source": [
    "## 自行车 AI 助手\n",
    "｜Bikes AI Assistant"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70a5da85-b6d6-4c45-a9dd-117caa121a89",
   "metadata": {},
   "source": [
    "在本节中，我们将创建一个 AI 客户支持助手，以帮助潜在客户购买他们的下一辆星辰自行车。\n",
    "\n",
    "我们将首先设置一个合适的**系统上下文（system context）** 并实例化一个聊天机器人实例。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f807d3b9-00a0-428d-beb4-07e061fcc06d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "system_context = \"\"\"\n",
    "You are a friendly chatbot knowledgeable about bicycles. \\\n",
    "When asked about specific bike models or features, you try to provide accurate and helpful answers. \\\n",
    "Your goal is to assist and inform potential customers to the best of your ability in 50 words or less.\n",
    "\"\"\"\n",
    "\n",
    "chatbot = LlamaChatbot(system_context)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a340a315-10c6-465c-a027-1359ae45dfcc",
   "metadata": {},
   "source": [
    "让我们要求模型告诉我们有关最新自行车的更多信息。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80de7b39-d9e0-43db-906a-96fb33ce149f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(chatbot.chat(\"Can you tell me about the latest models?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cb5049d-7a9c-450c-85d9-cad7795c40e2",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a6963c6-fa77-4875-a2a9-6cf09dfc3755",
   "metadata": {},
   "source": [
    "这还不错，但当然我们希望助手告诉我们有关星辰自行车的型号！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "549fc0ac-9e17-477b-9f88-2cc49607d515",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "chatbot.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34f46541-c7c8-4f44-993f-21df834135b1",
   "metadata": {},
   "source": [
    "## 星辰自行车 AI 助手\n",
    "｜Star Bikes AI Assistant"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57f31c9b-d1d5-4dbc-81c4-c2999a007433",
   "metadata": {},
   "source": [
    "让我们创建一个新的聊天机器人，包括上面提供的`bikes`数据，以便它在对话期间参考。在以下**系统上下文** 中，我们为模型提供了一个**提示（cue）** ，让它始终以询问还能提供什么帮助来结束交换。这不仅对于 AI 助手来说是一个好主意，而且在实践中，可以防止模型无限期地继续，或者在只需要一个交换时尝试生成多个交换。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67cb3496-72a5-4f62-a170-0e246e62f101",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "system_context = f\"\"\"\n",
    "You are a friendly chatbot knowledgeable about these bicycles from Star Bikes {bikes}. \\\n",
    "When asked about specific bike models or features, you try to provide accurate and helpful answers. \\\n",
    "Your goal is to assist and inform potential customers to the best of your ability in 50 words or less. \\\n",
    "You always end by asking what else you can help with.\n",
    "\"\"\"\n",
    "\n",
    "chatbot = LlamaChatbot(system_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a291ed1b-caff-416f-a964-19ee773affe8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(chatbot.chat(\"Can you tell me about the latest models?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e6f4ca3-3dc5-453a-b6f7-b94c27981c07",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75bd4d51-3e6d-40e9-84b1-ff5cfd575134",
   "metadata": {},
   "source": [
    "这非常棒。让我们看看当被要求提供有关自行车的具体详细信息时，它会如何响应？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb9f4c4a-294c-4c2c-bbe0-263fde0c5eb6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(chatbot.chat(\"How much do each of the models cost?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d44016a-21e8-4bb5-bb69-a49b18e952cd",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1a0a2ca-0519-428f-9f78-bba5338e3c56",
   "metadata": {},
   "source": [
    "非常好。让我们看看它如何响应更模糊的查询。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb4deafd-9b62-4406-b7f4-8d9bf773dbd6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(chatbot.chat(\"I am more intersted in biking around town.\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a0e08ee-70ce-40bc-be89-b80efaf7d3da",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f966feaf-d013-4d2b-a53e-457ecbc10b65",
   "metadata": {},
   "source": [
    "总的来说，我们的助手似乎已经表现得相当出色。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c83c788c-e833-48a5-9696-4b9137cd3ed5",
   "metadata": {},
   "source": [
    "## 关于令牌数的考虑\n",
    "｜Considerations About Number of Tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b51933a1-e07c-4fa9-b836-60f27523da47",
   "metadata": {},
   "source": [
    "当我们将文本传递给像 LLaMA-2 这样的语言模型时，文本会被转换为**令牌（tokens）** ，即语言模型用于处理和生成文本的文本单元，例如单词或标点符号。\n",
    "\n",
    "像 LLaMA-2 这样的语言模型具有内在的**令牌限制（token limit）** ，即它们在一个提示-响应周期中可以处理的令牌数量的固定上限。这种限制是由于它们的设计以及处理令牌所需的计算资源造成的。在我们的 LLaMA-2 模型的情况下，**令牌限制** 设置为`4096`个令牌。给定模型的令牌限制可以通过其文档获得，但在其固有限制内也可以控制。当使用像我们这样的`transformers`管道时，我们使用`max_length`参数控制**令牌限制** 。\n",
    "\n",
    "内在的令牌限制或`max_length`参数（以较小者为准）决定了分配给*输入提示（input prompt）和模型输出（model's output）*的令牌总数。\n",
    "\n",
    "由于我们没有清除上面聊天交换中的聊天历史记录，因此让我们看一下当前`chatbot`实例的`conversation_history`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97255f11-7dd8-4fb8-bc63-32b7f5175e20",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(chatbot.conversation_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43394f98-71bd-4d45-8590-89220bda0809",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e22e3162-8e23-4725-b529-ac1b887b1873",
   "metadata": {},
   "source": [
    "为了支持获取所有这些字符串表示的**令牌** 数量，我们将使用上面定义的`concat_history`辅助函数来连接对话历史记录中的所有字符串。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b974926-4b7c-46ca-b291-a318fd3208f7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "conv_history = concat_history(chatbot.conversation_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd9ee2b0-e59e-4a6d-acc6-e06a4f60e4d3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(conv_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "456682a0-78fd-4e73-b228-0f44c2311197",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f8fed90-be75-43c6-9ec1-c92cf75aa46d",
   "metadata": {},
   "source": [
    "现在，我们将使用上面定义的另一个辅助函数`print_token_count`来**标记化（tokenize）** 我们的对话历史记录字符串，使用 LLaMA-2 的**分词器（tokenizer）** （如上导入）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d200a0fc-b1cc-4502-a18a-a4fdb235f0c2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print_token_count(conv_history, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "448d19d2-433f-4444-bf0d-9d965e76f76a",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c982e096-b692-4a85-8be0-d76b490749b9",
   "metadata": {},
   "source": [
    "让我们看看与聊天机器人的额外交换如何逐渐增加对话历史记录中的**令牌** 数量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "198b0214-25cc-4769-b66e-5223ad416913",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(chatbot.chat(\"What kind of bike would be best if I'm on a budget?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7edb140-6106-45fe-8f29-f2a5f66b4ae3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print_token_count(concat_history(chatbot.conversation_history), tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cfcca2a-854f-4dd6-86b2-3fefba87c734",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(chatbot.chat(\"What's the next most expensive bike after the Galaxy Rider?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ad11054-d79b-4bbd-9241-e0319a50e641",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print_token_count(concat_history(chatbot.conversation_history), tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85a24973-314a-423b-b382-32d8dbe823cb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(chatbot.chat(\"Why is titanium so good for a frame?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41c72ea7-2336-4320-a98e-bdd322ec51c7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print_token_count(concat_history(chatbot.conversation_history), tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10615ebc-4ff2-4bd1-a381-30b502b7cf56",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(chatbot.chat(\"Do you remember where I said I was most interested in riding?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9348ee5a-8e5c-49d9-87fd-3f7a50874fae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print_token_count(concat_history(chatbot.conversation_history), tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed953f4-1cfb-42c0-90e2-99f913c8c90a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(chatbot.chat(\"Can you please summarize our conversation for me?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95345a80-afd4-4187-8cdf-8fc8a774f84a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print_token_count(concat_history(chatbot.conversation_history), tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2108c8a9-d8cf-4c1c-a9d0-aced31bf9f3f",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a46c457-4f67-4afb-a482-00386b630878",
   "metadata": {},
   "source": [
    "为了结束此探索，我们将重置聊天机器人并再次打印令牌计数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98a5656e-fc22-4a7d-83e4-90bb8b41be04",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "chatbot.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8d76126-2cbe-42e1-b800-b83243a36dd4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print_token_count(concat_history(chatbot.conversation_history), tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "715bde14-8613-49a7-aa11-9dd454a83eac",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65263216-9d3a-4556-8d5d-7ecfa1a20c32",
   "metadata": {},
   "source": [
    "鉴于我们的聊天机器人实现通过在后续交换中将对话历史记录传递到提示中来存储先前的对话，因此，每次交换时，我们都在接近模型的**令牌限制** 。\n",
    "\n",
    "如上所述，我们正在使用的模型的内在**令牌限制** 是`4096`，如果您查看上面`generate`函数的定义，我们会将`4096`作为`max_length`参数传递。因此，我们还没有接近**令牌限制** ，但是，我们应该考虑如何确保此硬限制不会在使用我们的聊天机器人时造成问题。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac55f853-6bdd-4204-9eaf-20cae86e942c",
   "metadata": {},
   "source": [
    "## 限制聊天历史记录\n",
    "｜Limit Chat History"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "379d1801-499b-4866-a5df-d4007ea31053",
   "metadata": {},
   "source": [
    "下面是一个修改后的聊天类`LlamaChatbotWithHistoryLimit`。它接受`max_tokens`参数，以及一个将用于跟踪对话历史记录中存在的令牌数量的分词器。\n",
    "\n",
    "在对话历史记录将超过`max_tokens`的情况下，将调用`_trim_conversation_history`以弹出历史记录中最旧的对话，直到历史记录低于`max_tokens`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c2ee658-5985-48ff-8b87-f911c5f4b458",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class LlamaChatbotWithHistoryLimit:\n",
    "    \"\"\"\n",
    "    A chatbot interface for generating conversational responses using the LLaMA language model.\n",
    "\n",
    "    Attributes:\n",
    "        - system_context (str): Contextual information to provide to the language model for all conversations.\n",
    "        - conversation_history (list of tuples): Stores the history of the conversation, where each\n",
    "          tuple contains a user message and the corresponding agent response.\n",
    "        - tokenizer: The tokenizer used to tokenize the conversation for maintaining the history limit.\n",
    "        - max_tokens (int): The maximum number of tokens allowed in the conversation history.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, system_context, tokenizer, max_tokens=2048):\n",
    "        \"\"\"\n",
    "        Initializes a new instance of the LlamaChatbot class with a tokenizer and token limit.\n",
    "\n",
    "        Parameters:\n",
    "            - system_context (str): A string that sets the initial context for the language model.\n",
    "            - tokenizer: The tokenizer used to process the input and output for the language model.\n",
    "            - max_tokens (int): The maximum number of tokens to retain in the conversation history.\n",
    "        \"\"\"\n",
    "        self.system_context = system_context\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_tokens = max_tokens\n",
    "        self.conversation_history = []  # Initializes the conversation history\n",
    "\n",
    "    def chat(self, user_msg):\n",
    "        \"\"\"\n",
    "        Generates a response from the chatbot based on the user's message.\n",
    "\n",
    "        This method constructs a prompt with the current system context and conversation history,\n",
    "        sends it to the language model, and then stores the new user message and model's response\n",
    "        in the conversation history, ensuring that the history does not exceed the specified token limit.\n",
    "\n",
    "        Parameters:\n",
    "            - user_msg (str): The user's message to which the chatbot will respond.\n",
    "\n",
    "        Returns:\n",
    "            - str: The generated response from the chatbot.\n",
    "        \"\"\"\n",
    "        # Generate the prompt using the conversation history and the new user message\n",
    "        prompt = construct_prompt_with_context(user_msg, self.system_context, self.conversation_history)\n",
    "        \n",
    "        # Get the model's response\n",
    "        agent_response = generate(prompt)\n",
    "\n",
    "        # Store this interaction in the conversation history\n",
    "        self.conversation_history.append((user_msg, agent_response))\n",
    "\n",
    "        # Check and maintain the conversation history within the token limit\n",
    "        self._trim_conversation_history()\n",
    "\n",
    "        return agent_response\n",
    "\n",
    "    def _trim_conversation_history(self):\n",
    "        \"\"\"\n",
    "        Trims the conversation history to maintain the number of tokens below the specified limit.\n",
    "        \"\"\"\n",
    "        # Concatenate the conversation history into a single string\n",
    "        history_string = ''.join(user + agent for user, agent in self.conversation_history)\n",
    "        \n",
    "        # Calculate the number of tokens in the conversation history\n",
    "        history_tokens = len(self.tokenizer.encode(history_string))\n",
    "\n",
    "        # While the history exceeds the maximum token limit, remove the oldest items\n",
    "        while history_tokens > self.max_tokens:\n",
    "            # Always check if there's at least one item to pop\n",
    "            if self.conversation_history:\n",
    "                # Remove the oldest conversation tuple\n",
    "                self.conversation_history.pop(0)\n",
    "                # Recalculate the history string and its tokens\n",
    "                history_string = ''.join(user + agent for user, agent in self.conversation_history)\n",
    "                history_tokens = len(self.tokenizer.encode(history_string))\n",
    "            else:\n",
    "                # If the conversation history is empty, break out of the loop\n",
    "                break\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        Resets the conversation history of the chatbot.\n",
    "\n",
    "        This method clears the existing conversation history, effectively restarting the conversation.\n",
    "        \"\"\"\n",
    "        # Clear conversation history\n",
    "        self.conversation_history = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96740912-962e-46d5-8d1c-c6539b4e026d",
   "metadata": {},
   "source": [
    "让我们创建一个新的聊天机器人实例，这次使用`max_tokens`限制为`200`个令牌。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dcdd33e-20ed-4a2c-b75e-a9683a0fffce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "system_context = f\"\"\"\n",
    "You are a friendly chatbot knowledgeable about these bicycles from Star Bikes {bikes}. \\\n",
    "When asked about specific bike models or features, you try to provide accurate and helpful answers. \\\n",
    "Your goal is to assist and inform potential customers to the best of your ability in 50 words or less. \\\n",
    "You always end by asking what else you can help with.\n",
    "\"\"\"\n",
    "\n",
    "chatbot = LlamaChatbotWithHistoryLimit(system_context, tokenizer=tokenizer, max_tokens=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc914310-ea45-42f8-93c9-a5daa9c9d7d0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(chatbot.chat(\"Can you tell me about the latest models?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15746b3b-865d-42f0-83ed-a2ad99b27929",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76b747ce-4c49-497d-96cd-2f1e57040a20",
   "metadata": {},
   "source": [
    "我们将运行更多几次交换，并跟踪对话历史记录中的令牌数量。请记住，我们已将`max_tokens`设置为`200`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce146ee1-5554-41d8-a9e9-2b28f687fe2b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print_token_count(concat_history(chatbot.conversation_history), tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "895965d9-469d-4aa8-a039-1daf29e426c5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(chatbot.chat(\"How much do each of the models cost?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c39882df-017f-43b6-9f82-fecd421e477b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print_token_count(concat_history(chatbot.conversation_history), tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7c8fe2b-b4a9-430d-bfb3-295a05af8af4",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d63d4685-f4cb-4325-9f38-7ec21d0c1796",
   "metadata": {},
   "source": [
    "您可以看到令牌计数已减少到`96`，以防止我们超过指定的`200`个令牌限制。让我们观察更多几轮对话。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b387aca-23d9-4ef9-8271-4b58cc7466a2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(chatbot.chat(\"I am more intersted in biking around town.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "243c322b-c52a-43f4-87e2-87716c90f073",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print_token_count(concat_history(chatbot.conversation_history), tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "212308d0-07be-4ba0-9697-56511599037e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(chatbot.chat(\"What kind of bike would be best if I'm on a budget?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15b687d7-efbf-4b44-b719-18ffda87cacc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print_token_count(concat_history(chatbot.conversation_history), tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e87b6c15-bf5b-4b8d-a55d-ae9a0abfe611",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06765c5b-faef-4171-8b7b-66e847f0cf94",
   "metadata": {},
   "source": [
    "我们的聊天机器人已成功弹出早期的对话轮次以避免超过限制。\n",
    "\n",
    "当然，作为这种故障安全行为的交换，我们牺牲了对话历史记录的完美保留。在这里我们可以看到，与之前不同，当我们要求总结迄今为止的对话时，我们只收到了我们最近几次交换的摘要。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f965aa16-54d0-412f-9829-880967bc4557",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(chatbot.chat(\"Can you summarize our conversation?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82f0382b-c259-495d-827b-abf508d44d21",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print_token_count(concat_history(chatbot.conversation_history), tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b592e4d1-97ea-4b61-9002-8e34f0ce026e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "chatbot.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "291c137b-9817-49e0-a624-70d7020d5f47",
   "metadata": {},
   "source": [
    "## 最终练习：为自己的虚构公司创建 AI 助手\n",
    "｜Final Exercise: Create an AI Assistant for Your Own Fictitious Company"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c4d9001-947c-4c39-9d02-a2f463aaad41",
   "metadata": {},
   "source": [
    "利用您迄今为止学到的所有知识，为一个您选择的虚构公司创建一个 AI 助手。您的工作将包括几个主要步骤。\n",
    "\n",
    "1) 想出一个公司的创意，包括其名称和将要销售的产品。\n",
    "2) 使用我们的 LLaMA-2 模型为您的公司将要销售的产品生成合成数据。请参阅上面的“星辰自行车详细信息”部分或`bikes`字典作为示例。如果您在生成合成 JSON 数据时遇到困难，请参阅笔记本*3-Review Analyst.ipynb*。\n",
    "3) 创建 AI 助手，并为其提供您在上一步中生成的合成数据。您可以随意使用此笔记本中的`LlamaChatbotWithHistoryLimit`类。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "881eec26-11f0-4880-9c97-12c57bf29e39",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 关键概念回顾\n",
    "｜Key Concept Review"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ba8e70d-29c3-4aeb-8e7b-b467b1b746ba",
   "metadata": {},
   "source": [
    "本笔记本中介绍了以下关键概念：\n",
    "\n",
    "- **令牌（Token）：** 语言模型用于处理的文本片段，例如单词或标点符号。\n",
    "- **令牌限制（Token Limit）：** 语言模型在一个提示中可以处理的最大令牌数。\n",
    "- **分词器（Tokenizer）：** 一种将文本转换为令牌以供语言模型理解的工具。\n",
    "\n",
    "- **Token:** A piece of text, like a word or punctuation, used by language models for processing.\n",
    "- **Token Limit:** The maximum number of tokens a language model can process in a single prompt.\n",
    "- **Tokenizer:** A tool that converts text into tokens for language models to understand."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a22d6363",
   "metadata": {},
   "source": [
    "## 重新启动内核\n",
    "｜Restart the Kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cb91dab",
   "metadata": {},
   "source": [
    "为了释放下一个笔记本的 GPU 内存，请运行以下单元格以重新启动内核。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e333754-31d1-4d45-8c8f-a8db78466599",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from IPython import get_ipython\n",
    "\n",
    "get_ipython().kernel.do_shutdown(restart=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4ad3b8a",
   "metadata": {},
   "source": [
    "# [5LOI DEEP LEARNING INSTITUE](https://5loi.com)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "895b607f",
   "metadata": {},
   "source": [
    "### [AI COMMUNITY](https://www.theforage.cn/community)\n",
    "\n",
    "### [5LOI](https://5loi.com/about_loi)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4aa7125-186a-4d07-b3e8-75b779f944c3",
   "metadata": {},
   "source": [
    "![DLI Header](images/DLI_Header.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
