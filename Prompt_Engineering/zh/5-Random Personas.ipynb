{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1d81d68e-e246-4374-b4f9-02929a8e6696",
   "metadata": {},
   "source": [
    "![DLI Header](images/DLI_Header.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "706e8c2b-5f1b-4b7b-9ece-b6b341e8b49d",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 随机人物\n",
    "｜Random Personas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52e22814-a66d-4646-9966-9843a433ce70",
   "metadata": {},
   "source": [
    "在此笔记本中，您将学习如何控制模型生成的**温度** ，从而使您能够控制其响应的随机程度。利用**温度** ，您将能够创建各种 AI 个性。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ff739fa-f350-4b50-b3ad-d19fbefd4e6f",
   "metadata": {},
   "source": [
    "## 学习目标"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daefff31-716d-42f7-8362-26a10145727d",
   "metadata": {},
   "source": [
    "在完成此笔记本时，您将能够：\n",
    "- 解释随机**采样（sampling）** 如何导致非确定性 LLM 响应。\n",
    "- 通过调整**温度（temperature）** 来控制响应的随机程度。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3028817",
   "metadata": {},
   "source": [
    "## 视频演练"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "586f2867",
   "metadata": {},
   "source": [
    "执行下面的单元格以加载此笔记本的视频演练。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4ab4a4a-82bf-4eb2-95cb-cc37f06b7469",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "\n",
    "video_url = \"https://d36m44n9vdbmda.cloudfront.net/assets/s-fx-12-v1/v2/05-random.mp4\"\n",
    "\n",
    "video_html = f\"\"\"\n",
    "<video controls width=\"640\" height=\"360\">\n",
    "    <source src=\"{video_url}\" type=\"video/mp4\">\n",
    "    Your browser does not support the video tag.\n",
    "</video>\n",
    "\"\"\"\n",
    "\n",
    "display(HTML(video_html))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44272604-6cca-4cb4-b19d-2b8f415ebd74",
   "metadata": {},
   "source": [
    "## 创建 LLaMA-2 管道\n",
    "｜Create LLaMA-2 Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48c28974-d576-4886-ab0b-061659187313",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "model = \"TheBloke/Llama-2-13B-chat-GPTQ\"\n",
    "# model = \"TheBloke/Llama-2-7B-chat-GPTQ\"\n",
    "\n",
    "llama_pipe = pipeline(\"text-generation\", model=model, device_map=\"auto\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "984b612f-5ce9-4e47-ab25-cbe946164506",
   "metadata": {},
   "source": [
    "## 辅助函数"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4540012d-1d17-44e2-af7e-a5ff0c587101",
   "metadata": {},
   "source": [
    "在此笔记本中，我们将使用以下函数来支持我们与 LLM 的交互。现在可以随意浏览一下，因为在下面使用时会更详细地介绍。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76923260-7338-4380-9fd7-d3c7756d651e",
   "metadata": {},
   "source": [
    "### 生成模型响应\n",
    "｜Generate Model Responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5148dc93-f5bf-4242-b916-40087f2273c4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate(prompt, max_length=1024, pipe=llama_pipe, **kwargs):\n",
    "    \"\"\"\n",
    "    Generates a response to the given prompt using a specified language model pipeline.\n",
    "\n",
    "    This function takes a prompt and passes it to a language model pipeline, such as LLaMA, \n",
    "    to generate a text response. The function is designed to allow customization of the \n",
    "    generation process through various parameters and keyword arguments.\n",
    "\n",
    "    Parameters:\n",
    "    - prompt (str): The input text prompt to generate a response for.\n",
    "    - max_length (int): The maximum length of the generated response. Default is 1024 tokens.\n",
    "    - pipe (callable): The language model pipeline function used for generation. Default is llama_pipe.\n",
    "    - **kwargs: Additional keyword arguments that are passed to the pipeline function.\n",
    "\n",
    "    Returns:\n",
    "    - str: The generated text response from the model, trimmed of leading and trailing whitespace.\n",
    "\n",
    "    Example usage:\n",
    "    ```\n",
    "    prompt_text = \"Explain the theory of relativity.\"\n",
    "    response = generate(prompt_text, max_length=512, pipe=my_custom_pipeline, temperature=0.7)\n",
    "    print(response)\n",
    "    ```\n",
    "    \"\"\"\n",
    "\n",
    "    def_kwargs = dict(return_full_text=False, return_dict=False)\n",
    "    response = pipe(prompt.strip(), max_length=max_length, **kwargs, **def_kwargs)\n",
    "    return response[0]['generated_text'].strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f120b024-f651-4889-891c-75994ee196e7",
   "metadata": {},
   "source": [
    "### 构造提示，可选地使用系统上下文和/或示例\n",
    "｜Costruct Prompt, Optionally With System Context and/or Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d208aa65-1a47-4f96-be1a-7e98b4ba0050",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def construct_prompt_with_context(main_prompt, system_context=\"\", conversation_examples=[]):\n",
    "    \"\"\"\n",
    "    Constructs a complete structured prompt for a language model, including optional system context and conversation examples.\n",
    "\n",
    "    This function compiles a prompt that can be directly used for generating responses from a language model. \n",
    "    It creates a structured format that begins with an optional system context message, appends a series of conversational \n",
    "    examples as prior interactions, and ends with the main user prompt. If no system context or conversation examples are provided,\n",
    "    it will return only the main prompt.\n",
    "\n",
    "    Parameters:\n",
    "    - main_prompt (str): The core question or statement for the language model to respond to.\n",
    "    - system_context (str, optional): Additional context or information about the scenario or environment. Defaults to an empty string.\n",
    "    - conversation_examples (list of tuples, optional): Prior exchanges provided as context, where each tuple contains a user message \n",
    "      and a corresponding agent response. Defaults to an empty list.\n",
    "\n",
    "    Returns:\n",
    "    - str: A string formatted as a complete prompt ready for language model input. If no system context or examples are provided, returns the main prompt.\n",
    "\n",
    "    Example usage:\n",
    "    ```\n",
    "    main_prompt = \"I'm looking to improve my dialogue writing skills for my next short story. Any suggestions?\"\n",
    "    system_context = \"User is an aspiring author seeking to enhance dialogue writing techniques.\"\n",
    "    conversation_examples = [\n",
    "        (\"How can dialogue contribute to character development?\", \"Dialogue should reveal character traits and show personal growth over the story arc.\"),\n",
    "        (\"What are some common pitfalls in writing dialogue?\", \"Avoid exposition dumps in dialogue and make sure each character's voice is distinct.\")\n",
    "    ]\n",
    "\n",
    "    full_prompt = construct_prompt_with_context(main_prompt, system_context, conversation_examples)\n",
    "    print(full_prompt)\n",
    "    ```\n",
    "    \"\"\"\n",
    "    \n",
    "    # Return the main prompt if no system context or conversation examples are provided\n",
    "    if not system_context and not conversation_examples:\n",
    "        return main_prompt\n",
    "\n",
    "    # Start with the initial part of the prompt including the system context, if provided\n",
    "    full_prompt = f\"<s>[INST] <<SYS>>{system_context}<</SYS>>\\n\" if system_context else \"<s>[INST]\\n\"\n",
    "\n",
    "    # Add each example from the conversation_examples to the prompt\n",
    "    for user_msg, agent_response in conversation_examples:\n",
    "        full_prompt += f\"{user_msg} [/INST] {agent_response} </s><s>[INST]\"\n",
    "\n",
    "    # Add the main user prompt at the end\n",
    "    full_prompt += f\"{main_prompt} [/INST]\"\n",
    "\n",
    "    return full_prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8acd78c2-d700-4a71-baea-b1e4b76ad80c",
   "metadata": {},
   "source": [
    "## 默认非随机响应\n",
    "｜Default Non-random Responses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94a535af-43af-4d04-b8d1-ba2196161302",
   "metadata": {},
   "source": [
    "您可能已经注意到，在我们之前的笔记本中，我们从 LLaMA-2 模型获得的响应，假设我们没有编辑提示，是确定性的。让我们明确地看一下这一点。\n",
    "\n",
    "在这里，我们将使用我们的模型执行另一个生成任务，这次是生成他们 Galaxy Rider 山地自行车的虚构客户体验。在这里，我们将**系统上下文** 设置为适当的值，并提示模型回忆起他们骑自行车时难忘的一天。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d9ddb17-e619-4947-b0ba-0ade00a8c5ae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "system_context = \"\"\"\n",
    "You are a perennially satisfied customer who loves to reminisce about personal experiences with products. \\\n",
    "You never delve into technical specifics, as you believe it's the emotion and the joy that matter most. \\\n",
    "You're excited for others to feel the same euphoria and happiness you do. Your aim isn't to advertise, \\\n",
    "but to share a genuine, heartfelt story of joy and contentment.\n",
    "\"\"\"\n",
    "\n",
    "prompt = \"Recall a memorable day out with your Galaxy Rider mountain bike in 50 words or less.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7983d5e7-ab19-445e-86c2-81fe48cce6d6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(generate(construct_prompt_with_context(prompt, system_context)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5294b61-adcd-4a42-b813-533226809d70",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9efbde1d-9ba0-4a27-8091-a185236043dd",
   "metadata": {},
   "source": [
    "让我们使用完全相同的提示再生成几次响应。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3faea14c-9bb3-41f0-90db-fda38da3c874",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(generate(construct_prompt_with_context(prompt, system_context)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6f0a1c2-39e5-48c2-9a03-8578ebf6afbd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(generate(construct_prompt_with_context(prompt, system_context)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "117165bb-fbd6-494d-9f52-8d5f2138e141",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aef795d9-65d2-4be0-8d14-432cf7b6b128",
   "metadata": {},
   "source": [
    "如上所述，模型每次都生成完全相同的响应。对于许多场景，这正是我们想要的行为，但对于其他场景，我们希望在模型的响应中引入一定程度的随机性。为了实现这一点，我们将修改模型响应的**温度** 。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1bdc89d-281b-46f5-8f29-becd0359c0b0",
   "metadata": {},
   "source": [
    "## 采样和温度\n",
    "｜Sampling and Temperature"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2a5b4e0-67a3-490f-aff7-1882ba4854f6",
   "metadata": {},
   "source": [
    "在语言模型领域，**采样（sampling）** 是指模型通过对潜在下一个单词（实际上是标记（tokens），但出于我们的目的，我们可以将标记视为单词）的概率分布进行采样来生成文本的过程。当我们在没有启用**采样** 的情况下与语言模型交互时，模型以确定性方式运行，在生成文本时始终选择最可能的下一个单词。当您需要一致性和准确性时，这种默认行为很有用，但当您追求创造性和多样化的响应时，它可能会受到限制。\n",
    "\n",
    "\n",
    "在用于与我们的 LLaMA-2 模型交互的`transformers`管道上下文中，默认情况下**采样** 是*禁用的*。要启用采样，我们在调用我们的`generate`函数时设置`do_sample=True`，并在这样做时，指示模型根据概率分布选择单词，允许选择可能性较低的单词，这可能导致更具多样性和趣味性的文本。\n",
    "\n",
    "\n",
    "启用**采样** 后，我们还可以为**温度（temperature）** 传递特定的值，您可以将其视为响应的随机程度。对于`temperature`，我们传递一个介于`0.0`和`1.0`之间的值，较大的值表示较大的随机程度。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d9d9a51-d5ac-4fe5-a736-5155dfa9a08d",
   "metadata": {},
   "source": [
    "## 练习：随机响应\n",
    "｜Exercise: Random Responses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19d6d682-07d0-4674-87a9-20cc840ec59d",
   "metadata": {},
   "source": [
    "重用与上面相同的**系统上下文** 和提示，启用**采样** （`do_sample=True`）并将**温度** 设置为其最高值（`temperature=1.0`）。生成 3 个不同的响应以检查它们是否都唯一。\n",
    "\n",
    "如果您遇到困难，请查看下面的解决方案。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c1eb2ab-b768-4a87-aeb3-8e1e468058a8",
   "metadata": {},
   "source": [
    "## 您的工作在此处"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1929355e-85a0-4af2-9c67-90b6fefd754e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8244d459-7fdb-4737-8cd7-f2077de152c9",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 解决方案"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b65b2d-fdb6-4c65-8836-e8023cac56a6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(generate(construct_prompt_with_context(prompt, system_context), do_sample=True, temperature=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b1fda59-8421-4abe-8865-1d20daa43723",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(generate(construct_prompt_with_context(prompt, system_context), do_sample=True, temperature=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01ad85e2-13f3-40fc-91d6-e8106563661c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(generate(construct_prompt_with_context(prompt, system_context), do_sample=True, temperature=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4e43770-6d77-4eb7-9288-1cafe2cc1d7b",
   "metadata": {},
   "source": [
    "## 创造力和准确性\n",
    "｜Creativity and Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dc45442-ba4d-46ea-bf6b-b2ca4ec24270",
   "metadata": {},
   "source": [
    "作为对`temperature`的最后思考，值得一提的是，随机生成虽然在独特或创造性输出的上下文中很有用，但与响应的精确性和准确性并不特别一致。在需要模型生成精确和/或事实准确响应的上下文中，在提高温度时始终注意检查其输出。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "425d181a-1d5c-4c3a-87a6-fb454c2acf85",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 关键概念回顾"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc0a945f-30f6-4db1-9eb5-60dcc9d3d1bd",
   "metadata": {},
   "source": [
    "本笔记本中介绍了以下关键概念：\n",
    "\n",
    "- **采样（Sampling）：** 文本生成（text generation）中的一种过程，其中语言模型（language model）根据词汇表上的概率分布（probability distribution）选择下一个单词（token）。\n",
    "- **温度（Temperature）：** 一个超参数（hyperparameter），它控制模型在采样过程（sampling）中预测的随机程度（level of randomness）。较高的温度（temperature）会增加多样性（diversity），从而导致更具变化性和创造性的响应（varied and creative responses），而较低的温度会使模型的输出更具可预测性（predictable）和保守性（conservative）。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "193d3820-c0a8-497e-8dc0-fa741c1fba93",
   "metadata": {},
   "source": [
    "## 可选的高级练习"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "119a1b5a-4485-4f72-9c60-6257844367b3",
   "metadata": {},
   "source": [
    "如果您想超越课程的要求，以下是一些供您尝试的额外开放式练习。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa76f625-0299-4809-8d9d-c59896207cfe",
   "metadata": {},
   "source": [
    "### 使用 7B 模型"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5016a362-dd54-413e-9c50-fd89a91d86ca",
   "metadata": {},
   "source": [
    "在笔记本的顶部，在重新启动内核（请参见下面的单元格）之后，取消注释并使用 7B 模型而不是我们演示的 13B 模型。尝试在使用较小（较弱）模型的情况下获得令人满意的结果。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17909f3d-49de-42d4-861f-2a419ea404b8",
   "metadata": {},
   "source": [
    "### 创建交互式角色"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27b804fb-87f2-4448-b148-500b00158bda",
   "metadata": {},
   "source": [
    "现在您能够从虚构人物那里生成陈述，请尝试创建一个小型系统，该系统创建多个不同的个性并让他们相互交互。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c32df389-b99f-4b57-ab15-dc17556d87ed",
   "metadata": {},
   "source": [
    "### 让交互式角色玩游戏"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff82d544-edc3-4e3a-86bf-2cd180254aa2",
   "metadata": {},
   "source": [
    "扩展之前的练习，创建多个角色，他们朝着您创建的某个“游戏”定义的目标努力。这可能是试图让另一个角色说出某个特定的词语，泄露他们隐藏的宝藏的秘密位置，甚至是一些合作性的东西，角色必须共同努力才能实现某个目标。如果您真的想挑战一下，您甚至可以考虑创建两个以上的角色，甚至创建相互交互的玩家团队。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f25f4da",
   "metadata": {},
   "source": [
    "## 重新启动内核\n",
    "｜Restart the Kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7518918",
   "metadata": {},
   "source": [
    "为了释放下一个笔记本的 GPU 内存，请运行以下单元格以重新启动内核。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f8de2be-7716-4c23-a06f-7f8364e62bab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from IPython import get_ipython\n",
    "\n",
    "get_ipython().kernel.do_shutdown(restart=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79cb91cc",
   "metadata": {},
   "source": [
    "# [5LOI DEEP LEARNING INSTITUE](https://5loi.com)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac866a93",
   "metadata": {},
   "source": [
    "### [AI COMMUNITY](https://www.theforage.cn/community)\n",
    "\n",
    "### [5LOI](https://5loi.com/about_loi)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dab090e-9452-42f6-8268-436c2f89b46b",
   "metadata": {},
   "source": [
    "![DLI Header](images/DLI_Header.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
