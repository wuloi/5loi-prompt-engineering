{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "347dc7ef-aace-48a0-8a23-ecd9861a4ca3",
   "metadata": {},
   "source": [
    "![DLI Header](images/DLI_Header.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c220782-b82e-48ea-873b-92a5530db930",
   "metadata": {},
   "source": [
    "# Star Bikes AI Assistant"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4713e8c2-05c4-4815-a01f-c22340e868d3",
   "metadata": {},
   "source": [
    "In this notebook you'll make an AI assistant to help customers make the best decision about getting a new bike from Star Bikes. You will also take a short dive into **token limits** for the model you are working with, and its impact on retaining conversation history."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17942686-f99d-4077-be2e-8d40977391a8",
   "metadata": {},
   "source": [
    "## Learning Objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "013c3ef1-24e2-4545-bfcf-1cf7871d7d95",
   "metadata": {},
   "source": [
    "By the time you complete this notebook you will be able to:\n",
    "- Explain **token limits** and their impact on LLM behavior.\n",
    "- Build an AI assistant capable of (limited) conversation memory that is not subject to exceeding a set **token limit**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "049a06a3-0eca-43c9-811c-c49677cd7ac5",
   "metadata": {},
   "source": [
    "## Video Walkthrough"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94853247-1450-47a8-81f0-3ae7c46e0b20",
   "metadata": {},
   "source": [
    "Execute the cell below to load the video walkthrough of this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45cc9eab-80fa-48e2-891f-dcebf4c0c458",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    " from IPython.display import HTML\n",
    "\n",
    "video_url = \"https://d36m44n9vdbmda.cloudfront.net/assets/s-fx-12-v1/v2/07-assistant.mp4\"\n",
    "\n",
    "video_html = f\"\"\"\n",
    "<video controls width=\"640\" height=\"360\">\n",
    "    <source src=\"{video_url}\" type=\"video/mp4\">\n",
    "    Your browser does not support the video tag.\n",
    "</video>\n",
    "\"\"\"\n",
    "\n",
    "display(HTML(video_html))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63f19484-acf2-4669-bbca-298c1c512bd7",
   "metadata": {},
   "source": [
    "## Create LLaMA-2 Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab62455a-b3bf-41eb-8499-b85dc053c31f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "model = \"TheBloke/Llama-2-13B-chat-GPTQ\"\n",
    "# model = \"TheBloke/Llama-2-7B-chat-GPTQ\"\n",
    "\n",
    "llama_pipe = pipeline(\"text-generation\", model=model, device_map=\"auto\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b3faff0-3616-4186-8436-6365d4ebcd55",
   "metadata": {},
   "source": [
    "## Get LLaMA-2 Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fe3f005-1da2-4ae5-bf39-832aedf33a77",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfaf1de0-1019-4f34-a1ba-a44943ea4b48",
   "metadata": {},
   "source": [
    "## Helper Functions and Classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bc39d8e-4667-45a9-a4c7-ae0bba32ebe3",
   "metadata": {},
   "source": [
    "In this notebook we will use the following functions and classes to support our interaction with the LLM. Feel free to skim over them presently, as they are covered in greater detail when used below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c77ceca2-1a25-4ede-8cf8-11f98dcd14e6",
   "metadata": {},
   "source": [
    "### Generate Model Responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42343e1f-40d8-4b9a-a56c-53f60774dcd5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate(prompt, max_length=4096, pipe=llama_pipe, **kwargs):\n",
    "    \"\"\"\n",
    "    Generates a response to the given prompt using a specified language model pipeline.\n",
    "\n",
    "    This function takes a prompt and passes it to a language model pipeline, such as LLaMA, \n",
    "    to generate a text response. The function is designed to allow customization of the \n",
    "    generation process through various parameters and keyword arguments.\n",
    "\n",
    "    Parameters:\n",
    "    - prompt (str): The input text prompt to generate a response for.\n",
    "    - max_length (int): The maximum length of the generated response. Default is 1024 tokens.\n",
    "    - pipe (callable): The language model pipeline function used for generation. Default is llama_pipe.\n",
    "    - **kwargs: Additional keyword arguments that are passed to the pipeline function.\n",
    "\n",
    "    Returns:\n",
    "    - str: The generated text response from the model, trimmed of leading and trailing whitespace.\n",
    "\n",
    "    Example usage:\n",
    "    ```\n",
    "    prompt_text = \"Explain the theory of relativity.\"\n",
    "    response = generate(prompt_text, max_length=512, pipe=my_custom_pipeline, temperature=0.7)\n",
    "    print(response)\n",
    "    ```\n",
    "    \"\"\"\n",
    "\n",
    "    def_kwargs = dict(return_full_text=False, return_dict=False)\n",
    "    response = pipe(prompt.strip(), max_length=max_length, **kwargs, **def_kwargs)\n",
    "    return response[0]['generated_text'].strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeec8244-ebf1-4ffd-af1d-85224c9e3927",
   "metadata": {},
   "source": [
    "### Costruct Prompt, Optionally With System Context and/or Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40abda7d-fffc-4afb-bbb9-922dea42057b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def construct_prompt_with_context(main_prompt, system_context=\"\", conversation_examples=[]):\n",
    "    \"\"\"\n",
    "    Constructs a complete structured prompt for a language model, including optional system context and conversation examples.\n",
    "\n",
    "    This function compiles a prompt that can be directly used for generating responses from a language model. \n",
    "    It creates a structured format that begins with an optional system context message, appends a series of conversational \n",
    "    examples as prior interactions, and ends with the main user prompt. If no system context or conversation examples are provided,\n",
    "    it will return only the main prompt.\n",
    "\n",
    "    Parameters:\n",
    "    - main_prompt (str): The core question or statement for the language model to respond to.\n",
    "    - system_context (str, optional): Additional context or information about the scenario or environment. Defaults to an empty string.\n",
    "    - conversation_examples (list of tuples, optional): Prior exchanges provided as context, where each tuple contains a user message \n",
    "      and a corresponding agent response. Defaults to an empty list.\n",
    "\n",
    "    Returns:\n",
    "    - str: A string formatted as a complete prompt ready for language model input. If no system context or examples are provided, returns the main prompt.\n",
    "\n",
    "    Example usage:\n",
    "    ```\n",
    "    main_prompt = \"I'm looking to improve my dialogue writing skills for my next short story. Any suggestions?\"\n",
    "    system_context = \"User is an aspiring author seeking to enhance dialogue writing techniques.\"\n",
    "    conversation_examples = [\n",
    "        (\"How can dialogue contribute to character development?\", \"Dialogue should reveal character traits and show personal growth over the story arc.\"),\n",
    "        (\"What are some common pitfalls in writing dialogue?\", \"Avoid exposition dumps in dialogue and make sure each character's voice is distinct.\")\n",
    "    ]\n",
    "\n",
    "    full_prompt = construct_prompt_with_context(main_prompt, system_context, conversation_examples)\n",
    "    print(full_prompt)\n",
    "    ```\n",
    "    \"\"\"\n",
    "    \n",
    "    # Return the main prompt if no system context or conversation examples are provided\n",
    "    if not system_context and not conversation_examples:\n",
    "        return main_prompt\n",
    "\n",
    "    # Start with the initial part of the prompt including the system context, if provided\n",
    "    full_prompt = f\"<s>[INST] <<SYS>>{system_context}<</SYS>>\\n\" if system_context else \"<s>[INST]\\n\"\n",
    "\n",
    "    # Add each example from the conversation_examples to the prompt\n",
    "    for user_msg, agent_response in conversation_examples:\n",
    "        full_prompt += f\"{user_msg} [/INST] {agent_response} </s><s>[INST]\"\n",
    "\n",
    "    # Add the main user prompt at the end\n",
    "    full_prompt += f\"{main_prompt} [/INST]\"\n",
    "\n",
    "    return full_prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9d3b17d-2b8b-49a3-ba12-d6b1a6b20cc9",
   "metadata": {},
   "source": [
    "### LlamaChatbot Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36e18ccd-72e4-40aa-a2fb-d8359fb5fda8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class LlamaChatbot:\n",
    "    \"\"\"\n",
    "    A chatbot interface for generating conversational responses using the LLaMA language model.\n",
    "\n",
    "    Attributes:\n",
    "    - system_context (str): Contextual information to provide to the language model for all conversations.\n",
    "    - conversation_history (list of tuples): Stores the history of the conversation, where each\n",
    "      tuple contains a user message and the corresponding agent response.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, system_context):\n",
    "        \"\"\"\n",
    "        Initializes a new instance of the LlamaChatbot class.\n",
    "\n",
    "        Parameters:\n",
    "        - system_context (str): A string that sets the initial context for the language model.\n",
    "        \"\"\"\n",
    "        self.system_context = system_context\n",
    "        self.conversation_history = []  # Initializes the conversation history\n",
    "\n",
    "    def chat(self, user_msg):\n",
    "        \"\"\"\n",
    "        Generates a response from the chatbot based on the user's message.\n",
    "\n",
    "        This method constructs a prompt with the current system context and conversation history,\n",
    "        sends it to the language model, and then stores the new user message and model's response\n",
    "        in the conversation history.\n",
    "\n",
    "        Parameters:\n",
    "        - user_msg (str): The user's message to which the chatbot will respond.\n",
    "\n",
    "        Returns:\n",
    "        - str: The generated response from the chatbot.\n",
    "        \"\"\"\n",
    "        # Generate the prompt using the conversation history and the new user message\n",
    "        prompt = construct_prompt_with_context(user_msg, self.system_context, self.conversation_history)\n",
    "        \n",
    "        # Get the model's response\n",
    "        agent_response = generate(prompt)\n",
    "\n",
    "        # Store this interaction in the conversation history\n",
    "        self.conversation_history.append((user_msg, agent_response))\n",
    "\n",
    "        return agent_response\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        Resets the conversation history of the chatbot.\n",
    "\n",
    "        This method clears the existing conversation history, effectively restarting the conversation.\n",
    "        \"\"\"\n",
    "        # Clear conversation history\n",
    "        self.conversation_history = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "607c5fd5-0c47-40e6-894b-b673e4957de1",
   "metadata": {},
   "source": [
    "### LlamaChatBotWithHistoryLimit Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36917bf9-73e2-4580-a30e-7315e40b9a43",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class LlamaChatbotWithHistoryLimit:\n",
    "    \"\"\"\n",
    "    A chatbot interface for generating conversational responses using the LLaMA language model.\n",
    "\n",
    "    Attributes:\n",
    "        - system_context (str): Contextual information to provide to the language model for all conversations.\n",
    "        - conversation_history (list of tuples): Stores the history of the conversation, where each\n",
    "          tuple contains a user message and the corresponding agent response.\n",
    "        - tokenizer: The tokenizer used to tokenize the conversation for maintaining the history limit.\n",
    "        - max_tokens (int): The maximum number of tokens allowed in the conversation history.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, system_context, tokenizer, max_tokens=2048):\n",
    "        \"\"\"\n",
    "        Initializes a new instance of the LlamaChatbot class with a tokenizer and token limit.\n",
    "\n",
    "        Parameters:\n",
    "            - system_context (str): A string that sets the initial context for the language model.\n",
    "            - tokenizer: The tokenizer used to process the input and output for the language model.\n",
    "            - max_tokens (int): The maximum number of tokens to retain in the conversation history.\n",
    "        \"\"\"\n",
    "        self.system_context = system_context\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_tokens = max_tokens\n",
    "        self.conversation_history = []  # Initializes the conversation history\n",
    "\n",
    "    def chat(self, user_msg):\n",
    "        \"\"\"\n",
    "        Generates a response from the chatbot based on the user's message.\n",
    "\n",
    "        This method constructs a prompt with the current system context and conversation history,\n",
    "        sends it to the language model, and then stores the new user message and model's response\n",
    "        in the conversation history, ensuring that the history does not exceed the specified token limit.\n",
    "\n",
    "        Parameters:\n",
    "            - user_msg (str): The user's message to which the chatbot will respond.\n",
    "\n",
    "        Returns:\n",
    "            - str: The generated response from the chatbot.\n",
    "        \"\"\"\n",
    "        # Generate the prompt using the conversation history and the new user message\n",
    "        prompt = construct_prompt_with_context(user_msg, self.system_context, self.conversation_history)\n",
    "        \n",
    "        # Get the model's response\n",
    "        agent_response = generate(prompt)\n",
    "\n",
    "        # Store this interaction in the conversation history\n",
    "        self.conversation_history.append((user_msg, agent_response))\n",
    "\n",
    "        # Check and maintain the conversation history within the token limit\n",
    "        self._trim_conversation_history()\n",
    "\n",
    "        return agent_response\n",
    "\n",
    "    def _trim_conversation_history(self):\n",
    "        \"\"\"\n",
    "        Trims the conversation history to maintain the number of tokens below the specified limit.\n",
    "        \"\"\"\n",
    "        # Concatenate the conversation history into a single string\n",
    "        history_string = ''.join(user + agent for user, agent in self.conversation_history)\n",
    "        \n",
    "        # Calculate the number of tokens in the conversation history\n",
    "        history_tokens = len(self.tokenizer.encode(history_string))\n",
    "\n",
    "        # While the history exceeds the maximum token limit, remove the oldest items\n",
    "        while history_tokens > self.max_tokens:\n",
    "            # Always check if there's at least one item to pop\n",
    "            if self.conversation_history:\n",
    "                # Remove the oldest conversation tuple\n",
    "                self.conversation_history.pop(0)\n",
    "                # Recalculate the history string and its tokens\n",
    "                history_string = ''.join(user + agent for user, agent in self.conversation_history)\n",
    "                history_tokens = len(self.tokenizer.encode(history_string))\n",
    "            else:\n",
    "                # If the conversation history is empty, break out of the loop\n",
    "                break\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        Resets the conversation history of the chatbot.\n",
    "\n",
    "        This method clears the existing conversation history, effectively restarting the conversation.\n",
    "        \"\"\"\n",
    "        # Clear conversation history\n",
    "        self.conversation_history = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd12556-fb8e-4768-9044-811622cec89b",
   "metadata": {},
   "source": [
    "### Print Number of Tokens in a Given String"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f2f0379-07aa-4583-9b37-2995aa882ef0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def print_token_count(text, tokenizer):\n",
    "    \"\"\"\n",
    "    Calculate and return the number of tokens in a given text using a specified tokenizer.\n",
    "\n",
    "    This function takes a string of text and a tokenizer. It uses the tokenizer to encode the text\n",
    "    into tokens and then returns the count of these tokens.\n",
    "\n",
    "    Parameters:\n",
    "    - text (str): The input string to be tokenized.\n",
    "    - tokenizer: A tokenizer instance capable of encoding text into tokens.\n",
    "\n",
    "    Returns:\n",
    "    - int: The number of tokens in the input text as determined by the tokenizer.\n",
    "    \"\"\"\n",
    "    return len(tokenizer.encode(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2821401d-ac98-4b30-9f6f-70ce07b731ea",
   "metadata": {},
   "source": [
    "### Concatenate Conversation History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ea30bb2-94e3-4f6d-94fd-d46e43645c9c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def concat_history(tuples_list):\n",
    "    \"\"\"\n",
    "    Concatenates texts from a list of 2-tuples.\n",
    "\n",
    "    Each tuple in the list is expected to contain two strings. The function\n",
    "    will concatenate all the first elements followed by all the second elements\n",
    "    in their respective order of appearance in the list.\n",
    "\n",
    "    Parameters:\n",
    "    - tuples_list (list of 2-tuples): A list where each element is a tuple of two strings.\n",
    "\n",
    "    Returns:\n",
    "    - str: A single string that is the result of concatenating all the texts from the tuples.\n",
    "\n",
    "    Example usage:\n",
    "    ```\n",
    "    conversation_tuples = [\n",
    "        ('Question 1', 'Answer 1'),\n",
    "        ('Question 2', 'Answer 2'),\n",
    "        ('Question 3', 'Answer 3')\n",
    "    ]\n",
    "\n",
    "    concatenated_text = concatenate_texts_from_tuples(conversation_tuples)\n",
    "    print(concatenated_text)\n",
    "    ```\n",
    "    \"\"\"\n",
    "    # Concatenate all the first and second elements of the tuples\n",
    "    return ''.join(question + response for question, response in tuples_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6781a436-aec7-47d6-83a3-3cccf373c978",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef88361d-a326-4a8d-bd75-31742be6378b",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Star Bikes Details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09ea1e03-4438-4bea-af72-e5a5fe2783ef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "bikes = [\n",
    "    {\n",
    "        \"model\": \"Galaxy Rider\",\n",
    "        \"type\": \"Mountain\",\n",
    "        \"features\": {\n",
    "            \"frame\": \"Aluminum alloy\",\n",
    "            \"gears\": \"21-speed Shimano\",\n",
    "            \"brakes\": \"Hydraulic disc\",\n",
    "            \"tires\": \"27.5-inch all-terrain\",\n",
    "            \"suspension\": \"Full, adjustable\",\n",
    "            \"color\": \"Matte black with green accents\"\n",
    "        },\n",
    "        \"usps\": [\"Lightweight frame\", \"Quick gear shift\", \"Durable tires\"],\n",
    "        \"price\": 799.95,\n",
    "        \"internal_id\": \"GR2321\",\n",
    "        \"weight\": \"15.3 kg\",\n",
    "        \"manufacturer_location\": \"Taiwan\"\n",
    "    },\n",
    "    {\n",
    "        \"model\": \"Nebula Navigator\",\n",
    "        \"type\": \"Hybrid\",\n",
    "        \"features\": {\n",
    "            \"frame\": \"Carbon fiber\",\n",
    "            \"gears\": \"18-speed Nexus\",\n",
    "            \"brakes\": \"Mechanical disc\",\n",
    "            \"tires\": \"26-inch city slick\",\n",
    "            \"suspension\": \"Front only\",\n",
    "            \"color\": \"Glossy white\"\n",
    "        },\n",
    "        \"usps\": [\"Sleek design\", \"Efficient on both roads and trails\", \"Ultra-lightweight\"],\n",
    "        \"price\": 649.99,\n",
    "        \"internal_id\": \"NN4120\",\n",
    "        \"weight\": \"13.5 kg\",\n",
    "        \"manufacturer_location\": \"Germany\"\n",
    "    },\n",
    "    {\n",
    "        \"model\": \"Cosmic Comet\",\n",
    "        \"type\": \"Road\",\n",
    "        \"features\": {\n",
    "            \"frame\": \"Titanium\",\n",
    "            \"gears\": \"24-speed Campagnolo\",\n",
    "            \"brakes\": \"Rim brakes\",\n",
    "            \"tires\": \"700C road\",\n",
    "            \"suspension\": \"None\",\n",
    "            \"color\": \"Metallic blue\"\n",
    "        },\n",
    "        \"usps\": [\"Super aerodynamic\", \"High-speed performance\", \"Professional-grade components\"],\n",
    "        \"price\": 1199.50,\n",
    "        \"internal_id\": \"CC5678\",\n",
    "        \"weight\": \"11 kg\",\n",
    "        \"manufacturer_location\": \"Italy\"\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ffb9670-9447-43bd-9f42-1c36adc9fe00",
   "metadata": {},
   "source": [
    "## Bikes AI Assistant"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70a5da85-b6d6-4c45-a9dd-117caa121a89",
   "metadata": {},
   "source": [
    "In this section we will be creating an AI customer support assistant that will help potential customers in their purchase of their next Star Bike.\n",
    "\n",
    "We will begin by setting an appropriate **system context** and instantiating a chatbot instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f807d3b9-00a0-428d-beb4-07e061fcc06d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "system_context = \"\"\"\n",
    "You are a friendly chatbot knowledgeable about bicycles. \\\n",
    "When asked about specific bike models or features, you try to provide accurate and helpful answers. \\\n",
    "Your goal is to assist and inform potential customers to the best of your ability in 50 words or less.\n",
    "\"\"\"\n",
    "\n",
    "chatbot = LlamaChatbot(system_context)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a340a315-10c6-465c-a027-1359ae45dfcc",
   "metadata": {},
   "source": [
    "Let's ask the model to tell us about the lasest bikes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80de7b39-d9e0-43db-906a-96fb33ce149f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(chatbot.chat(\"Can you tell me about the latest models?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cb5049d-7a9c-450c-85d9-cad7795c40e2",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a6963c6-fa77-4875-a2a9-6cf09dfc3755",
   "metadata": {},
   "source": [
    "This isn't bad, but of course we want the assistant to tell us about the models by Star Bikes!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "549fc0ac-9e17-477b-9f88-2cc49607d515",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "chatbot.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34f46541-c7c8-4f44-993f-21df834135b1",
   "metadata": {},
   "source": [
    "## Star Bikes AI Assistant"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57f31c9b-d1d5-4dbc-81c4-c2999a007433",
   "metadata": {},
   "source": [
    "Let's create a new chatbot, including the `bikes` data from above for it to refer to during conversation. In the following **system context** we provide the model with a **cue** to always end the exchange by asking what else it can help with. Not only is this a good idea for an AI assistant, but in practice, prevents the model from going on indefinitely, or attempting to generate multiple exchanges when only one is required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67cb3496-72a5-4f62-a170-0e246e62f101",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "system_context = f\"\"\"\n",
    "You are a friendly chatbot knowledgeable about these bicycles from Star Bikes {bikes}. \\\n",
    "When asked about specific bike models or features, you try to provide accurate and helpful answers. \\\n",
    "Your goal is to assist and inform potential customers to the best of your ability in 50 words or less. \\\n",
    "You always end by asking what else you can help with.\n",
    "\"\"\"\n",
    "\n",
    "chatbot = LlamaChatbot(system_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a291ed1b-caff-416f-a964-19ee773affe8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(chatbot.chat(\"Can you tell me about the latest models?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e6f4ca3-3dc5-453a-b6f7-b94c27981c07",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75bd4d51-3e6d-40e9-84b1-ff5cfd575134",
   "metadata": {},
   "source": [
    "That's pretty great. Let's see how it responds when asked for specific details about the bike?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb9f4c4a-294c-4c2c-bbe0-263fde0c5eb6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(chatbot.chat(\"How much do each of the models cost?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d44016a-21e8-4bb5-bb69-a49b18e952cd",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1a0a2ca-0519-428f-9f78-bba5338e3c56",
   "metadata": {},
   "source": [
    "Very good. Let's see how it responds to a more nebulous query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb4deafd-9b62-4406-b7f4-8d9bf773dbd6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(chatbot.chat(\"I am more intersted in biking around town.\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a0e08ee-70ce-40bc-be89-b80efaf7d3da",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f966feaf-d013-4d2b-a53e-457ecbc10b65",
   "metadata": {},
   "source": [
    "All in all it seems like our assistant, already, is performing quite well."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c83c788c-e833-48a5-9696-4b9137cd3ed5",
   "metadata": {},
   "source": [
    "## Considerations About Number of Tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b51933a1-e07c-4fa9-b836-60f27523da47",
   "metadata": {},
   "source": [
    "When we pass text to a language model like LLaMA-2, the text is converted into **tokens**, units of text, such as a word or punctuation mark, that language models use for processing and generating text.\n",
    "\n",
    "Language models like LLaMA-2 operate with an intrinsic **token limit**, a fixed upper boundary on the number of tokens they can process in a single prompt-response cycle. This limitation is due to their design and the computational resources required to handle the tokens. In the case of our LLaMA-2 model, the **token limit** is set at `4096` tokens. The token limit for a given model can be obtained through its documentation, but within its inherent limitation, can also be controlled. When using a `transformers` pipeline as we are, we control the **token limit** with the `max_length` argument.\n",
    "\n",
    "The instrinsic token limitation, or `max_length` argument (whichever is less) dictates the total number of tokens allotted for *both the input prompt and the model's output*.\n",
    "\n",
    "Since we did not clear the chat history from the chat exchange above, let's look at the current `chatbot` instance's `conversation_history`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97255f11-7dd8-4fb8-bc63-32b7f5175e20",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(chatbot.conversation_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43394f98-71bd-4d45-8590-89220bda0809",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e22e3162-8e23-4725-b529-ac1b887b1873",
   "metadata": {},
   "source": [
    "To support getting a count of how many **tokens** all of these strings represent, we will use a `concat_history` helper function defined above to concatenate all of the strings in our conversation history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b974926-4b7c-46ca-b291-a318fd3208f7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "conv_history = concat_history(chatbot.conversation_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd9ee2b0-e59e-4a6d-acc6-e06a4f60e4d3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(conv_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "456682a0-78fd-4e73-b228-0f44c2311197",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f8fed90-be75-43c6-9ec1-c92cf75aa46d",
   "metadata": {},
   "source": [
    "Now, we will use another helper function defined above, `print_token_count`, to **tokenize** our conversation history string, using the LLaMA-2 **tokenizer** (imported above)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d200a0fc-b1cc-4502-a18a-a4fdb235f0c2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print_token_count(conv_history, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "448d19d2-433f-4444-bf0d-9d965e76f76a",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c982e096-b692-4a85-8be0-d76b490749b9",
   "metadata": {},
   "source": [
    "Let's look at how additional exchanges with the chatbot gradually increases the number of **tokens** in the conversation history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "198b0214-25cc-4769-b66e-5223ad416913",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(chatbot.chat(\"What kind of bike would be best if I'm on a budget?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7edb140-6106-45fe-8f29-f2a5f66b4ae3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print_token_count(concat_history(chatbot.conversation_history), tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cfcca2a-854f-4dd6-86b2-3fefba87c734",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(chatbot.chat(\"What's the next most expensive bike after the Galaxy Rider?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ad11054-d79b-4bbd-9241-e0319a50e641",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print_token_count(concat_history(chatbot.conversation_history), tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85a24973-314a-423b-b382-32d8dbe823cb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(chatbot.chat(\"Why is titanium so good for a frame?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41c72ea7-2336-4320-a98e-bdd322ec51c7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print_token_count(concat_history(chatbot.conversation_history), tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10615ebc-4ff2-4bd1-a381-30b502b7cf56",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(chatbot.chat(\"Do you remember where I said I was most interested in riding?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9348ee5a-8e5c-49d9-87fd-3f7a50874fae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print_token_count(concat_history(chatbot.conversation_history), tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed953f4-1cfb-42c0-90e2-99f913c8c90a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(chatbot.chat(\"Can you please summarize our conversation for me?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95345a80-afd4-4187-8cdf-8fc8a774f84a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print_token_count(concat_history(chatbot.conversation_history), tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2108c8a9-d8cf-4c1c-a9d0-aced31bf9f3f",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a46c457-4f67-4afb-a482-00386b630878",
   "metadata": {},
   "source": [
    "To conclude this exploration, we will reset the chatbot and print the token count one more time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98a5656e-fc22-4a7d-83e4-90bb8b41be04",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "chatbot.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8d76126-2cbe-42e1-b800-b83243a36dd4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print_token_count(concat_history(chatbot.conversation_history), tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "715bde14-8613-49a7-aa11-9dd454a83eac",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65263216-9d3a-4556-8d5d-7ecfa1a20c32",
   "metadata": {},
   "source": [
    "Given that our chatbot implementation is storing previous conversations by passing the conversation history into the prompt on subsequent exchanges, we are, with every exchange, approaching the **token limit** of our model.\n",
    "\n",
    "As mentioned above, the intrinsic **token limit** of the model we are using is `4096`, and if you look at the `generate` function definition above, we are passing in `4096` as the `max_length` argument. Thus, we have not yet gotten close to the **token limit**, however, we should take into account how to make sure this hard limit does not create problems in the use of our chatbot."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac55f853-6bdd-4204-9eaf-20cae86e942c",
   "metadata": {},
   "source": [
    "## Limit Chat History"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "379d1801-499b-4866-a5df-d4007ea31053",
   "metadata": {},
   "source": [
    "Below is a modified chat class `LlamaChatbotWithHistoryLimit`. It accepts a `max_tokens` argument, and also a `tokenizer` that will be used to keep track of the number of tokens present in the conversation history.\n",
    "\n",
    "In the scenario that the conversation history will exceed `max_tokens`, `_trim_conversation_history` will be called to pop off the oldest conversations in the history until the history is below `max_tokens`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c2ee658-5985-48ff-8b87-f911c5f4b458",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class LlamaChatbotWithHistoryLimit:\n",
    "    \"\"\"\n",
    "    A chatbot interface for generating conversational responses using the LLaMA language model.\n",
    "\n",
    "    Attributes:\n",
    "        - system_context (str): Contextual information to provide to the language model for all conversations.\n",
    "        - conversation_history (list of tuples): Stores the history of the conversation, where each\n",
    "          tuple contains a user message and the corresponding agent response.\n",
    "        - tokenizer: The tokenizer used to tokenize the conversation for maintaining the history limit.\n",
    "        - max_tokens (int): The maximum number of tokens allowed in the conversation history.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, system_context, tokenizer, max_tokens=2048):\n",
    "        \"\"\"\n",
    "        Initializes a new instance of the LlamaChatbot class with a tokenizer and token limit.\n",
    "\n",
    "        Parameters:\n",
    "            - system_context (str): A string that sets the initial context for the language model.\n",
    "            - tokenizer: The tokenizer used to process the input and output for the language model.\n",
    "            - max_tokens (int): The maximum number of tokens to retain in the conversation history.\n",
    "        \"\"\"\n",
    "        self.system_context = system_context\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_tokens = max_tokens\n",
    "        self.conversation_history = []  # Initializes the conversation history\n",
    "\n",
    "    def chat(self, user_msg):\n",
    "        \"\"\"\n",
    "        Generates a response from the chatbot based on the user's message.\n",
    "\n",
    "        This method constructs a prompt with the current system context and conversation history,\n",
    "        sends it to the language model, and then stores the new user message and model's response\n",
    "        in the conversation history, ensuring that the history does not exceed the specified token limit.\n",
    "\n",
    "        Parameters:\n",
    "            - user_msg (str): The user's message to which the chatbot will respond.\n",
    "\n",
    "        Returns:\n",
    "            - str: The generated response from the chatbot.\n",
    "        \"\"\"\n",
    "        # Generate the prompt using the conversation history and the new user message\n",
    "        prompt = construct_prompt_with_context(user_msg, self.system_context, self.conversation_history)\n",
    "        \n",
    "        # Get the model's response\n",
    "        agent_response = generate(prompt)\n",
    "\n",
    "        # Store this interaction in the conversation history\n",
    "        self.conversation_history.append((user_msg, agent_response))\n",
    "\n",
    "        # Check and maintain the conversation history within the token limit\n",
    "        self._trim_conversation_history()\n",
    "\n",
    "        return agent_response\n",
    "\n",
    "    def _trim_conversation_history(self):\n",
    "        \"\"\"\n",
    "        Trims the conversation history to maintain the number of tokens below the specified limit.\n",
    "        \"\"\"\n",
    "        # Concatenate the conversation history into a single string\n",
    "        history_string = ''.join(user + agent for user, agent in self.conversation_history)\n",
    "        \n",
    "        # Calculate the number of tokens in the conversation history\n",
    "        history_tokens = len(self.tokenizer.encode(history_string))\n",
    "\n",
    "        # While the history exceeds the maximum token limit, remove the oldest items\n",
    "        while history_tokens > self.max_tokens:\n",
    "            # Always check if there's at least one item to pop\n",
    "            if self.conversation_history:\n",
    "                # Remove the oldest conversation tuple\n",
    "                self.conversation_history.pop(0)\n",
    "                # Recalculate the history string and its tokens\n",
    "                history_string = ''.join(user + agent for user, agent in self.conversation_history)\n",
    "                history_tokens = len(self.tokenizer.encode(history_string))\n",
    "            else:\n",
    "                # If the conversation history is empty, break out of the loop\n",
    "                break\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        Resets the conversation history of the chatbot.\n",
    "\n",
    "        This method clears the existing conversation history, effectively restarting the conversation.\n",
    "        \"\"\"\n",
    "        # Clear conversation history\n",
    "        self.conversation_history = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96740912-962e-46d5-8d1c-c6539b4e026d",
   "metadata": {},
   "source": [
    "Let's create a new chatbot instance, this time with a `max_tokens` limit of `200` tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dcdd33e-20ed-4a2c-b75e-a9683a0fffce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "system_context = f\"\"\"\n",
    "You are a friendly chatbot knowledgeable about these bicycles from Star Bikes {bikes}. \\\n",
    "When asked about specific bike models or features, you try to provide accurate and helpful answers. \\\n",
    "Your goal is to assist and inform potential customers to the best of your ability in 50 words or less. \\\n",
    "You always end by asking what else you can help with.\n",
    "\"\"\"\n",
    "\n",
    "chatbot = LlamaChatbotWithHistoryLimit(system_context, tokenizer=tokenizer, max_tokens=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc914310-ea45-42f8-93c9-a5daa9c9d7d0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(chatbot.chat(\"Can you tell me about the latest models?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15746b3b-865d-42f0-83ed-a2ad99b27929",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76b747ce-4c49-497d-96cd-2f1e57040a20",
   "metadata": {},
   "source": [
    "We will run a few more exchanges, keeping track of the number of tokens in the conversation history. Keep in mind that we have set `max_tokens` to `200`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce146ee1-5554-41d8-a9e9-2b28f687fe2b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print_token_count(concat_history(chatbot.conversation_history), tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "895965d9-469d-4aa8-a039-1daf29e426c5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(chatbot.chat(\"How much do each of the models cost?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c39882df-017f-43b6-9f82-fecd421e477b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print_token_count(concat_history(chatbot.conversation_history), tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7c8fe2b-b4a9-430d-bfb3-295a05af8af4",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d63d4685-f4cb-4325-9f38-7ec21d0c1796",
   "metadata": {},
   "source": [
    "You can see that the token count has been reduced to `96` in order to prevent our going over the specified limit of `200` tokens. Let's observe a few more rounds of conversation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b387aca-23d9-4ef9-8271-4b58cc7466a2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(chatbot.chat(\"I am more intersted in biking around town.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "243c322b-c52a-43f4-87e2-87716c90f073",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print_token_count(concat_history(chatbot.conversation_history), tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "212308d0-07be-4ba0-9697-56511599037e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(chatbot.chat(\"What kind of bike would be best if I'm on a budget?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15b687d7-efbf-4b44-b719-18ffda87cacc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print_token_count(concat_history(chatbot.conversation_history), tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e87b6c15-bf5b-4b8d-a55d-ae9a0abfe611",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06765c5b-faef-4171-8b7b-66e847f0cf94",
   "metadata": {},
   "source": [
    "Our chatbot is succesfully popping off earlier rounds of conversation to avoid going over the limit.\n",
    "\n",
    "Of course, in exchange for this failsafe behavior, we have traded off perfect retention of the conversation history. Here we can see that unlike before, when we ask for a summary of the conversation thus far, we only receive a recap of our most recent exchanges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f965aa16-54d0-412f-9829-880967bc4557",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(chatbot.chat(\"Can you summarize our conversation?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82f0382b-c259-495d-827b-abf508d44d21",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print_token_count(concat_history(chatbot.conversation_history), tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b592e4d1-97ea-4b61-9002-8e34f0ce026e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "chatbot.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "291c137b-9817-49e0-a624-70d7020d5f47",
   "metadata": {},
   "source": [
    "## Final Exercise: Create an AI Assistant for Your Own Fictitious Company"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c4d9001-947c-4c39-9d02-a2f463aaad41",
   "metadata": {},
   "source": [
    "Using everything you've learned thus far, create an AI assistant for a fictitious company of your choosing. Your work will consist of several major steps.\n",
    "\n",
    "1) Come up with an idea for a company, including its name and what it is going to sell.\n",
    "1) Use our LLaMA-2 model to generate synthetic data for the products your company will sell. See the *Star Bikes Details* section above, or the `bikes` dictionary as an example. Refer to notebook *3-Review Analyst.ipynb* if you get stuck generating synthetic JSON data.\n",
    "1) Create the AI assistant, providing it the synthetic data your generated in the previous step. You're more than welcome to use the `LlamaChatbotWithHistoryLimit` class from this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "881eec26-11f0-4880-9c97-12c57bf29e39",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Key Concept Review"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ba8e70d-29c3-4aeb-8e7b-b467b1b746ba",
   "metadata": {},
   "source": [
    "The following key concepts were introduced in this notebook:\n",
    "\n",
    "- **Token:** A piece of text, like a word or punctuation, used by language models for processing.\n",
    "- **Token Limit:** The maximum number of tokens a language model can process in a single prompt.\n",
    "- **Tokenizer:** A tool that converts text into tokens for language models to understand."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80dadf18-3fda-451b-a684-58510245cfb8",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Restart the Kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "049cf48a-82c8-4010-a863-01dad76b8187",
   "metadata": {},
   "source": [
    "In order to free up GPU memory for other notebooks, please run the following cell to restart the kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e333754-31d1-4d45-8c8f-a8db78466599",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from IPython import get_ipython\n",
    "\n",
    "get_ipython().kernel.do_shutdown(restart=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4aa7125-186a-4d07-b3e8-75b779f944c3",
   "metadata": {},
   "source": [
    "![DLI Header](images/DLI_Header.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
