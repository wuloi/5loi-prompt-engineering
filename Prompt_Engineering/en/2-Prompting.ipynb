{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bb68f6a4-67c2-4f6b-bd90-4fc7e361fe86",
   "metadata": {},
   "source": [
    "![DLI Header](images/DLI_Header.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e27fba5-6948-42c9-9ac3-bf8a67244619",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Iterative Prompt Development"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e13d039-9ec5-4617-912f-4bfc84f9324d",
   "metadata": {},
   "source": [
    "In this notebook we warm up by iterating on a set of simple prompts, familiarizing ourselves with the `transformers` pipeline and the LLaMA-2 model we will be using throughout the course.\n",
    "\n",
    "By iteratively experimenting with seemingly simple prompts, we will begin to see the importance of creating prompts that are **specific**, provide **cues** and will also learn about how to give the model **\"time to think\"** when given tasks that it might find challenging."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83db67bb-a953-4210-a032-bf9f7957ebad",
   "metadata": {},
   "source": [
    "## Learning Objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8646c89b-a172-4773-8b93-1a213df8d746",
   "metadata": {},
   "source": [
    "By the time you complete this notebook you will be able to:\n",
    "- Use a `transformers` pipeline to generate responses from a LLaMA-2 LLM.\n",
    "- Craft prompts that are **specific**.\n",
    "- Craft prompts that give the model **\"time to think\"**.\n",
    "- Provide **cues** to the model to guide its response."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89c0fd90-b2c4-4432-a6f0-7d378325d0ed",
   "metadata": {},
   "source": [
    "## Video Walkthrough"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eeab60a-a06a-43ff-8ddf-51d0d8f4a32f",
   "metadata": {},
   "source": [
    "Execute the cell below to load the video walkthrough of this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf5f1c59-ec1f-44c3-8628-41a240f610fc",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    " from IPython.display import HTML\n",
    "\n",
    "video_url = \"https://d36m44n9vdbmda.cloudfront.net/assets/s-fx-12-v1/v2/02-prompting.mp4\"\n",
    "\n",
    "video_html = f\"\"\"\n",
    "<video controls width=\"640\" height=\"360\">\n",
    "    <source src=\"{video_url}\" type=\"video/mp4\">\n",
    "    Your browser does not support the video tag.\n",
    "</video>\n",
    "\"\"\"\n",
    "\n",
    "display(HTML(video_html))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "527e2324-6ed6-4658-910f-d15c68e26248",
   "metadata": {},
   "source": [
    "## Create LLaMA-2 Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "465f9aa8-b9d0-4c87-9b54-bcb9d59f9880",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "model = \"TheBloke/Llama-2-13B-chat-GPTQ\"\n",
    "\n",
    "llama_pipe = pipeline(\"text-generation\", model=model, device_map=\"auto\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cc37a21-9dd5-46b0-a99a-243082a12981",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8af26cb7-f326-4aec-91b8-128d87b8a7d3",
   "metadata": {},
   "source": [
    "In this notebook we will use the following function to support our interaction with the LLM."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69b99117-4086-4737-ba05-f576ba8e9875",
   "metadata": {},
   "source": [
    "### Generate Model Responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fca80c8-610c-4f46-bb11-65d1ad3dbc59",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate(prompt, max_length=1024, pipe=llama_pipe, **kwargs):\n",
    "    \"\"\"\n",
    "    Generates a response to the given prompt using a specified language model pipeline.\n",
    "\n",
    "    This function takes a prompt and passes it to a language model pipeline, such as LLaMA, \n",
    "    to generate a text response. The function is designed to allow customization of the \n",
    "    generation process through various parameters and keyword arguments.\n",
    "\n",
    "    Parameters:\n",
    "    - prompt (str): The input text prompt to generate a response for.\n",
    "    - max_length (int): The maximum length of the generated response. Default is 1024 tokens.\n",
    "    - pipe (callable): The language model pipeline function used for generation. Default is llama_pipe.\n",
    "    - **kwargs: Additional keyword arguments that are passed to the pipeline function.\n",
    "\n",
    "    Returns:\n",
    "    - str: The generated text response from the model, trimmed of leading and trailing whitespace.\n",
    "\n",
    "    Example usage:\n",
    "    ```\n",
    "    prompt_text = \"Explain the theory of relativity.\"\n",
    "    response = generate(prompt_text, max_length=512, pipe=my_custom_pipeline, temperature=0.7)\n",
    "    print(response)\n",
    "    ```\n",
    "    \"\"\"\n",
    "\n",
    "    def_kwargs = dict(return_full_text=False, return_dict=False)\n",
    "    response = pipe(prompt.strip(), max_length=max_length, **kwargs, **def_kwargs)\n",
    "    return response[0]['generated_text'].strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2999e0dc-b67a-436d-a6df-ff86b8e5c0d1",
   "metadata": {
    "tags": []
   },
   "source": [
    "## The Capital of California"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aaf04e1-e084-4d05-a1a8-e51331f3c6eb",
   "metadata": {},
   "source": [
    "Let's begin with a very simple prompt, which we will pass to our `generate` function in order to get a response back from the LLaMA-2 model we are using. In this series of prompts we are interested for the model to respond to us with the capital of the state of California, which is *Sacramento*.\n",
    "\n",
    "Our hope for this experiment is to get back the exact response `\"Sacramento\"` without anything else in the repsonse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01702d62-b226-45b6-8e20-ed7f410b62e3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt = \"What is the capital of California?\"\n",
    "\n",
    "print(generate(prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b7c70c6-e7e8-46f0-8d28-5033460263a8",
   "metadata": {
    "tags": []
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cbac8c9-319e-43f3-9859-b2f269bb50ff",
   "metadata": {},
   "source": [
    "The model did not understand that we only wanted the name of the capital city, without any other context, so let's craft a prompt that is more **specific**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a69727a1-e364-4a17-93d1-721be30b63e8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt = \"What is the capital of California? Only answer this question and do so in as few a words as possible.\"\n",
    "\n",
    "print(generate(prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "855424dc-6d97-41c4-8347-d98aba83a787",
   "metadata": {
    "tags": []
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de56c7ff-2627-4fca-9c97-4177340358c5",
   "metadata": {},
   "source": [
    "That is an improvement, but we are still getting a leading `Answer: ` in the response. Let's try to prevent this behavior by providing the model with the **cue** `Answer: `. Doing so may prevent the model from providing that text itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd4cbbf1-7b45-481f-8a2b-a4b6589aa800",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt = \"What is the capital of California? Only answer this question and do so in as few a words as possible. Answer: \"\n",
    "\n",
    "print(generate(prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b27326d-a109-4483-a820-73cd66c63fbd",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Vowels in Sacramento"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeeec1c9-d7bc-4a56-8938-8292f2b50331",
   "metadata": {},
   "source": [
    "In the following section we try to get the model to do something a little more complicated: tell us all the vowels in the name of the capital of California.\n",
    "\n",
    "The correct answer is S**a**cr**a**m**e**nt**o** -> **aaeo** -> **aeo**. It's worth noting that in order to make this easy on myself (and you) I performed multiple steps to arrive at my answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "546b97c2-b2a1-4251-adbc-864c271f2e0b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt = \"Tell me the vowels in the capital of California.\"\n",
    "\n",
    "print(generate(prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92e13825-1418-432b-b5ea-167111451243",
   "metadata": {
    "tags": []
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baf59eea-2643-43c8-be7f-7a1bb5f658a8",
   "metadata": {},
   "source": [
    "When models are faced with the need to reason in a way that requires multiple steps, it often helps to construct a prompt requesting that the model perform multiple intermediary steps, almost like asking it to show its work. This technique is often referred to as giving the model **\"time to think\"**.\n",
    "\n",
    "The prompt below aims for the same end result, but asks the model to take the intermediate step of responding with the capital of California, before then responding with the vowels in it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91edcd68-c6ea-4dce-8f3a-1ae4a326ac56",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt = \"Tell me the capital of California, and then tell me all the vowels in it.\"\n",
    "\n",
    "print(generate(prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8682e8d0-fb74-4b1e-8046-7b1037e83cb4",
   "metadata": {
    "tags": []
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e33833fe-bd3f-482f-a4d8-068b65ec4639",
   "metadata": {},
   "source": [
    "Now that we see the effectiveness of giving the model **\"time to think\"**, let's try again with a slightly more complicated task: the vowels in the capital of California in reverse alphabetical order.\n",
    "\n",
    "The correct answer is S**a**cr**a**m**e**nt**o** -> **aaeo** -> **aeo** -> **oea**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd3413ea-8224-4a30-8ab5-b89f31f7f254",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt = \"Tell me the vowels in the capital of California in reverse alphabetical order?\"\n",
    "\n",
    "print(generate(prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63e66050-4449-4c32-8bfa-ed0fa015c776",
   "metadata": {
    "tags": []
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f7acd4f-1c53-47b5-961f-3f5fbf62311a",
   "metadata": {},
   "source": [
    "In order to assist the model, let's again give it **\"time to think\"** by prompting it to break down the task into intermediate steps and show its work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62f3756b-b976-43fb-b082-334bdecbadfc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt = \"Tell me the capital of California, and then tell me all the vowels in it, then tell me the vowels in reverse-alphabetical order.\"\n",
    "\n",
    "print(generate(prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d74e501e-75d9-46e0-8fef-84a9cfd33166",
   "metadata": {},
   "source": [
    "## Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fb4e198-d274-43cd-ba2c-620b8fbdd45b",
   "metadata": {},
   "source": [
    "While LLMs aren't necessarily the best tool for performing math, as an exercise, generate a response from the prompt below, which intends to get the product of multiplying 23 and 34, and then iteratively develop a prompt which results in your getting the correct answer. Be sure to consider how you can be **precise** in your prompt, and also, provide an opportunity for the model to have **\"time to think\"**.\n",
    "\n",
    "If you get stuck, a solution is provided below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14541792-ee4e-44e9-8d13-a99c3a8d8d3b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "23*34 # Show the actual answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24769517-7106-44dd-9e9b-a286a5f15483",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt = \"23x34\" # While you and I understand the intention of this prompt, to the model it is not at all **precise**\n",
    "\n",
    "print(generate(prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "897ea6f6-1259-4b1f-977c-e26cc2bdb489",
   "metadata": {},
   "source": [
    "### Your Work Here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3a7ab61-dc0e-4ec2-b79d-7f7cc843dd63",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "65c1181a-21c7-41bd-acbd-817aa20ae510",
   "metadata": {},
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3531114a-2570-4707-9afb-0d999841c2ee",
   "metadata": {},
   "source": [
    "Click on the `...` to see a working solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02322095-a320-4a0a-81b7-b3a62bc6ff18",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt = \"Calculate the product of 23 and 34. Use the steps typical of long multiplication and show your work.\"\n",
    "\n",
    "print(generate(prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db9253ae-933e-4bd0-8227-466b55526928",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Key Concept Review"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f1408fc-a6bf-458d-a532-9635677300b9",
   "metadata": {},
   "source": [
    "The following key concepts were introduced in this notebook:\n",
    "\n",
    "- **Precise**: Being as explicit as necessary to guide the response of an LLM.\n",
    "- **Cue**: A conclusion to a prompt that guides its response, often to prevent it from including the cue itself in its response.\n",
    "- **\"Time to think\"**: A quality in prompts that supports LLM responses (often requiring calculation) by asking for the model to take multiple steps and show its work."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8e05bfc-8d95-4f64-8aca-32a9d5322bfc",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Restart the Kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c726c6cc-27e7-4f70-98a0-8bee153d1b38",
   "metadata": {},
   "source": [
    "In order to free up GPU memory for the next notebook, please run the following cell to restart the kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f288ecb-54dc-4f4e-8cc9-54230e5f5ab3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from IPython import get_ipython\n",
    "\n",
    "get_ipython().kernel.do_shutdown(restart=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "873337fb-d71e-4a08-9426-ced44978af4b",
   "metadata": {},
   "source": [
    "![DLI Header](images/DLI_Header.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
