{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6f4b0ba3-1eca-4e08-a935-8aa7f4465c40",
   "metadata": {},
   "source": [
    "![DLI Header](images/DLI_Header.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4eb1df6-6e7e-4a84-bd61-5d72091226e8",
   "metadata": {},
   "source": [
    "# Chatbot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a78aff64-cba8-490f-a48e-45966b2e68b9",
   "metadata": {},
   "source": [
    "In this notebook you will begin to create chatbot functionality, creating an AI bot capable of retaining conversation history."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "504b12a7-0bf2-4974-a6c4-05b5a7b64644",
   "metadata": {},
   "source": [
    "## Learning Objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25eb791e-c828-4593-889b-2c95068ed2ff",
   "metadata": {},
   "source": [
    "By the time you complete this notebook you will be able to:\n",
    "- Create chatbot functionality from our LLaMA-2 model, capable of retaining conversation history."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da4d6b40-a690-46a9-b801-ffb5151dedcb",
   "metadata": {},
   "source": [
    "## Video Walkthrough"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af22e3ab-3be1-4885-8906-169b96a6ec32",
   "metadata": {},
   "source": [
    "Execute the cell below to load the video walkthrough of this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a147ae85-0e68-487f-8c4e-99dbbe704310",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    " from IPython.display import HTML\n",
    "\n",
    "video_url = \"https://d36m44n9vdbmda.cloudfront.net/assets/s-fx-12-v1/v2/06-chatbot.mp4\"\n",
    "\n",
    "video_html = f\"\"\"\n",
    "<video controls width=\"640\" height=\"360\">\n",
    "    <source src=\"{video_url}\" type=\"video/mp4\">\n",
    "    Your browser does not support the video tag.\n",
    "</video>\n",
    "\"\"\"\n",
    "\n",
    "display(HTML(video_html))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63f19484-acf2-4669-bbca-298c1c512bd7",
   "metadata": {},
   "source": [
    "## Create LLaMA-2 Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab62455a-b3bf-41eb-8499-b85dc053c31f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "model = \"TheBloke/Llama-2-13B-chat-GPTQ\"\n",
    "# model = \"TheBloke/Llama-2-7B-chat-GPTQ\"\n",
    "\n",
    "llama_pipe = pipeline(\"text-generation\", model=model, device_map=\"auto\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82975443-881e-4188-a5dc-c4f44ceb58ea",
   "metadata": {},
   "source": [
    "## Helper Functions and Classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ee245ec-bfef-4fe3-9617-31a5b2893b70",
   "metadata": {},
   "source": [
    "In this notebook we will use the following functions and classes to support our interaction with the LLM. Feel free to skim over them presently, as they are covered in greater detail when used below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c77ceca2-1a25-4ede-8cf8-11f98dcd14e6",
   "metadata": {},
   "source": [
    "### Generate Model Responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42343e1f-40d8-4b9a-a56c-53f60774dcd5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate(prompt, max_length=1024, pipe=llama_pipe, **kwargs):\n",
    "    \"\"\"\n",
    "    Generates a response to the given prompt using a specified language model pipeline.\n",
    "\n",
    "    This function takes a prompt and passes it to a language model pipeline, such as LLaMA, \n",
    "    to generate a text response. The function is designed to allow customization of the \n",
    "    generation process through various parameters and keyword arguments.\n",
    "\n",
    "    Parameters:\n",
    "    - prompt (str): The input text prompt to generate a response for.\n",
    "    - max_length (int): The maximum length of the generated response. Default is 1024 tokens.\n",
    "    - pipe (callable): The language model pipeline function used for generation. Default is llama_pipe.\n",
    "    - **kwargs: Additional keyword arguments that are passed to the pipeline function.\n",
    "\n",
    "    Returns:\n",
    "    - str: The generated text response from the model, trimmed of leading and trailing whitespace.\n",
    "\n",
    "    Example usage:\n",
    "    ```\n",
    "    prompt_text = \"Explain the theory of relativity.\"\n",
    "    response = generate(prompt_text, max_length=512, pipe=my_custom_pipeline, temperature=0.7)\n",
    "    print(response)\n",
    "    ```\n",
    "    \"\"\"\n",
    "\n",
    "    def_kwargs = dict(return_full_text=False, return_dict=False)\n",
    "    response = pipe(prompt.strip(), max_length=max_length, **kwargs, **def_kwargs)\n",
    "    return response[0]['generated_text'].strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77d2cbc1-a033-48bd-9f84-39aed664a4de",
   "metadata": {},
   "source": [
    "### Costruct Prompt, Optionally With System Context and/or Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40abda7d-fffc-4afb-bbb9-922dea42057b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def construct_prompt_with_context(main_prompt, system_context=\"\", conversation_examples=[]):\n",
    "    \"\"\"\n",
    "    Constructs a complete structured prompt for a language model, including optional system context and conversation examples.\n",
    "\n",
    "    This function compiles a prompt that can be directly used for generating responses from a language model. \n",
    "    It creates a structured format that begins with an optional system context message, appends a series of conversational \n",
    "    examples as prior interactions, and ends with the main user prompt. If no system context or conversation examples are provided,\n",
    "    it will return only the main prompt.\n",
    "\n",
    "    Parameters:\n",
    "    - main_prompt (str): The core question or statement for the language model to respond to.\n",
    "    - system_context (str, optional): Additional context or information about the scenario or environment. Defaults to an empty string.\n",
    "    - conversation_examples (list of tuples, optional): Prior exchanges provided as context, where each tuple contains a user message \n",
    "      and a corresponding agent response. Defaults to an empty list.\n",
    "\n",
    "    Returns:\n",
    "    - str: A string formatted as a complete prompt ready for language model input. If no system context or examples are provided, returns the main prompt.\n",
    "\n",
    "    Example usage:\n",
    "    ```\n",
    "    main_prompt = \"I'm looking to improve my dialogue writing skills for my next short story. Any suggestions?\"\n",
    "    system_context = \"User is an aspiring author seeking to enhance dialogue writing techniques.\"\n",
    "    conversation_examples = [\n",
    "        (\"How can dialogue contribute to character development?\", \"Dialogue should reveal character traits and show personal growth over the story arc.\"),\n",
    "        (\"What are some common pitfalls in writing dialogue?\", \"Avoid exposition dumps in dialogue and make sure each character's voice is distinct.\")\n",
    "    ]\n",
    "\n",
    "    full_prompt = construct_prompt_with_context(main_prompt, system_context, conversation_examples)\n",
    "    print(full_prompt)\n",
    "    ```\n",
    "    \"\"\"\n",
    "    \n",
    "    # Return the main prompt if no system context or conversation examples are provided\n",
    "    if not system_context and not conversation_examples:\n",
    "        return main_prompt\n",
    "\n",
    "    # Start with the initial part of the prompt including the system context, if provided\n",
    "    full_prompt = f\"<s>[INST] <<SYS>>{system_context}<</SYS>>\\n\" if system_context else \"<s>[INST]\\n\"\n",
    "\n",
    "    # Add each example from the conversation_examples to the prompt\n",
    "    for user_msg, agent_response in conversation_examples:\n",
    "        full_prompt += f\"{user_msg} [/INST] {agent_response} </s><s>[INST]\"\n",
    "\n",
    "    # Add the main user prompt at the end\n",
    "    full_prompt += f\"{main_prompt} [/INST]\"\n",
    "\n",
    "    return full_prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a17c1228-3280-4798-9558-e19139b77b6c",
   "metadata": {},
   "source": [
    "### LlamaChatbot Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36e18ccd-72e4-40aa-a2fb-d8359fb5fda8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class LlamaChatbot:\n",
    "    \"\"\"\n",
    "    A chatbot interface for generating conversational responses using the LLaMA language model.\n",
    "\n",
    "    Attributes:\n",
    "    - system_context (str): Contextual information to provide to the language model for all conversations.\n",
    "    - conversation_history (list of tuples): Stores the history of the conversation, where each\n",
    "      tuple contains a user message and the corresponding agent response.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, system_context):\n",
    "        \"\"\"\n",
    "        Initializes a new instance of the LlamaChatbot class.\n",
    "\n",
    "        Parameters:\n",
    "        - system_context (str): A string that sets the initial context for the language model.\n",
    "        \"\"\"\n",
    "        self.system_context = system_context\n",
    "        self.conversation_history = []  # Initializes the conversation history\n",
    "\n",
    "    def chat(self, user_msg):\n",
    "        \"\"\"\n",
    "        Generates a response from the chatbot based on the user's message.\n",
    "\n",
    "        This method constructs a prompt with the current system context and conversation history,\n",
    "        sends it to the language model, and then stores the new user message and model's response\n",
    "        in the conversation history.\n",
    "\n",
    "        Parameters:\n",
    "        - user_msg (str): The user's message to which the chatbot will respond.\n",
    "\n",
    "        Returns:\n",
    "        - str: The generated response from the chatbot.\n",
    "        \"\"\"\n",
    "        # Generate the prompt using the conversation history and the new user message\n",
    "        prompt = construct_prompt_with_context(user_msg, self.system_context, self.conversation_history)\n",
    "        \n",
    "        # Get the model's response\n",
    "        agent_response = generate(prompt)\n",
    "\n",
    "        # Store this interaction in the conversation history\n",
    "        self.conversation_history.append((user_msg, agent_response))\n",
    "\n",
    "        return agent_response\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        Resets the conversation history of the chatbot.\n",
    "\n",
    "        This method clears the existing conversation history, effectively restarting the conversation.\n",
    "        \"\"\"\n",
    "        # Clear conversation history\n",
    "        self.conversation_history = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d710c5b-5ac6-4f34-b0c1-e6518af71615",
   "metadata": {},
   "source": [
    "## No Conversation Memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e530c8c9-0156-49c3-9670-b41abd86410e",
   "metadata": {},
   "source": [
    "Let's strike up a conversation with our LLM. We are going to provide it a **system context** that it should be a friendly chatbot, and (naively) encourage it to always remember our name if provided in a conversation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10bd66e2-8337-4eb9-9f0a-3e49939d0247",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "system_context = \"\"\"\n",
    "You are a friendly chatbot always eager to help and engage in meaningful conversation. \\\n",
    "You always remember the details of our previous conversations, \\\n",
    "especially if a user gives them their name.\n",
    "\"\"\"\n",
    "\n",
    "prompt = \"Hello my name is Star. Nice to meet you!\"\n",
    "\n",
    "print(generate(construct_prompt_with_context(prompt, system_context)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe86fb71-f720-458f-af70-c897efa81990",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "837cd227-0900-498a-9f4d-bcdf0a9e77bc",
   "metadata": {},
   "source": [
    "The model certainly appears to be eager to show that it remembers who we are. Let's see what happens when we actually put its name-retention to the test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2d0bcd6-11f5-4f33-9a8e-f657ce4a4ca8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "system_context = \"\"\"\n",
    "You are a friendly chatbot always eager to help and engage in meaningful conversation. \\\n",
    "You always remember the details of our previous conversations, \\\n",
    "especially if a user gives them their name.\n",
    "\"\"\"\n",
    "\n",
    "prompt = \"Can you remind me what my name is?\"\n",
    "\n",
    "print(generate(construct_prompt_with_context(prompt, system_context)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51c9400d-64a8-4107-a441-6cb70004e603",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45217bd0-699e-4126-9e69-f2f4ab22ce38",
   "metadata": {},
   "source": [
    "It likely comes as no surprise that the model does not remember our name because in spite of how it presents itself to us, we have provided it with no capacity to remember any of the details from previous conversational exchanges. The model seems to insist that our name is \"Emily,\" which is clearly incorrect. When models generate responses that are fabricated, often with confidence, we call this **hallucination**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ffb9670-9447-43bd-9f42-1c36adc9fe00",
   "metadata": {},
   "source": [
    "## Create Conversation Memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb21775d-758f-4f92-9224-7227156f3243",
   "metadata": {},
   "source": [
    "In order to create a chatbot experience where the model can retain information from previous exchanges, we are going to use a `LlamaChatbot` class (defined above). Here's the `help` output from our class definition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdf3e428-7890-4e39-9f7c-639044c947d4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "help(LlamaChatbot)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18d28517-0239-450d-a077-ff087cbe8226",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51ecb676-bdf1-4e58-a150-dcfc7e09e2e8",
   "metadata": {},
   "source": [
    "Most pertinent to our immediate goal is the creation of a `conversation_history` list that we will append to anytime we invoke the `chat` method. We will be reusing some of the same logic from previous notebooks, in particular leveraging the LLaMA-2 **prompt template**, so that each user/model interaction if formatted properly, and then prepended to the prompt for subsequent exchanges between the user and the model.\n",
    "\n",
    "It would be accurate to say that with each interaction, we are performing **few-shot learning** where the instructive examples are simply the previous interactions.\n",
    "\n",
    "Let's see it in action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f807d3b9-00a0-428d-beb4-07e061fcc06d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "system_context = \"\"\"\n",
    "You are a friendly chatbot always eager to help and engage in meaningful conversation. You are always kind \\\n",
    "but also repectful and professional.\n",
    "\"\"\"\n",
    "\n",
    "chatbot = LlamaChatbot(system_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f94b8d9d-b76a-4df8-9886-eeca66fccaff",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(chatbot.chat(\"Hi, my name is Star. Nice to meet you!\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d8995df-c073-4f59-b098-f5c9034c285b",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "932831fc-9227-4aef-992e-21e2926f24a9",
   "metadata": {},
   "source": [
    "So far so good. Let's see now if the model is able to \"recall\" our name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2950da7-9863-4765-8750-da1de864e4fe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(chatbot.chat(\"Can you remind me what my name is?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c83c561b-f85d-4771-ae86-dfba2d58e973",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb67e3de-2ac6-4aeb-b251-337df87a7846",
   "metadata": {},
   "source": [
    "Success! Let's take a look at the model's conversation history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "073ad5ac-3e79-40ac-8291-ad811aa0bd6c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "chatbot.conversation_history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ad988f1-c292-41fd-9660-4d9eb066fcea",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45d5d2ef-27aa-4bbe-922b-bd025c82010a",
   "metadata": {},
   "source": [
    "Given that `conversation_history` is prepended to each new prompt, it makes sense that the model is able to generate responses based on previous exchanges.\n",
    "\n",
    "The `reset` method will clear `conversation_history`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "729a3ad7-2d20-4cc8-8b96-95254b4e0e51",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "chatbot.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "543f81f1-aefe-4334-a470-cbca4a521d64",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(chatbot.chat(\"Can you remind me what my name is?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7defa717-6ade-4757-894d-8ddd544f5d1c",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f22e5d0-457d-49e8-b718-72c37a5d8722",
   "metadata": {},
   "source": [
    "And now it is no surprise that the model is unable to \"recall\" details from our previous exchanges."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ca0b735-208a-4b3d-be23-17a6a17262e2",
   "metadata": {},
   "source": [
    "## Exercise: Task Tracker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "913bdea1-877b-484b-bb7d-39f116f908ec",
   "metadata": {},
   "source": [
    "Create an assistant that can keep track of what you need to get done today. It should be able to add and remove things from your list based on your dialogue, and at any given time, accurately remind you of what you have left to do.\n",
    "\n",
    "See the solution below if you get stuck."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00f41c8e-3220-464c-b411-f6598e9b50d6",
   "metadata": {},
   "source": [
    "### Your Work Here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc457e8-33de-4e41-9cb2-d583a7095f8f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "01eaf61f-9fe0-47f1-95f5-9902fb336469",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f49ea64-5bab-4fa1-9a1d-f311b8c9fc32",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "system_context = \"\"\"\n",
    "You are an assistant that help me keep track of what I need to do. You keep track of tasks that I \\\n",
    "provide you, and when asked remind me of what I have left to do.\n",
    "\"\"\"\n",
    "\n",
    "chatbot = LlamaChatbot(system_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42af759c-4e54-442d-a823-7dcda80c1708",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(chatbot.chat(\"I need to do the following things today: eat breakfast, eat lunch, eat dinner, go to work, exercise, and clean the house\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efdac879-04c6-4dc8-bdb6-5a46ad1cbd86",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(chatbot.chat(\"Yes, I also need to hang out with friends.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70360868-62f1-4029-98c0-f20f9ad80d10",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(chatbot.chat(\"Okay, I'm done eating breakfast and exercising.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d488ee-64d4-41ee-b59b-aae1b15ea4e5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(chatbot.chat(\"I've eaten lunch. Sometime today I need to call the bike shop.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1afe3a2-e054-4c89-bd98-9c3b9026b3e9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(chatbot.chat(\"I finished work, hung out with friends, cleaned the house, and called the bike shop.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c333c312-dd3b-401f-a6cc-1281f6ba4b91",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(chatbot.chat(\"I just ate dinner. Now I just need to go to sleep.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b823025-3bbd-4958-a5ef-acbcca859654",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "chatbot.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05a8ae85-9c93-463a-a327-3d93397a412b",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Key Concept Review"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc97760f-ee70-4a39-ade0-9fe63231ec5b",
   "metadata": {},
   "source": [
    "The following key concepts were introduced in this notebook:\n",
    "\n",
    "- **Hallucination:** When a model generates, often with some expressed confidence, untrue or inaccurate responses."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd47cc91-8655-4a58-98dc-75b4dfc26cbb",
   "metadata": {},
   "source": [
    "## Optional Advanced Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2f282b3-1d7f-4f49-96e5-e803692d3276",
   "metadata": {},
   "source": [
    "**NOTE:** In the next notebook we are going to discover limits to the amount of conversation the model is able to store before things start to go wrong. With that in mind, and before suggesting additional experimentation, if you find that the model is starting to produce only empty responses, move on to the next section so you can learn what this is all about."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "834be291-bf2d-4f78-9038-81af05bb8981",
   "metadata": {},
   "source": [
    "If you'd like to go above and beyond the requirements of the course, below are some additional open-ended exercises for you to try."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9c74a75-ba20-4682-bb42-fed8cbe4f647",
   "metadata": {},
   "source": [
    "### Use the 7B Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33f8b94c-afe7-4131-be09-84e73072e533",
   "metadata": {},
   "source": [
    "At the top of the notebook, after restarting the kernel (see cell below), uncomment and use the 7B model instead of the 13B model we demoed. Try to get satisfying results in spite of using the smaller (weaker) model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28c23284-7338-4036-a80a-e543186f951c",
   "metadata": {},
   "source": [
    "### Make a Helper Bot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c259b51-701d-4b4e-92ba-f178302cd8a7",
   "metadata": {},
   "source": [
    "Make a bot that someone can use when they are having a hard day, that gives encouragement, praise, and empathy, and knows when to respond and when to ask for more questions about what is going on for the user."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "013befef-423b-4947-8bf3-3cfe9f28ae49",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Restart the Kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0299caa-5f61-4e9a-b2fd-004b771ca711",
   "metadata": {},
   "source": [
    "In order to free up GPU memory for the next notebook, please run the following cell to restart the kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ce3340b-1e90-4485-abfb-bdaab9cc0894",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from IPython import get_ipython\n",
    "\n",
    "get_ipython().kernel.do_shutdown(restart=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1528b26a-6725-405f-a845-1300f4022626",
   "metadata": {},
   "source": [
    "![DLI Header](images/DLI_Header.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
