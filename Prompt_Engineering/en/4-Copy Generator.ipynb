{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "79cfdbf4-16a0-4b49-8ec8-146731290429",
   "metadata": {},
   "source": [
    "![DLI Header](images/DLI_Header.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af339990-a99d-484a-86e1-68cd5f284f16",
   "metadata": {},
   "source": [
    "# Star Bikes Marketing Copy Generator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f473856a-e4a0-40f2-9ee5-2aed42e2d0e1",
   "metadata": {},
   "source": [
    "In this notebook you will build an AI-powered marketing copy writer, capable to perform a number of generative tasks. You will learn how to edit the model's **system message** to define its role for response generation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1fcef33-9af0-4fbd-a95d-bafcd4345ce4",
   "metadata": {},
   "source": [
    "## Learning Objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be50e4b0-e588-4f97-8d60-6860827aa997",
   "metadata": {},
   "source": [
    "By the time you complete this notebook you will be able to:\n",
    "- Perform a variety of **text generation** tasks using LLaMA-2.\n",
    "- Use **sytem context** to provide the LLaMA-2 model with a definition of its overarching role."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c59404a-c8b1-4227-a3f2-da0eeffe3b7d",
   "metadata": {},
   "source": [
    "## Video Walkthrough"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8d86f5e-b354-4742-b771-4c0f2ede7cb3",
   "metadata": {},
   "source": [
    "Execute the cell below to load the video walkthrough of this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c5783c7-0fa8-4787-a3de-93b40aa74dc6",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    " from IPython.display import HTML\n",
    "\n",
    "video_url = \"https://d36m44n9vdbmda.cloudfront.net/assets/s-fx-12-v1/v2/04-copy.mp4\"\n",
    "\n",
    "video_html = f\"\"\"\n",
    "<video controls width=\"640\" height=\"360\">\n",
    "    <source src=\"{video_url}\" type=\"video/mp4\">\n",
    "    Your browser does not support the video tag.\n",
    "</video>\n",
    "\"\"\"\n",
    "\n",
    "display(HTML(video_html))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8d7f877-c256-4808-b29f-0165f66870b7",
   "metadata": {},
   "source": [
    "## Create LLaMA-2 Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4939afbd-23ca-4db8-b12b-209c224ee4ab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "model = \"TheBloke/Llama-2-13B-chat-GPTQ\"\n",
    "# model = \"TheBloke/Llama-2-7B-chat-GPTQ\"\n",
    "\n",
    "llama_pipe = pipeline(\"text-generation\", model=model, device_map=\"auto\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "682be0d3-4ba9-4a89-9db3-f8ec4b311185",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f6d6710-8530-44e3-b7d8-0899e948bada",
   "metadata": {},
   "source": [
    "In this notebook we will use the following functions to support our interaction with the LLM. Feel free to skim over them presently, as they are covered in greater detail when used below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "408e3628-7fee-4202-8734-2e8da3adf574",
   "metadata": {},
   "source": [
    "### Generate Model Responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fda46556-8b36-407d-94d0-1483cb065e9c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate(prompt, max_length=1024, pipe=llama_pipe, **kwargs):\n",
    "    \"\"\"\n",
    "    Generates a response to the given prompt using a specified language model pipeline.\n",
    "\n",
    "    This function takes a prompt and passes it to a language model pipeline, such as LLaMA, \n",
    "    to generate a text response. The function is designed to allow customization of the \n",
    "    generation process through various parameters and keyword arguments.\n",
    "\n",
    "    Parameters:\n",
    "    - prompt (str): The input text prompt to generate a response for.\n",
    "    - max_length (int): The maximum length of the generated response. Default is 1024 tokens.\n",
    "    - pipe (callable): The language model pipeline function used for generation. Default is llama_pipe.\n",
    "    - **kwargs: Additional keyword arguments that are passed to the pipeline function.\n",
    "\n",
    "    Returns:\n",
    "    - str: The generated text response from the model, trimmed of leading and trailing whitespace.\n",
    "\n",
    "    Example usage:\n",
    "    ```\n",
    "    prompt_text = \"Explain the theory of relativity.\"\n",
    "    response = generate(prompt_text, max_length=512, pipe=my_custom_pipeline, temperature=0.7)\n",
    "    print(response)\n",
    "    ```\n",
    "    \"\"\"\n",
    "\n",
    "    def_kwargs = dict(return_full_text=False, return_dict=False)\n",
    "    response = pipe(prompt.strip(), max_length=max_length, **kwargs, **def_kwargs)\n",
    "    return response[0]['generated_text'].strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52e07bb2-1cee-4785-99b9-b67fe2fb5a5d",
   "metadata": {},
   "source": [
    "### Costruct Prompt, Optionally With System Context and/or Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d4c9a4f-570b-4de7-b46c-8aba8a571edc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def construct_prompt_with_context(main_prompt, system_context=\"\", conversation_examples=[]):\n",
    "    \"\"\"\n",
    "    Constructs a complete structured prompt for a language model, including optional system context and conversation examples.\n",
    "\n",
    "    This function compiles a prompt that can be directly used for generating responses from a language model. \n",
    "    It creates a structured format that begins with an optional system context message, appends a series of conversational \n",
    "    examples as prior interactions, and ends with the main user prompt. If no system context or conversation examples are provided,\n",
    "    it will return only the main prompt.\n",
    "\n",
    "    Parameters:\n",
    "    - main_prompt (str): The core question or statement for the language model to respond to.\n",
    "    - system_context (str, optional): Additional context or information about the scenario or environment. Defaults to an empty string.\n",
    "    - conversation_examples (list of tuples, optional): Prior exchanges provided as context, where each tuple contains a user message \n",
    "      and a corresponding agent response. Defaults to an empty list.\n",
    "\n",
    "    Returns:\n",
    "    - str: A string formatted as a complete prompt ready for language model input. If no system context or examples are provided, returns the main prompt.\n",
    "\n",
    "    Example usage:\n",
    "    ```\n",
    "    main_prompt = \"I'm looking to improve my dialogue writing skills for my next short story. Any suggestions?\"\n",
    "    system_context = \"User is an aspiring author seeking to enhance dialogue writing techniques.\"\n",
    "    conversation_examples = [\n",
    "        (\"How can dialogue contribute to character development?\", \"Dialogue should reveal character traits and show personal growth over the story arc.\"),\n",
    "        (\"What are some common pitfalls in writing dialogue?\", \"Avoid exposition dumps in dialogue and make sure each character's voice is distinct.\")\n",
    "    ]\n",
    "\n",
    "    full_prompt = construct_prompt_with_context(main_prompt, system_context, conversation_examples)\n",
    "    print(full_prompt)\n",
    "    ```\n",
    "    \"\"\"\n",
    "    \n",
    "    # Return the main prompt if no system context or conversation examples are provided\n",
    "    if not system_context and not conversation_examples:\n",
    "        return main_prompt\n",
    "\n",
    "    # Start with the initial part of the prompt including the system context, if provided\n",
    "    full_prompt = f\"<s>[INST] <<SYS>>{system_context}<</SYS>>\\n\" if system_context else \"<s>[INST]\\n\"\n",
    "\n",
    "    # Add each example from the conversation_examples to the prompt\n",
    "    for user_msg, agent_response in conversation_examples:\n",
    "        full_prompt += f\"{user_msg} [/INST] {agent_response} </s><s>[INST]\"\n",
    "\n",
    "    # Add the main user prompt at the end\n",
    "    full_prompt += f\"{main_prompt} [/INST]\"\n",
    "\n",
    "    return full_prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47884189-00e6-4c0d-8e5c-edf41411f088",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Star Bikes  Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a27cac-12ac-4f7c-8065-bb541eb82394",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "bikes = [\n",
    "    {\n",
    "        \"model\": \"Galaxy Rider\",\n",
    "        \"type\": \"Mountain\",\n",
    "        \"features\": {\n",
    "            \"frame\": \"Aluminum alloy\",\n",
    "            \"gears\": \"21-speed Shimano\",\n",
    "            \"brakes\": \"Hydraulic disc\",\n",
    "            \"tires\": \"27.5-inch all-terrain\",\n",
    "            \"suspension\": \"Full, adjustable\",\n",
    "            \"color\": \"Matte black with green accents\"\n",
    "        },\n",
    "        \"usps\": [\"Lightweight frame\", \"Quick gear shift\", \"Durable tires\"],\n",
    "        \"price\": 799.95,\n",
    "        \"internal_id\": \"GR2321\",\n",
    "        \"weight\": \"15.3 kg\",\n",
    "        \"manufacturer_location\": \"Taiwan\"\n",
    "    },\n",
    "    {\n",
    "        \"model\": \"Nebula Navigator\",\n",
    "        \"type\": \"Hybrid\",\n",
    "        \"features\": {\n",
    "            \"frame\": \"Carbon fiber\",\n",
    "            \"gears\": \"18-speed Nexus\",\n",
    "            \"brakes\": \"Mechanical disc\",\n",
    "            \"tires\": \"26-inch city slick\",\n",
    "            \"suspension\": \"Front only\",\n",
    "            \"color\": \"Glossy white\"\n",
    "        },\n",
    "        \"usps\": [\"Sleek design\", \"Efficient on both roads and trails\", \"Ultra-lightweight\"],\n",
    "        \"price\": 649.99,\n",
    "        \"internal_id\": \"NN4120\",\n",
    "        \"weight\": \"13.5 kg\",\n",
    "        \"manufacturer_location\": \"Germany\"\n",
    "    },\n",
    "    {\n",
    "        \"model\": \"Cosmic Comet\",\n",
    "        \"type\": \"Road\",\n",
    "        \"features\": {\n",
    "            \"frame\": \"Titanium\",\n",
    "            \"gears\": \"24-speed Campagnolo\",\n",
    "            \"brakes\": \"Rim brakes\",\n",
    "            \"tires\": \"700C road\",\n",
    "            \"suspension\": \"None\",\n",
    "            \"color\": \"Metallic blue\"\n",
    "        },\n",
    "        \"usps\": [\"Super aerodynamic\", \"High-speed performance\", \"Professional-grade components\"],\n",
    "        \"price\": 1199.50,\n",
    "        \"internal_id\": \"CC5678\",\n",
    "        \"weight\": \"11 kg\",\n",
    "        \"manufacturer_location\": \"Italy\"\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32a4dc85-0f8b-4bb1-a41c-f9d6ee3159b9",
   "metadata": {},
   "source": [
    "## The Full LLaMA-2 Prompt Template"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc46532c-53c4-462f-b8dd-69923319d6da",
   "metadata": {},
   "source": [
    "In the previous notebook, we leveraged the LLaMA-2 **prompt template** to support **few-shot learning**, but mentioned that we were using a slightly modified version of the prompt template. Specifically, we left out a section of the prompt template's user message called the **system message**, or **system context**, or **system prompt** (terms which we will use interchangeably). Below is the entire LLaMA-2 prompt template, including its section for **system context**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79d27794-a34e-4224-a034-072141a3c94a",
   "metadata": {
    "tags": []
   },
   "source": [
    "```python\n",
    "<s>[INST] <<SYS>>\n",
    "{{ system_context }}\n",
    "<</SYS>>\n",
    "\n",
    "{{ user_msg_1 }} [/INST] {{ model_answer_1 }} </s>\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0a99577-670c-4dc2-863e-ea76b411c7ef",
   "metadata": {},
   "source": [
    "**System context** is a part of the user side of the user/model interaction, and goes in between the `<<SYS>>` and `<</SYS>>` tags. The **system context** is a preliminary statement or contextual cue designed to orient an AI model's response towards a specific framework or understanding of a task.\n",
    "\n",
    "There are no hard and fast rules about what belongs in the **system context** but we should consider it primarily to set the role of the model, or any context that will apply all of its responses.\n",
    "\n",
    "Here is the default **system message** for the LLaMA-2 chat model, used during its instruction fine-tuning:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6928fa1-d99a-47af-a69e-23d3c2bac996",
   "metadata": {
    "tags": []
   },
   "source": [
    ">You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe. Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\n",
    ">If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9428f44-b399-4280-9011-68b08bc251e7",
   "metadata": {},
   "source": [
    "## Setting the System Context"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53a8cb5e-1a77-4156-9b3e-bbfb3eed1b1c",
   "metadata": {},
   "source": [
    "\n",
    "The following `construct_prompt_with_context` function will help us constuct prompts with an updated **system message** using the LLaMA-2 **prompt template**. This function also allows us, should we wish, to perform **few-shot learning** by passing in a list of 2-tuple example interactions, as we did in the last section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "798d623e-8ff3-4142-8d63-93848c5a82c1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def construct_prompt_with_context(main_prompt, system_context=\"\", conversation_examples=[]):\n",
    "    \"\"\"\n",
    "    Constructs a complete structured prompt for a language model, including optional system context and conversation examples.\n",
    "\n",
    "    This function compiles a prompt that can be directly used for generating responses from a language model. \n",
    "    It creates a structured format that begins with an optional system context message, appends a series of conversational \n",
    "    examples as prior interactions, and ends with the main user prompt. If no system context or conversation examples are provided,\n",
    "    it will return only the main prompt.\n",
    "\n",
    "    Parameters:\n",
    "    - main_prompt (str): The core question or statement for the language model to respond to.\n",
    "    - system_context (str, optional): Additional context or information about the scenario or environment. Defaults to an empty string.\n",
    "    - conversation_examples (list of tuples, optional): Prior exchanges provided as context, where each tuple contains a user message \n",
    "      and a corresponding agent response. Defaults to an empty list.\n",
    "\n",
    "    Returns:\n",
    "    - str: A string formatted as a complete prompt ready for language model input. If no system context or examples are provided, returns the main prompt.\n",
    "\n",
    "    Example usage:\n",
    "    ```\n",
    "    main_prompt = \"I'm looking to improve my dialogue writing skills for my next short story. Any suggestions?\"\n",
    "    system_context = \"User is an aspiring author seeking to enhance dialogue writing techniques.\"\n",
    "    conversation_examples = [\n",
    "        (\"How can dialogue contribute to character development?\", \"Dialogue should reveal character traits and show personal growth over the story arc.\"),\n",
    "        (\"What are some common pitfalls in writing dialogue?\", \"Avoid exposition dumps in dialogue and make sure each character's voice is distinct.\")\n",
    "    ]\n",
    "\n",
    "    full_prompt = construct_prompt_with_context(main_prompt, system_context, conversation_examples)\n",
    "    print(full_prompt)\n",
    "    ```\n",
    "    \"\"\"\n",
    "    \n",
    "    # Return the main prompt if no system context or conversation examples are provided\n",
    "    if not system_context and not conversation_examples:\n",
    "        return main_prompt\n",
    "\n",
    "    # Start with the initial part of the prompt including the system context, if provided\n",
    "    full_prompt = f\"<s>[INST] <<SYS>>{system_context}<</SYS>>\\n\" if system_context else \"<s>[INST]\\n\"\n",
    "\n",
    "    # Add each example from the conversation_examples to the prompt\n",
    "    for user_msg, agent_response in conversation_examples:\n",
    "        full_prompt += f\"{user_msg} [/INST] {agent_response} </s><s>[INST]\"\n",
    "\n",
    "    # Add the main user prompt at the end\n",
    "    full_prompt += f\"{main_prompt} [/INST]\"\n",
    "\n",
    "    return full_prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "249e919d-7f72-4aa6-9234-b5fd7c8baefa",
   "metadata": {},
   "source": [
    "## Star Bikes Marketing Copy Generator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8cb5d20-19bd-4571-b34e-68e8155dad6e",
   "metadata": {},
   "source": [
    "Let's employ our LLaMA-2 model to serve as a marketing copy generator. For its task we will provide it with a JSON object that gives relevant specifications about one of the Star Bikes bicycle models we want it to write copy for. If you haven't already, please check out the *Star Bikes Data* section above for the definition of `bikes`.\n",
    "\n",
    "We will begin with a simple prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82cc4a56-c930-48cd-8be7-532bcb4162ce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt = f\"\"\"\n",
    "Write marketing copy for the following bicycle: {bikes[0]}\n",
    "\"\"\"\n",
    "\n",
    "print(generate(prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53e9821b-de8d-4081-b208-9d1ee791d98d",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4724a9e-1266-4bd8-b0c7-9c29212a4c64",
   "metadata": {},
   "source": [
    "This is not bad, and knowing what you do already, you could probably iterate on a prompt that tightened up the model's response. But assuming we want our model to serve as a marketing copy writer, perhaps writing copy in a variety of formats, let's provide it knowledge about its role by providing it with **system context**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a54a453-585a-4e67-9133-e0fb55cb8873",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "system_context = f\"\"\"\n",
    "You are a marketing copy writer for Star Bikes.\n",
    "\"\"\"\n",
    "\n",
    "prompt = f\"\"\"\n",
    "{bikes[0]}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4902c7bb-db2a-46c3-a555-a433c57bbea9",
   "metadata": {},
   "source": [
    "Using our `construct_prompt_with_context` function we can now create a prompt that sets the **system context** appropriately for the LLaMA-2 prompt template."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "767b7503-8a06-4c50-b9b2-17d2b275daa3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt_with_system_context = construct_prompt_with_context(prompt, system_context)\n",
    "print(prompt_with_system_context)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7609f10-f4a4-4d50-9360-d218909c8185",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c674e694-8fe3-47bf-b123-5623a81f0dc2",
   "metadata": {},
   "source": [
    "Using our prompt with **system context**, let's see what kind of response we get back from the model. Worth mentioning is that our main `prompt` (see above) provides no instruction about what the model is to do. We are relying on the set **system context** to guide the model's behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b681bc97-63ae-4d37-8f2b-c18977c7b52c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(generate(prompt_with_system_context))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "decc9717-dd16-4334-9640-61a8d226f059",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c53c6b56-90ad-4f8a-8681-5ee828d50065",
   "metadata": {},
   "source": [
    "That's not bad at all, but let's be more **precise** in the **system context** that the model's role is to only generate marketing copy, and not any leading conversation text like `Sure! Here's the marketing copy for the Galaxy Rider mountain bike:` as we got in the last response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e8265b6-06e3-45ed-9969-56a4f7dcf894",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "system_context = f\"\"\"\n",
    "You are a marketing copy writer for Star Bikes. You only write marketing copy and never any \\\n",
    "leading comments or pieces of conversation.\n",
    "\"\"\"\n",
    "\n",
    "prompt = f\"\"\"\n",
    "{bikes[0]}\n",
    "\"\"\"\n",
    "\n",
    "print(generate(construct_prompt_with_context(prompt, system_context)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6261046-d977-4aba-95f2-0c52aa6a36c2",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1af91df7-ee21-4240-9cd1-5937262c0d92",
   "metadata": {},
   "source": [
    "That did not seem to help. Let's iterate. Perhaps if we tell the model it is a machine, it will not try to have human-like conversations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdfd1d32-3e8f-49a6-9a0f-22e2b89fd672",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "system_context = f\"\"\"\n",
    "You are a non-conversant machine that generates marketing copy in 100 words or less. You work for Star Bikes.\n",
    "\"\"\"\n",
    "\n",
    "prompt = f\"\"\"\n",
    "{bikes[0]}\n",
    "\"\"\"\n",
    "\n",
    "print(generate(construct_prompt_with_context(prompt, system_context)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c51f6bd0-9162-4035-809b-6fe273737ce6",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f4c2d4c-ead3-47d4-b6af-0f3b6c375925",
   "metadata": {},
   "source": [
    "Much better! Just like with all prompt engineering, developing effective **system prompts** is often an iterative process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0347268b-ce78-48f8-a24b-49cf0d3c244f",
   "metadata": {},
   "source": [
    "## Enforce Brevity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14bf3681-0133-4d33-b632-d4453dd94f83",
   "metadata": {},
   "source": [
    "Let's assume that we only want responses that are ~100 words. We can update the **system context** to reflect this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a959dda4-4196-425c-954f-75c10ff30545",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "system_context = f\"\"\"\n",
    "You are a non-conversant machine that generates marketing copy in 100 words or less. You work for Star Bikes.\n",
    "\"\"\"\n",
    "\n",
    "prompt = f\"\"\"\n",
    "{bikes[0]}\n",
    "\"\"\"\n",
    "\n",
    "print(generate(construct_prompt_with_context(prompt, system_context)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bde70ee8-01da-406e-a6bf-32b51ce522bc",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a8c36ba-7434-44d6-bb83-517800944c83",
   "metadata": {},
   "source": [
    "Excellent. Now that we have a setup that appears to be working well for us, let's try it out on data from the rest of our bikes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b75d271-072f-493d-bce2-4f9994b2a844",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for bike in bikes[1:]:\n",
    "    print(generate(construct_prompt_with_context(bike, system_context)))\n",
    "    print(\"\\n-----\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72596c88-4c23-4cd7-addf-1692c28c6c36",
   "metadata": {},
   "source": [
    "## Exercise: Generate Marketing Emails"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "691d517d-65da-4ccf-95ee-a80cbb468544",
   "metadata": {},
   "source": [
    "Using what you've learned so far, create a prompt (likely leveraging its **system context**) that writes marketing emails to a user for a specific bike. The email should address the recipient by their name.\n",
    "\n",
    "See the solution below if you get stuck."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd9392c0-bc95-4b03-b372-ba1fa7c25813",
   "metadata": {},
   "source": [
    "### Your Work Here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3b8ee94-39b6-40d0-9621-6452ed715014",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1b2ac353-a873-430b-8a10-c93e45419ae5",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2256a6e4-fec7-4d4c-80f9-697865ed4e77",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "system_context = f\"\"\"\n",
    "You are a non-conversant machine that generates marketing emails in 100 words or less. You work for Star Bikes.\n",
    "\"\"\"\n",
    "\n",
    "prompt = f\"\"\"\n",
    "Recipient Name: Stella\n",
    "{bikes[0]}\n",
    "\"\"\"\n",
    "\n",
    "print(generate(construct_prompt_with_context(prompt, system_context)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ada345a-39f1-42d7-a63a-7e97a1317978",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Key Concept Review"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7532c88-f77e-4fbf-8919-98bd4d53bd85",
   "metadata": {},
   "source": [
    "The following key concepts were introduced in this notebook:\n",
    "\n",
    "- **System Message:** Part of instruction fine-tuned models' prompt templates that allow users to set the role or overarching context of its behavior."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bfb5f5f-fb8a-458d-8b2d-14164857164d",
   "metadata": {},
   "source": [
    "## Optional Advanced Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff817bde-7bbb-4a88-9a0b-887ca533f702",
   "metadata": {},
   "source": [
    "If you'd like to go above and beyond the requirements of the course, below are some additional open-ended exercises for you to try."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81cf8fdb-7ddc-446e-b530-d25eb8a25069",
   "metadata": {},
   "source": [
    "### Use the 7B Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc567fb7-8364-4c8c-95a1-0a510036fa0c",
   "metadata": {},
   "source": [
    "At the top of the notebook, after restarting the kernel (see cell below), uncomment and use the 7B model instead of the 13B model we demoed. Try to get satisfying results in spite of using the smaller (weaker) model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76cbccb1-bc7e-4b72-9f25-37b797095749",
   "metadata": {},
   "source": [
    "### Experiment with What Works Best"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dd98e41-ec75-4fc2-b51d-94139083abd8",
   "metadata": {},
   "source": [
    "We arrived upon several effective prompts using a combination of iterating on a basic prompt, editing the system message, and providing examples (aka \"shots\") to help the model. It's often more of an art than a science which approach will work better: see if you can get acceptable results by emphasizing changes to all 3 of these \"levers\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20744407-6de1-4ac3-b08e-dc5e75972cb9",
   "metadata": {},
   "source": [
    "### Make an Email Generation Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cf2d2a7-dea2-4b9c-a140-80b639e7e0ac",
   "metadata": {},
   "source": [
    "Expand on the work from the exercise above to create a pipeline that given a collection of recipients, can generate emails for each of them. You might consider creating synthetic user data with more than just the recipient's name, such as bike's they have previously purchased or expressed interest in, whether an email has already been sent to them, etc., and then generating emails that are more relevant to these details.\n",
    "\n",
    "You might also consider doing more in the context of **few-shot** learning to structure the emails in a specific manner."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37acb023-5f07-4bb9-8343-c7e3230deea6",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Restart the Kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46a8ecaf-b860-487f-8b4d-b4dd358fa06f",
   "metadata": {},
   "source": [
    "In order to free up GPU memory for the next notebook, please run the following cell to restart the kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "379dc277-e2f2-4ac3-8972-842ad073edb5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from IPython import get_ipython\n",
    "\n",
    "get_ipython().kernel.do_shutdown(restart=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ead6c0a2-798a-460d-b025-40735e64f6af",
   "metadata": {},
   "source": [
    "![DLI Header](images/DLI_Header.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
