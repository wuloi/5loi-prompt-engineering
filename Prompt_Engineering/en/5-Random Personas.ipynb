{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1d81d68e-e246-4374-b4f9-02929a8e6696",
   "metadata": {},
   "source": [
    "![DLI Header](images/DLI_Header.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "706e8c2b-5f1b-4b7b-9ece-b6b341e8b49d",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Random Personas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52e22814-a66d-4646-9966-9843a433ce70",
   "metadata": {},
   "source": [
    "In this notebook you will learn how to control the **temperature** of the model's generation, giving you a measure of control over how random its responses are. Leveraging **temperature**, you'll be able to create a variety of AI personalities."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ff739fa-f350-4b50-b3ad-d19fbefd4e6f",
   "metadata": {},
   "source": [
    "## Learning Objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daefff31-716d-42f7-8362-26a10145727d",
   "metadata": {},
   "source": [
    "By the time you complete this notebook you will be able to:\n",
    "- Explain how random **sampling** can make for non-deterministic LLM responses.\n",
    "- Control the degree of randomness in a response by adjusting its **temperature**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a637834-47c5-47c6-8a5f-2e460c8c7401",
   "metadata": {},
   "source": [
    "## Video Walkthrough"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe924aa8-0cc7-4f28-be75-aab8da75f2e1",
   "metadata": {},
   "source": [
    "Execute the cell below to load the video walkthrough of this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4ab4a4a-82bf-4eb2-95cb-cc37f06b7469",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    " from IPython.display import HTML\n",
    "\n",
    "video_url = \"https://d36m44n9vdbmda.cloudfront.net/assets/s-fx-12-v1/v2/05-random.mp4\"\n",
    "\n",
    "video_html = f\"\"\"\n",
    "<video controls width=\"640\" height=\"360\">\n",
    "    <source src=\"{video_url}\" type=\"video/mp4\">\n",
    "    Your browser does not support the video tag.\n",
    "</video>\n",
    "\"\"\"\n",
    "\n",
    "display(HTML(video_html))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44272604-6cca-4cb4-b19d-2b8f415ebd74",
   "metadata": {},
   "source": [
    "## Create LLaMA-2 Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48c28974-d576-4886-ab0b-061659187313",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "model = \"TheBloke/Llama-2-13B-chat-GPTQ\"\n",
    "# model = \"TheBloke/Llama-2-7B-chat-GPTQ\"\n",
    "\n",
    "llama_pipe = pipeline(\"text-generation\", model=model, device_map=\"auto\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "984b612f-5ce9-4e47-ab25-cbe946164506",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4540012d-1d17-44e2-af7e-a5ff0c587101",
   "metadata": {},
   "source": [
    "In this notebook we will use the following functions to support our interaction with the LLM. Feel free to skim over them presently, as they are covered in greater detail when used below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76923260-7338-4380-9fd7-d3c7756d651e",
   "metadata": {},
   "source": [
    "### Generate Model Responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5148dc93-f5bf-4242-b916-40087f2273c4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate(prompt, max_length=1024, pipe=llama_pipe, **kwargs):\n",
    "    \"\"\"\n",
    "    Generates a response to the given prompt using a specified language model pipeline.\n",
    "\n",
    "    This function takes a prompt and passes it to a language model pipeline, such as LLaMA, \n",
    "    to generate a text response. The function is designed to allow customization of the \n",
    "    generation process through various parameters and keyword arguments.\n",
    "\n",
    "    Parameters:\n",
    "    - prompt (str): The input text prompt to generate a response for.\n",
    "    - max_length (int): The maximum length of the generated response. Default is 1024 tokens.\n",
    "    - pipe (callable): The language model pipeline function used for generation. Default is llama_pipe.\n",
    "    - **kwargs: Additional keyword arguments that are passed to the pipeline function.\n",
    "\n",
    "    Returns:\n",
    "    - str: The generated text response from the model, trimmed of leading and trailing whitespace.\n",
    "\n",
    "    Example usage:\n",
    "    ```\n",
    "    prompt_text = \"Explain the theory of relativity.\"\n",
    "    response = generate(prompt_text, max_length=512, pipe=my_custom_pipeline, temperature=0.7)\n",
    "    print(response)\n",
    "    ```\n",
    "    \"\"\"\n",
    "\n",
    "    def_kwargs = dict(return_full_text=False, return_dict=False)\n",
    "    response = pipe(prompt.strip(), max_length=max_length, **kwargs, **def_kwargs)\n",
    "    return response[0]['generated_text'].strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f120b024-f651-4889-891c-75994ee196e7",
   "metadata": {},
   "source": [
    "### Costruct Prompt, Optionally With System Context and/or Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d208aa65-1a47-4f96-be1a-7e98b4ba0050",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def construct_prompt_with_context(main_prompt, system_context=\"\", conversation_examples=[]):\n",
    "    \"\"\"\n",
    "    Constructs a complete structured prompt for a language model, including optional system context and conversation examples.\n",
    "\n",
    "    This function compiles a prompt that can be directly used for generating responses from a language model. \n",
    "    It creates a structured format that begins with an optional system context message, appends a series of conversational \n",
    "    examples as prior interactions, and ends with the main user prompt. If no system context or conversation examples are provided,\n",
    "    it will return only the main prompt.\n",
    "\n",
    "    Parameters:\n",
    "    - main_prompt (str): The core question or statement for the language model to respond to.\n",
    "    - system_context (str, optional): Additional context or information about the scenario or environment. Defaults to an empty string.\n",
    "    - conversation_examples (list of tuples, optional): Prior exchanges provided as context, where each tuple contains a user message \n",
    "      and a corresponding agent response. Defaults to an empty list.\n",
    "\n",
    "    Returns:\n",
    "    - str: A string formatted as a complete prompt ready for language model input. If no system context or examples are provided, returns the main prompt.\n",
    "\n",
    "    Example usage:\n",
    "    ```\n",
    "    main_prompt = \"I'm looking to improve my dialogue writing skills for my next short story. Any suggestions?\"\n",
    "    system_context = \"User is an aspiring author seeking to enhance dialogue writing techniques.\"\n",
    "    conversation_examples = [\n",
    "        (\"How can dialogue contribute to character development?\", \"Dialogue should reveal character traits and show personal growth over the story arc.\"),\n",
    "        (\"What are some common pitfalls in writing dialogue?\", \"Avoid exposition dumps in dialogue and make sure each character's voice is distinct.\")\n",
    "    ]\n",
    "\n",
    "    full_prompt = construct_prompt_with_context(main_prompt, system_context, conversation_examples)\n",
    "    print(full_prompt)\n",
    "    ```\n",
    "    \"\"\"\n",
    "    \n",
    "    # Return the main prompt if no system context or conversation examples are provided\n",
    "    if not system_context and not conversation_examples:\n",
    "        return main_prompt\n",
    "\n",
    "    # Start with the initial part of the prompt including the system context, if provided\n",
    "    full_prompt = f\"<s>[INST] <<SYS>>{system_context}<</SYS>>\\n\" if system_context else \"<s>[INST]\\n\"\n",
    "\n",
    "    # Add each example from the conversation_examples to the prompt\n",
    "    for user_msg, agent_response in conversation_examples:\n",
    "        full_prompt += f\"{user_msg} [/INST] {agent_response} </s><s>[INST]\"\n",
    "\n",
    "    # Add the main user prompt at the end\n",
    "    full_prompt += f\"{main_prompt} [/INST]\"\n",
    "\n",
    "    return full_prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8acd78c2-d700-4a71-baea-b1e4b76ad80c",
   "metadata": {},
   "source": [
    "## Default Non-random Responses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94a535af-43af-4d04-b8d1-ba2196161302",
   "metadata": {},
   "source": [
    "You may have noticed in previous notebooks that the responses we are getting back from our LLaMA-2 model, assuming we don't edit the prompt, are deterministic. Let's take an explicit look at this.\n",
    "\n",
    "Here we are going to use our model for another generative task, this time to generate fictitious customer experiences with their Galaxy Rider mountain bike. Here we set the **system context** appropriately, and prompt the model to recall a memorable day on their bike."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d9ddb17-e619-4947-b0ba-0ade00a8c5ae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "system_context = \"\"\"\n",
    "You are a perennially satisfied customer who loves to reminisce about personal experiences with products. \\\n",
    "You never delve into technical specifics, as you believe it's the emotion and the joy that matter most. \\\n",
    "You're excited for others to feel the same euphoria and happiness you do. Your aim isn't to advertise, \\\n",
    "but to share a genuine, heartfelt story of joy and contentment.\n",
    "\"\"\"\n",
    "\n",
    "prompt = \"Recall a memorable day out with your Galaxy Rider mountain bike in 50 words or less.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7983d5e7-ab19-445e-86c2-81fe48cce6d6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(generate(construct_prompt_with_context(prompt, system_context)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5294b61-adcd-4a42-b813-533226809d70",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9efbde1d-9ba0-4a27-8091-a185236043dd",
   "metadata": {},
   "source": [
    "Let's generate a response a couple more times with the exact same prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3faea14c-9bb3-41f0-90db-fda38da3c874",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(generate(construct_prompt_with_context(prompt, system_context)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6f0a1c2-39e5-48c2-9a03-8578ebf6afbd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(generate(construct_prompt_with_context(prompt, system_context)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "117165bb-fbd6-494d-9f52-8d5f2138e141",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aef795d9-65d2-4be0-8d14-432cf7b6b128",
   "metadata": {},
   "source": [
    "As stated, the model is generating the exact same response every time. For many scenarios this is exactly the behavior we would like, but for others, we would like to introduce a degree of randomness in to the model's responses. To accomplish this we will modify the **temperature** of the model's responses."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1bdc89d-281b-46f5-8f29-becd0359c0b0",
   "metadata": {},
   "source": [
    "## Sampling and Temperature"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2a5b4e0-67a3-490f-aff7-1882ba4854f6",
   "metadata": {},
   "source": [
    "In the realm of language models, **sampling** is the process by which a model generates text by sampling from the probability distribution of potential next words (tokens, actually, but for our purposes we can think of tokens as words). When we interact with language models without enabling **sampling**, the model operates deterministically, consistently selecting the most probable next word when generating text. This default behavior is useful when you need consistency and accuracy, but it can be limiting when you're aiming for creative and varied responses.\n",
    "\n",
    "Within the context of the `transformers` pipeline we are using to interact with our LLaMA-2 model, **sampling** is *disabled* by default. To enable sampling we set `do_sample=True` when calling our `generate` function, and in doing so, instruct the model to pick words based on a probability distribution, allowing less likely words to be chosen, which can result in more diverse and interesting text.\n",
    "\n",
    "Once **sampling** is enabled, we can also pass in specific values for **temperature**, which you can think of as the degree of randomness in the response. For `temperature` we pass in a value between `0.0` and `1.0`, with larger values indicating a larger degree of randomness."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d9d9a51-d5ac-4fe5-a736-5155dfa9a08d",
   "metadata": {},
   "source": [
    "## Exercise: Random Responses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19d6d682-07d0-4674-87a9-20cc840ec59d",
   "metadata": {},
   "source": [
    "Reusing the same **system context** and prompt as above, enable **sampling** (`do_sample=True`) and set the **temperature** to its highest value (`temperature=1.0`). Generate 3 different responses to check that they are each unique.\n",
    "\n",
    "If you get stuck, see the solution below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c1eb2ab-b768-4a87-aeb3-8e1e468058a8",
   "metadata": {},
   "source": [
    "## Your Work Here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1929355e-85a0-4af2-9c67-90b6fefd754e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8244d459-7fdb-4737-8cd7-f2077de152c9",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b65b2d-fdb6-4c65-8836-e8023cac56a6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(generate(construct_prompt_with_context(prompt, system_context), do_sample=True, temperature=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b1fda59-8421-4abe-8865-1d20daa43723",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(generate(construct_prompt_with_context(prompt, system_context), do_sample=True, temperature=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01ad85e2-13f3-40fc-91d6-e8106563661c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(generate(construct_prompt_with_context(prompt, system_context), do_sample=True, temperature=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4e43770-6d77-4eb7-9288-1cafe2cc1d7b",
   "metadata": {},
   "source": [
    "## Creativity and Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dc45442-ba4d-46ea-bf6b-b2ca4ec24270",
   "metadata": {},
   "source": [
    "As a final thought on `temperature` it's worth mentioning that random generation, while helpful in the context of unique or creative outputs, is not particularly in line with precision and accuracy of responses. In contexts where it is essential that the model is generating precise and/or factually accurate responses, always take care to check its outputs when increasing its temperature."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "425d181a-1d5c-4c3a-87a6-fb454c2acf85",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Key Concept Review"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc0a945f-30f6-4db1-9eb5-60dcc9d3d1bd",
   "metadata": {},
   "source": [
    "The following key concepts were introduced in this notebook:\n",
    "\n",
    "- **Sampling:** A process in text generation where the language model selects the next word (token) based on a probability distribution over the vocabulary.\n",
    "- **Temperature:** A hyperparameter that controls the level of randomness in the model's predictions during sampling. A higher temperature increases diversity, leading to more varied and creative responses, while a lower temperature makes the model's output more predictable and conservative."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "193d3820-c0a8-497e-8dc0-fa741c1fba93",
   "metadata": {},
   "source": [
    "## Optional Advanced Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "119a1b5a-4485-4f72-9c60-6257844367b3",
   "metadata": {},
   "source": [
    "If you'd like to go above and beyond the requirements of the course, below are some additional open-ended exercises for you to try."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa76f625-0299-4809-8d9d-c59896207cfe",
   "metadata": {},
   "source": [
    "### Use the 7B Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5016a362-dd54-413e-9c50-fd89a91d86ca",
   "metadata": {},
   "source": [
    "At the top of the notebook, after restarting the kernel (see cell below), uncomment and use the 7B model instead of the 13B model we demoed. Try to get satisfying results in spite of using the smaller (weaker) model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17909f3d-49de-42d4-861f-2a419ea404b8",
   "metadata": {},
   "source": [
    "### Make Interacting Characters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27b804fb-87f2-4448-b148-500b00158bda",
   "metadata": {},
   "source": [
    "Now that you are able to generate statements from fictitous people, try to create a small system that creates more than one distinct personality and makes them interact with each other."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c32df389-b99f-4b57-ab15-dc17556d87ed",
   "metadata": {},
   "source": [
    "### Make Interacting Characters Play a Game"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff82d544-edc3-4e3a-86bf-2cd180254aa2",
   "metadata": {},
   "source": [
    "Extend the previous exercise to create more than one character who are working toward an end goal defined by some \"game\" you create. This could be trying to get the other character to say a certain word, give away a secret location to a treasure they've hidden, or even something collaborative whereby the character's have to work together to acheive some goal. If you're really up for a challenge you might even consider creating more than 2 characters, or even teams of players interacting with each other."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3b8885f-3035-44bc-9df7-501dcab4b705",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Restart the Kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d99ee978-eca8-49d8-b3e6-95d423f616b2",
   "metadata": {},
   "source": [
    "In order to free up GPU memory for the next notebook, please run the following cell to restart the kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f8de2be-7716-4c23-a06f-7f8364e62bab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from IPython import get_ipython\n",
    "\n",
    "get_ipython().kernel.do_shutdown(restart=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dab090e-9452-42f6-8268-436c2f89b46b",
   "metadata": {},
   "source": [
    "![DLI Header](images/DLI_Header.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
