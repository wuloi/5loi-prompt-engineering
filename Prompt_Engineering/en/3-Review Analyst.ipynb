{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f7d4cbde-2e0f-4b0c-9292-417f4e3da5d6",
   "metadata": {},
   "source": [
    "![DLI Header](images/DLI_Header.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81fcd005-0a74-4a60-8843-427c21374d4d",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Star Bikes Product Review Analyst"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04683de3-1f6f-4797-94e7-b83cba2768c1",
   "metadata": {},
   "source": [
    "In this notebook you'll build an AI-powered document analyst, capable of performing **sentiment analysis** and generating data for downstream tasks. You will learn how to employ **few-shot learning** with a language model by providing it with instructive examples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f15a78b9-482c-4908-a8e8-0d47b65c7a14",
   "metadata": {},
   "source": [
    "## Learning Objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "388db68b-72f2-411c-b2d1-8d14b4595862",
   "metadata": {},
   "source": [
    "By the time you complete this notebook you will be able to:\n",
    "- Perform **sentiment analysis** on unstructured text using LLaMA-2.\n",
    "- Explain what the LLaMA-2 **prompt template** is, and how it was used during **instruction fine-tuning**.\n",
    "- Guide and improve model performance using **few-shot learning**.\n",
    "- Use LLaMA-2 to generate JSON data for potential use in downstream processing tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed10afb1-0d76-48d6-b71c-06bf3f09dd9b",
   "metadata": {},
   "source": [
    "## Video Walkthrough"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e482cfad-1361-4cfd-9c8a-e44e4c730fda",
   "metadata": {},
   "source": [
    "Execute the cell below to load the video walkthrough of this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34431682-a99d-4b0d-a8fa-f9a3b86716f4",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    " from IPython.display import HTML\n",
    "\n",
    "video_url = \"https://d36m44n9vdbmda.cloudfront.net/assets/s-fx-12-v1/v2/03-analyst.mp4\"\n",
    "\n",
    "video_html = f\"\"\"\n",
    "<video controls width=\"640\" height=\"360\">\n",
    "    <source src=\"{video_url}\" type=\"video/mp4\">\n",
    "    Your browser does not support the video tag.\n",
    "</video>\n",
    "\"\"\"\n",
    "\n",
    "display(HTML(video_html))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b5785bb-fd00-4f29-bd6e-48fd893b1888",
   "metadata": {},
   "source": [
    "## Create LLaMA-2 Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4939afbd-23ca-4db8-b12b-209c224ee4ab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "model = \"TheBloke/Llama-2-13B-chat-GPTQ\"\n",
    "# model = \"TheBloke/Llama-2-7B-chat-GPTQ\"\n",
    "\n",
    "llama_pipe = pipeline(\"text-generation\", model=model, device_map=\"auto\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d27570d1-9c9b-445f-a5dc-bff5be51731e",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a92ece14-a296-4c51-b664-9e682a5d84fc",
   "metadata": {},
   "source": [
    "In this notebook we will use the following functions to support our interaction with the LLM. Feel free to skim over them presently, as they are covered in greater detail when used below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89ed9f47-4a60-4b0a-8a57-172158e05608",
   "metadata": {},
   "source": [
    "### Generate Model Responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fda46556-8b36-407d-94d0-1483cb065e9c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate(prompt, max_length=1024, pipe=llama_pipe, **kwargs):\n",
    "    \"\"\"\n",
    "    Generates a response to the given prompt using a specified language model pipeline.\n",
    "\n",
    "    This function takes a prompt and passes it to a language model pipeline, such as LLaMA, \n",
    "    to generate a text response. The function is designed to allow customization of the \n",
    "    generation process through various parameters and keyword arguments.\n",
    "\n",
    "    Parameters:\n",
    "    - prompt (str): The input text prompt to generate a response for.\n",
    "    - max_length (int): The maximum length of the generated response. Default is 1024 tokens.\n",
    "    - pipe (callable): The language model pipeline function used for generation. Default is llama_pipe.\n",
    "    - **kwargs: Additional keyword arguments that are passed to the pipeline function.\n",
    "\n",
    "    Returns:\n",
    "    - str: The generated text response from the model, trimmed of leading and trailing whitespace.\n",
    "\n",
    "    Example usage:\n",
    "    ```\n",
    "    prompt_text = \"Explain the theory of relativity.\"\n",
    "    response = generate(prompt_text, max_length=512, pipe=my_custom_pipeline, temperature=0.7)\n",
    "    print(response)\n",
    "    ```\n",
    "    \"\"\"\n",
    "\n",
    "    def_kwargs = dict(return_full_text=False, return_dict=False)\n",
    "    response = pipe(prompt.strip(), max_length=max_length, **kwargs, **def_kwargs)\n",
    "    return response[0]['generated_text'].strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "302792f2-2cf2-4924-94d4-05cd5da61238",
   "metadata": {},
   "source": [
    "### Create Prompt With Examples (Few-shot Learning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e002b417-f5b6-4813-85be-97c11fe366e2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def prompt_with_examples(prompt, examples=[]):\n",
    "    \"\"\"\n",
    "    Constructs a structured prompt string for language models with instructional examples.\n",
    "\n",
    "    This function takes an initial prompt and a list of example prompt-response pairs, then \n",
    "    formats them into a single string enclosed by special start and end tokens used for \n",
    "    instructing the model. Each example is included in the final prompt, which could be \n",
    "    beneficial for models that take into account the context provided by examples.\n",
    "\n",
    "    Parameters:\n",
    "    - prompt (str): The main prompt to be processed by the language model.\n",
    "    - examples (list of tuples): A list where each tuple contains a pair of strings \n",
    "      (example_prompt, example_response). Default is an empty list.\n",
    "\n",
    "    Returns:\n",
    "    - str: A string with the structured prompt and examples formatted for a language model.\n",
    "    \n",
    "    Example usage:\n",
    "    ```\n",
    "    main_prompt = \"Translate the following sentence into French:\"\n",
    "    example_pairs = [(\"Hello, how are you?\", \"Bonjour, comment Ã§a va?\"),\n",
    "                     (\"Thank you very much!\", \"Merci beaucoup!\")]\n",
    "    formatted_prompt = prompt_with_examples(main_prompt, example_pairs)\n",
    "    print(formatted_prompt)\n",
    "    ```\n",
    "    \"\"\"\n",
    "    \n",
    "    # Start with the initial part of the prompt\n",
    "    full_prompt = \"<s>[INST]\\n\"\n",
    "\n",
    "    # Add each example to the prompt\n",
    "    for example_prompt, example_response in examples:\n",
    "        full_prompt += f\"{example_prompt} [/INST] {example_response} </s><s>[INST]\"\n",
    "\n",
    "    # Add the main prompt and close the template\n",
    "    full_prompt += f\"{prompt} [/INST]\"\n",
    "\n",
    "    return full_prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "853c8e9d-f25f-4dcf-8fa9-7afa611a9aef",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Data - Starlight Cruiser Reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82e8362b-9aff-4904-8ff6-c7db191b0c1c",
   "metadata": {},
   "source": [
    "The following are customer reviews for a the Starlight Cruiser bicycle, a model offered by the fictitious bike company Star Bikes. We will be providing these reviews to the LLM for it to perform **sentiment analysis** and other analytical tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a78c172-41c1-4468-9719-e953d4f55c8b",
   "metadata": {},
   "source": [
    "**Neutral Review**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87bda9b4-1eff-47fd-96a2-9afbf2eab339",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "review = \"\"\"\n",
    "I recently purchased the Starlight Cruiser from Star Bikes, and I've been thoroughly impressed. \\\n",
    "The ride is smooth and it handles urban terrains with ease. \\\n",
    "However, I did find the seat a bit uncomfortable for longer rides. \\\n",
    "Also, the color options could be better. Despite these minor drawbacks, \\\n",
    "the build quality and the performance of the bike are commendable. It's a good value for the money.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7506315-1a7a-4bba-8853-fac6ca63b32a",
   "metadata": {},
   "source": [
    "**Negative Review**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0dba279-2d55-448b-ba0f-4c9714ddb2dd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "review_negative = \"\"\"\n",
    "Got the Starlight Cruiser last week, and I'm a bit disappointed. \\\n",
    "The brakes are not as responsive as I'd like and the gears often get stuck. \\\n",
    "The design is good but performance-wise, it leaves much to be desired.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0454aeb-e64a-45d7-87fd-1a5c31e87153",
   "metadata": {},
   "source": [
    "**Positive Review**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d3dd4da-a422-4834-b182-11dae014aaa2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "review_positive = \"\"\"\n",
    "I recently purchased the Starlight Cruiser from Star Bikes, and I've been thoroughly impressed. \\\n",
    "The ride is smooth and it handles urban terrains with ease. \\\n",
    "The seat was very comfortable for longer rides and the color options were great. \\\n",
    "The build quality and the performance of the bike are commendable. It's a good value for the money.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e16d2548-a72f-4b43-9c00-2525004c91f9",
   "metadata": {},
   "source": [
    "## Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90f05f3a-40b6-4c62-b11c-9aa7de83e116",
   "metadata": {},
   "source": [
    "We will begin by asking the model to perform **sentiment analysis** by telling us the overall sentiment of one of our reviews. Ultimately we would like the model to provide us with a response of just a single word, either `positive`, `negative`, or `neutral`.\n",
    "\n",
    "Just as a heads up, the following cell is going to result in a very strange output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b502daf6-bcbd-4bde-bec1-ba95a8eeebc0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt = f\"\"\"\n",
    "What is the overall sentiment of {review}\n",
    "\"\"\"\n",
    "\n",
    "print(generate(prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c75167ad-a9db-48c2-b9ab-3ad608e81da9",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcdd4f30-9e35-4c1c-8399-14d0f700135a",
   "metadata": {},
   "source": [
    "It's not totally clear why the model gave us such junk with the prompt above, but it makes clear the importance of working on prompts iteratively, and brings again to the forefront the idea of **precision**. Let's make a very small update to the prompt by adding a `?` to the end, which should make more clear to the model that we are asking it a question we are hoping to get a response to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "033d8d0e-91d3-4602-a6d8-0c35e8c96060",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt = f\"\"\"\n",
    "What is the overall sentiment of {review}?\n",
    "\"\"\"\n",
    "\n",
    "print(generate(prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3dc56ef-0b57-4c56-bbbd-406b75e530b7",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f370c6c9-dac5-4699-97f0-8a50f41ca318",
   "metadata": {},
   "source": [
    "The `?` made a huge difference! This example serves to remind us that minor tweaks to prompts can sometimes lead to drastic changes in the model's responses.\n",
    "\n",
    "Given that our goal here is to get a single word response back from the model, let's iterate on the prompt to be more **specific** about the response we would like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5d8becd-feac-4749-b5d5-7e988569e41d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt = f\"\"\"\n",
    "What is the overall sentiment of this review {review}?\n",
    "\n",
    "Choose one of \"positive\", \"negative\", or \"neutral\".\n",
    "\"\"\"\n",
    "\n",
    "print(generate(prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ba52b8a-9af2-4dd9-9a9d-cf8ecca5c3d3",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67774d5b-785f-44b5-a2a3-71a1c4e4e737",
   "metadata": {},
   "source": [
    "The model continues to be more helpful than we would like, providing us with more than just the single word response we are aiming for. Let's iterate on the prompt one more time by providing it a **cue** for its response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6058b768-bda7-470e-b287-d0f611a9297e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt = f\"\"\"\n",
    "What is the overall sentiment of this review {review}?\n",
    "\n",
    "Choose one of \"positive\", \"negative\", or \"neutral\".\n",
    "Overall Sentiment: \n",
    "\"\"\"\n",
    "\n",
    "print(generate(prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88a93eff-cd4e-4e55-b7ae-567d3a602a7d",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67981387-132c-4cd7-9faf-6f1b89ce7d60",
   "metadata": {},
   "source": [
    "That's much better. However, when I read the review myself, I have to say, I might categorize the review as \"neutral\" rather than \"positive\". Here's the review again for reference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6dd3f6a-98a2-4b9b-9cb6-00dfeea37187",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "review = \"\"\"\n",
    "I recently purchased the Starlight Cruiser from Star Bikes, and I've been thoroughly impressed. \\\n",
    "The ride is smooth and it handles urban terrains with ease. \\\n",
    "However, I did find the seat a bit uncomfortable for longer rides. \\\n",
    "Also, the color options could be better. Despite these minor drawbacks, \\\n",
    "the build quality and the performance of the bike are commendable. It's a good value for the money.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0e98a13-70be-41fe-a63b-3cd53727527e",
   "metadata": {},
   "source": [
    "Again, to emphasize how minor changes can drastically impact a model's behavior, let's slightly modify the prompt by changing the order of the options the model has to choose from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16e58aa2-408c-49cb-b9ac-e53ef7e7f1b3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# NOTE: 'Choose one of \"neutral\", \"negative\", or \"positive\".' is in a different order than the prompt immediately above\n",
    "prompt = f\"\"\"\n",
    "What is the overall sentiment of this review {review}?\n",
    "\n",
    "Choose one of \"neutral\", \"negative\", or \"positive\".\n",
    "Overall Sentiment: \n",
    "\"\"\"\n",
    "\n",
    "print(generate(prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c899ece-4f7a-44ba-a4d6-b6976327fb58",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77b478d2-c73d-4595-a082-fed49e50b863",
   "metadata": {},
   "source": [
    "Just to conclude this minor experiment, let's try yet another order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e551d12-88fb-4188-9c7f-140fea141b31",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt = f\"\"\"\n",
    "What is the overall sentiment of this review {review}?\n",
    "\n",
    "Choose one of \"negative\", \"neutral\", or \"positive\".\n",
    "Overall Sentiment: \n",
    "\"\"\"\n",
    "\n",
    "print(generate(prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6f6d7b4-ca76-4588-86b4-1f9f87f38c85",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a75dda3e-479a-4120-8df2-16de33fb4907",
   "metadata": {},
   "source": [
    "Our prompt does not currently give us confidence that we are going to get meaningful responses from the model. In order to get more reliable, trustworthy responses, let's turn our attention to an important technique, **few-shot learning**, which will allow us to provide instructive examples to the model about how it ought to behave."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c1989ec-f6ba-463a-acfe-0047a1eb628e",
   "metadata": {},
   "source": [
    "## Provide Examples: Few-shot Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc4167b3-3cf0-42c7-aab8-7a9872a7dd71",
   "metadata": {},
   "source": [
    "Depending on the number of examples provided, the following technique is referred to as **one-shot learning**, **two-shot learning**, **three-shot learning** (etc.), **many-shot learning** and **one-to-many shot learning**. In each case a **shot** is an example prompt/response cycle provided to the model to help guide its behavior.\n",
    "\n",
    "The shots are typically prepended to the main prompt we wish the model to generate a response for. Depending on the model that is being used, there are specific ways to format our shots that will help the model understand that what we are providing it are prompt/response examples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65088138-500f-4af3-b27e-d8fded21e835",
   "metadata": {},
   "source": [
    "## Instruction Fine-tuned (Chat) Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff42afbd-36d8-4d91-b09d-8de021e7c081",
   "metadata": {},
   "source": [
    "If you have ever looked at model repositories, say on Hugging Face, you may have noticed that there are [some models that are of the `-chat` variant](https://huggingface.co/models?sort=trending&search=meta-llama%2FLlama-2-13b). These are models (for example, `Llama-2-13b-chat`) that have had additional training on top of base pretrained models (for example `Llama-2-13b`), typically in order to make them better at following instructions, in support of use in chat applications. They have been **instruction fine-tuned**.\n",
    "\n",
    "While pretrained LLM base models are typically well-suited for generating text by understanding the probability of what would come next given some input, simply providing the next-most-likely text is not the same as responding to a question or instruction.\n",
    "\n",
    "During **instruction fine-tuning**, models are trained on a vast amount of example interactions between a user and a model so that the model can learn how to appropriately follow instructions or respond appropriately in a chat dialogue context."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e4b374f-9cf6-445f-99eb-ec0438285c3a",
   "metadata": {},
   "source": [
    "## The LLaMA-2 Prompt Template"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1104a133-81d7-47c3-94fd-ce7a700bec7c",
   "metadata": {},
   "source": [
    "Depending on the instruction fine-tuned model (that is, the chat variant), the example interactions that it was **instruction fine-tuned** on will be formatted in different ways. The template used for this training process is called the **prompt template**. You can typically find a given model's prompt template in its documentation.\n",
    "\n",
    "Below is the LLaMA-2 prompt template. It is actually a slightly simplified version, excluding a component that we will be looking at later in the course."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "284a62b0-0a63-4d49-8962-5b482f9443ad",
   "metadata": {
    "tags": []
   },
   "source": [
    "```python\n",
    "<s>[INST] {{ user_msg_1 }} [/INST] {{ model_answer_1 }} </s>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c865dd1-b305-422c-9e3a-5a8431f423bf",
   "metadata": {},
   "source": [
    "Let's disect this **prompt template**.\n",
    "- A single user/model interaction is contained between the `<s>` and `</s>` tags.\n",
    "- The user part of the user/model interaction is contained between the `[INST]` and `[/INST]` tags.\n",
    "- The model part of the user/model interaction follows the `[/INST]` tag and ends at the interaction-concluding `</s>` tag."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb282cab-88e2-452b-8510-dd85a96627e0",
   "metadata": {},
   "source": [
    "During **instruction fine-tuning**, the model was given many user/model examples using this **prompt template**. With that in mind, we can leverage its training and use this same **prompt template** to provide our own instructive examples to the model for how it ought to behave."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "661caa7d-219c-4af8-b278-f398bfc54c1e",
   "metadata": {},
   "source": [
    "## Providing Instructive Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b48e6a8a-b711-4816-8bbf-5d577a72bc1c",
   "metadata": {},
   "source": [
    "The following `prompt_with_examples` function will help us constuct prompts that include prepended instructive examples, using the LLaMA-2 **prompt template**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b68c3cc-c90c-41e7-8824-a0339c95f418",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def prompt_with_examples(prompt, examples=[]):\n",
    "    \"\"\"\n",
    "    Constructs a structured prompt string for language models with instructional examples.\n",
    "\n",
    "    This function takes an initial prompt and a list of example prompt-response pairs, then \n",
    "    formats them into a single string enclosed by special start and end tokens used for \n",
    "    instructing the model. Each example is included in the final prompt, which could be \n",
    "    beneficial for models that take into account the context provided by examples.\n",
    "\n",
    "    Parameters:\n",
    "    - prompt (str): The main prompt to be processed by the language model.\n",
    "    - examples (list of tuples): A list where each tuple contains a pair of strings \n",
    "      (example_prompt, example_response). Default is an empty list.\n",
    "\n",
    "    Returns:\n",
    "    - str: A string with the structured prompt and examples formatted for a language model.\n",
    "    \n",
    "    Example usage:\n",
    "    ```\n",
    "    main_prompt = \"Translate the following sentence into French:\"\n",
    "    example_pairs = [(\"Hello, how are you?\", \"Bonjour, comment Ã§a va?\"),\n",
    "                     (\"Thank you very much!\", \"Merci beaucoup!\")]\n",
    "    formatted_prompt = prompt_with_examples(main_prompt, example_pairs)\n",
    "    print(formatted_prompt)\n",
    "    ```\n",
    "    \"\"\"\n",
    "    \n",
    "    # Start with the initial part of the prompt\n",
    "    full_prompt = \"<s>[INST]\\n\"\n",
    "\n",
    "    # Add each example to the prompt\n",
    "    for example_prompt, example_response in examples:\n",
    "        full_prompt += f\"{example_prompt} [/INST] {example_response} </s><s>[INST]\"\n",
    "\n",
    "    # Add the main prompt and close the template\n",
    "    full_prompt += f\"{prompt} [/INST]\"\n",
    "\n",
    "    return full_prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92dbfb02-5e03-45b7-9f6b-ce2a31de6c39",
   "metadata": {},
   "source": [
    "## One-Shot Learning Example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7d3aba2-72b9-44ca-b329-a0d77dd97d5e",
   "metadata": {},
   "source": [
    "Let's briefly step away from our sentiment analysis task, and use a simple text generation prompt to explore how we can use the `prompt_with_examples` function to provide an instructive example, or put another way, perform **one-shot learning**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "633d2775-f0a3-406e-bfd3-2f853e47e513",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "example_prompt = \"Give me an all uppercase color that starts with the letter 'O'.\"\n",
    "example_response = \"ORANGE\"\n",
    "\n",
    "# The function expects prompt/response pairs to be 2-tuples\n",
    "example_1 = (example_prompt, example_response)\n",
    "\n",
    "# The function expects all prompt/response 2-tuples to be in a list\n",
    "examples = [example_1]\n",
    "\n",
    "# This is the \"main\" prompt we actually want the model to respond to\n",
    "prompt = \"Give me an all uppercase color that starts with the letter 'P'.\"\n",
    "\n",
    "# Use `prompt_with_examples` to create a one-shot learning prompt, with a single example prepended to our main prompt\n",
    "prompt_with_one_example = prompt_with_examples(prompt, examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aee3644c-d99b-4d7c-8604-590cf258a78a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(prompt_with_one_example)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb1c4033-c4dd-4dc4-a159-b43e2ca36690",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05349fdc-ce30-4174-8c3e-491c2c4773d2",
   "metadata": {},
   "source": [
    "`prompt_with_one_example` above includes a single user/model interaction (`<s>...</s>`), using the LLaMA-2 **prompt template**, prepended to the main prompt. Note that the main prompt only includes the user part of the interaction (between the `[INST]` and `[/INST]` tags) and leaves the rest of the interaction (the model's response and the `</s>` tag) for the model to complete.\n",
    "\n",
    "Before using `prompt_with_one_example` let's see what kind of response we get back from the model passing in just the main prompt, without an instructive example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2f3b822-5e26-4a8b-b539-b4db786ac20e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(generate(prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf2db258-76dc-45cd-ba44-4104e5c3229a",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78ebbb77-3f76-4214-b589-67f19e39ea99",
   "metadata": {},
   "source": [
    "We see that while we got `PURPLE` we also got additional chat-like response prior to the output that we want.\n",
    "\n",
    "Now let's get a response using `prompt_with_one_example`, which contains an example of how the model should response with only the word for the color we are interested in it generating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dd004cd-7bd2-4fa9-8b74-8a4ee569eb6a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(generate(prompt_with_one_example))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b78cfb00-37d0-43ec-8781-af39f626eb37",
   "metadata": {},
   "source": [
    "## Sentiment Analysis With Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcfa5cfe-db99-407b-a87c-2d0b49860f86",
   "metadata": {},
   "source": [
    "Since when we left off with our sentiment analysis task, we lacked confidence about whether the model would correctly label a neutral review, let's apply what we just learned about **one-shot learning** to provide our model with an instructive example of responding to what, as a human, we would consider a neutral review.\n",
    "\n",
    "Here is an example we would like classified as a neutral review, which while clearly not negative contains both positive and negative sentiments about the bike."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82827c7d-43de-4a3e-89c2-78fdf0c331f9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "example_neutral_review = \"\"\"\n",
    "I've had the chance to put several miles on my new Starlight Cruiser from Star Bikes. \n",
    "First off, the bike's design is sleek, and it provides an exceptionally stable ride, \n",
    "even when navigating the bustle of city streets. The gear shifting is fluid, \n",
    "and the bike feels robust, promising longevity. On the downside, the braking system, \n",
    "while reliable, lacks the responsiveness I've experienced with other bikes. \n",
    "I also noticed that the handlebar grips can become rather uncomfortable on prolonged journeys. \n",
    "Nevertheless, these issues aside, the bike offers impressive performance for its price range, \n",
    "making it a solid, middle-of-the-road choice for both commuting and leisure rides.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf740a62-21b6-495d-abf8-85fa4698f1e1",
   "metadata": {},
   "source": [
    "We will use the example review to construct an `examples` list we can pass into the `prompt_with_examples` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54ef51c0-3f0a-43a5-8ac4-6ee8773a11dd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "example_prompt_neutral = f\"\"\"\n",
    "What is the overall sentiment of this review {example_neutral_review}?\n",
    "\n",
    "Choose one of \"negative\", \"neutral\", or \"positive\".\n",
    "Overall Sentiment: \n",
    "\"\"\"\n",
    "example_response_neutral = \"neutral\"\n",
    "\n",
    "example_neutral = (example_prompt_neutral, example_response_neutral)\n",
    "examples = [example_neutral]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f18bef23-5d4d-4cd8-acb1-99178ffbf29d",
   "metadata": {},
   "source": [
    "Now we construct the main prompt, which again uses the review from above that we hope to be classified as neutral."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fd7bd41-1da5-4485-9041-63bc260f46ab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt = f\"\"\"\n",
    "What is the overall sentiment of this review {review}?\n",
    "\n",
    "Choose one of \"negative\", \"neutral\", or \"positive\".\n",
    "Overall Sentiment: \n",
    "\"\"\"\n",
    "\n",
    "prompt_with_one_example = prompt_with_examples(prompt, examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e11ac4b-6bff-42bf-b157-aca8d4965c24",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(generate(prompt_with_one_example))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04cff4a0-5b07-4925-a5b3-81eec9bd67ee",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d93ce3d-3921-4d6c-acf0-c1946fe2bd30",
   "metadata": {},
   "source": [
    "Unfortunately, the model is still labeling the review as `\"Positive\"`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a4c1e60-fe03-41ae-90f0-1ffc3e236956",
   "metadata": {},
   "source": [
    "## Two-shot Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bf121c2-9846-4324-91a0-410cbb55dbfa",
   "metadata": {},
   "source": [
    "Often, when **one-shot learning** is not sufficient to get the kind of behavior we would like, we can include additional examples to further support the model's behavior.\n",
    "\n",
    "In our case, in addition to the neutral example we have given the model, let's also provide it with an example of a positive review in hopes that it will be more clear about the difference between the two."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41171f60-f586-4782-ac90-5c13a123a697",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "example_review_positive = \"\"\"\n",
    "I've been absolutely delighted with my Starlight Cruiser purchase from Star Bikes. \n",
    "The bike exudes a charm with its sleek design that turns heads as I glide through city lanes. \n",
    "It's not just about looks though; the bike performs wonderfully. The gears shift like a dream, \n",
    "making for a ride that's as smooth as silk across various urban terrains. I was initially skeptical \n",
    "about the comfort of the seat, but it proved to be pleasantly supportive, even on my longer weekend adventures. \n",
    "While the color choices were limited, I found one that suited my style perfectly. \n",
    "Any minor imperfections pale in comparison to the bike's overall quality and the sheer joy it brings to my daily commutes. \n",
    "For the price, the Starlight Cruiser is an undeniable gem that I would happily recommend.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d759acb3-a10a-4808-86fe-a1a24f464260",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "example_prompt_positive = f\"\"\"\n",
    "What is the overall sentiment of this review {example_review_positive}?\n",
    "\n",
    "Choose one of \"negative\", \"neutral\", or \"positive\".\n",
    "Overall Sentiment: \n",
    "\"\"\"\n",
    "example_response_positive = \"positive\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1df1fa0e-d666-4f26-a5da-da3b7d5fd2ee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "main_prompt = f\"\"\"\n",
    "What is the overall sentiment of this review {review}?\n",
    "\n",
    "Choose one of \"negative\", \"neutral\", or \"positive\".\n",
    "Overall Sentiment: \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71865d91-1ac5-457c-a594-096690a07a3d",
   "metadata": {},
   "source": [
    "### Exercise: Perform Two-shot Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e64fc1b-7fda-46f0-ab67-efdee1c5edff",
   "metadata": {},
   "source": [
    "Perform **two-shot learning**, providing the model with both a neutral and a positive example interaction before prompting it for a response to `review` which we are hoping it will classify as `neutral`.\n",
    "\n",
    "- Use `example_neutral` (already defined above) as one example.\n",
    "- Use `example_review_positive`, `example_prompt_positive` and `example_response_positive` above to construct a positive user/model interaction example.\n",
    "- Use both examples (neutral and positive) along with `main_prompt` above, to construct a prompt with two examples (using the `prompt_with_examples` function).\n",
    "- Generate and print a model response using your prompt with two examples.\n",
    "\n",
    "If you get stuck, there is a working solution below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1fddc1b-c703-4419-aaaa-b52be5bbcc02",
   "metadata": {},
   "source": [
    "### Your Work Below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d346908-9b38-445a-b0e6-31314c2ab828",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4763b2ae-7531-49b3-9308-061a190fd492",
   "metadata": {},
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9ccf53c-c82a-4071-8c84-e651c0c5cf59",
   "metadata": {},
   "source": [
    "Click on the ... to see a working solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e1c2e73-dfc7-4082-a8f7-f3851301572d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "example_positive = (example_prompt_positive, example_response_positive)\n",
    "examples = [example_neutral, example_positive]\n",
    "\n",
    "prompt_with_two_examples = prompt_with_examples(main_prompt, examples)\n",
    "\n",
    "print(generate(prompt_with_two_examples))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e878248-8405-49ad-8103-ea7c2eb5b8c0",
   "metadata": {},
   "source": [
    "## Generate Data for Downstream Consumption"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5a60b41-a460-4ccf-9608-4916b17e0523",
   "metadata": {},
   "source": [
    "Now that our model is able to perform **sentiment analysis** effectively, let's extend its analysis capabilities to be able to generate JSON objects for downstream consumption that contain a given review's positive and negative points.\n",
    "\n",
    "We will begin iterating on a prompt by simply asking the model to separately list out the positive and negative points in a review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3c61db3-9b8e-4dcb-bd6f-18eb7e1dfb4c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt = f\"\"\"\n",
    "From the review, list the positive points and negative points separately: {review}\n",
    "\"\"\"\n",
    "\n",
    "print(generate(prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acce4fc3-ab0f-41d7-ad69-db056584768b",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b911c64-88d7-4219-a8c7-eee724efa2ee",
   "metadata": {},
   "source": [
    "The model did quite well. Let's iterate now to try to get the model to produce a JSON object for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b3dc9cf-6dcf-41a8-b357-797f26ea93b2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt = f\"\"\"\n",
    "From the review, list down the positive points and negative points separately, in JSON: {review}\n",
    "\"\"\"\n",
    "\n",
    "print(generate(prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bf57028-ebde-4222-af29-f412cfbd092f",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c6bbf8a-f762-4867-a6a9-72bbe3a5cfdd",
   "metadata": {},
   "source": [
    "That did not appear to make much of a difference. Let's be more **precise** in our prompt about how we want the data formatted. (Note: we must use double curly braces `{{` and `}}` rather than single curly braces `{` and `}` below because we are using a Python f-string, which interprets single curly braces as placeholders for Python values to be interpolated.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a351589-8031-4ecf-b335-2d354d9bba0a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt = f\"\"\"\n",
    "From the review below, list down the positive points and negative points separately, in JSON. Use the following format:\n",
    "\n",
    "{{\"positive\": [], \"negative\": []}}\n",
    "\n",
    "Review: {review}\n",
    "\"\"\"\n",
    "\n",
    "print(generate(prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9e79ad7-8bff-438d-aa63-91098ab8cb79",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a44d5e44-1921-484d-b3a0-2896a68b8a70",
   "metadata": {},
   "source": [
    "This also did not seem to make much of a difference."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dcdd580-fd71-4c1c-ae0e-257173c4db20",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Exercise: Successful JSON Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7641f227-3353-42dd-9620-36afa52b8318",
   "metadata": {},
   "source": [
    "Using what you have learned thus far, successfully complete our task to get the model to create JSON outputs. In case you'd like to provide the model with instructive examples, two example reviews, along with their corresponding JSON outputs have been provided to you below.\n",
    "\n",
    "Also below is a `pretty_print_json` helper function which you should be able to pass the response from the LLM. If the response if valid JSON, the function will print it with nice indenting.\n",
    "\n",
    "If you get stuck, there are 2 solutions below. They include several cells which are currently hidden. You can display them by clicking on the `+ N cells hidden` buttons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29d66b66-2675-42de-a9f0-8612a4d1167d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "example_reviews = [\n",
    "\"\"\"\\\n",
    "I recently purchased the Starlight Cruiser from Star Bikes, and I've been thoroughly impressed. \\\n",
    "The ride is smooth and it handles urban terrains with ease. \\\n",
    "However, I did find the seat a bit uncomfortable for longer rides. \\\n",
    "Also, the color options could be better. Despite these minor drawbacks, \\\n",
    "the build quality and the performance of the bike are commendable. It's a good value for the money.\\\n",
    "\"\"\",\n",
    "\"\"\"\\\n",
    "Got the Starlight Cruiser last week, and I'm a bit disappointed. \\\n",
    "The brakes are not as responsive as I'd like and the gears often get stuck. \\\n",
    "The design is good but performance-wise, it leaves much to be desired.\\\n",
    "\"\"\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afc94723-a0f1-4747-84c3-acbaec993943",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "example_outputs = [\n",
    "    {\n",
    "        \"positive\": [\"smooth ride\", \"ease of handling urban terrains\", \"good value for the money\"],\n",
    "        \"negative\": [\"seat uncomfortable for longer rides\", \"limited color options\"]\n",
    "    },\n",
    "    {\n",
    "        \"positive\": [\"good design\"],\n",
    "        \"negative\": [\"brakes not repsonsive\", \"gears often get stuck\", \"performance leaves much to be desired\"]\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67c7faab-ad5e-4a02-abbb-e3e97f56c20c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def pretty_print_json(json_string):\n",
    "    print(json.dumps(json.loads(json_string), indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ff812a8-22c5-436d-aca0-bef2f42eacdc",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Solution 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "355fadea-39db-4406-9f26-d238ba555e34",
   "metadata": {},
   "source": [
    "Create a list of prompt/response 2-tuple examples. Note, in this implementation we are simply using the review as the prompt, without any additional input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba63c6ef-54bf-42ab-a80c-cb34c5b8a7d5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "examples = [(example_review, json.dumps(example_output)) for example_review, example_output in zip(example_reviews, example_outputs)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b36ba2a0-eb41-4486-93bd-f7be9155b61d",
   "metadata": {},
   "source": [
    "Check the formatting of our examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7b76248-6060-4ca4-9ee2-5fa77c783d87",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e595ea4-0f21-4ada-8611-3d6e36cde3db",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5effcf36-a628-49b3-bd60-fce2d031a372",
   "metadata": {},
   "source": [
    "Use `review` as our prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45ac716d-7adb-46ac-a1c2-06a7b7dbbdd0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt = review"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2563dc99-094d-46bb-9090-80cb227b91b6",
   "metadata": {},
   "source": [
    "Generate a response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bb98ea0-64d2-4c71-9f3e-a0af29a921bb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "json_response = generate(prompt_with_examples(prompt, examples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f00ee72-d283-4a4f-a9bd-bb0fc1f9f78f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pretty_print_json(json_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ee16a34-9528-4a39-a12a-36cd2dc2ff13",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Solution 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac35791e-a2ea-4eed-83aa-77153e96f629",
   "metadata": {},
   "source": [
    "While *Solution 1* demonstrates an effective use of **two-shot learning**, in line with this notebook's objectives, it's worth mentioning I was also able to get a working solution by adding a **cue** of `JSON:` to the prompt we were iterating on earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5440a4a4-1169-4bec-b695-41241720e378",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt = f\"\"\"\n",
    "From the review below, list down the positive points and negative points separately, in JSON. Use the following format:\n",
    "\n",
    "{{\"positive\": [], \"negative\": []}}\n",
    "\n",
    "Review: {review}\n",
    "JSON:\n",
    "\"\"\"\n",
    "\n",
    "pretty_print_json(generate(prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef00f573-c735-4b49-9c0a-268ddef53d17",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Key Concept Review"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f70a0eb3-ca0e-4a0b-9883-4c647b2a69a2",
   "metadata": {},
   "source": [
    "The following key concepts were introduced in this notebook:\n",
    "\n",
    "- **Sentiment Analysis:** Identifying the mood or sentiment for a piece of text.\n",
    "- **Instruction Fine-Tuning:** Improving a model's task performance through tailored example-based learning.\n",
    "- **LLaMA-2 Prompt Template:** A pre-designed format guiding LLaMA-2 model responses, used during instruction fine-tuning.\n",
    "- **Few-shot Learning:** Prepending one-to-many instructive examples to a model to improve its responses."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57ed6228-d930-4216-981b-fbb17f43169d",
   "metadata": {},
   "source": [
    "## Optional Advanced Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f085bcc0-e14f-493f-8d82-2de8479b88fd",
   "metadata": {},
   "source": [
    "If you'd like to go above and beyond the requirements of the course, below are some additional open-ended exercises for you to try."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c4e5e3c-4d6b-40ba-a394-7303f6d51c02",
   "metadata": {},
   "source": [
    "### Varied Output Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1e86f04-9286-46a2-94c1-1324f4b917d3",
   "metadata": {},
   "source": [
    "We were successful in generating JSON. Try to generate different forms of data, like HTML tables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ab309cc-36af-4de4-9612-db45e834aaa6",
   "metadata": {},
   "source": [
    "### Use the 7B Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1bbea16-622a-489d-ad55-470d16847e32",
   "metadata": {},
   "source": [
    "At the top of the notebook, after restarting the kernel (see cell below), uncomment and use the 7B model instead of the 13B model we demoed. Try to get satisfying results through prompt engineering in spite of using the smaller (weaker) model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ff80ed9-ac57-4aff-8ea1-56cce851733c",
   "metadata": {},
   "source": [
    "### Reuse the Model for Additional Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da9608db-f236-407d-9c3c-0b52d6fbc0e6",
   "metadata": {},
   "source": [
    "One of the superpowers of LLMs like LLaMA-2 is that they are capable to perform many tasks that we may have historically needed many different models to perform.\n",
    "\n",
    "Consider that if you were to have gathered \"positive\" and \"negative\" points for many reviews, you might have many items that you recognize as similar, but that the model recorded as different strings, for example \"Nice tires\" and \"Awesome tires\". Can you use reuse the model to create data that somehow captures these kinds of items, which you and I recognize as the same, to be captured in the data as being the same?\n",
    "\n",
    "What other kinds of analysis would you be interested to perform?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caee7b83-5f14-45e2-9958-90fea3374a76",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Restart the Kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "334779e1-fecf-4157-ab7c-9bddb41f3b29",
   "metadata": {},
   "source": [
    "In order to free up GPU memory for the next notebook, please run the following cell to restart the kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e35a6feb-846c-450e-8991-63c3b06392cc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from IPython import get_ipython\n",
    "\n",
    "get_ipython().kernel.do_shutdown(restart=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f6fef8c-117b-4acd-9ff9-fd681b7f4215",
   "metadata": {},
   "source": [
    "![DLI Header](images/DLI_Header.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
